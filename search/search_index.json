{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to CAST AI \u00b6 It is one platform that cuts your cloud costs, boosts DevOps, and prevents downtime. Easy to start \u00b6 First, connect your cluster to CAST AI and see how much you can save by optimizing cluster configuration. After exploring the available savings and cost reports, you can onboard the cluster into CAST AI and set the Autoscaler policies, which will manage the cluster for you. It takes less than 5 minutes to fully onboard a cluster, please follow the guide to learn more . CAST AI abstracts layers of technical complexity from the user, so there is little knowledge required to use the product: basics of running Kubernetes cluster in the public cloud (AWS, GCP, AKS) working knowledge of kubectl command line utility for creating and managing Kubernetes clusters Community \u00b6 We invite you to join our community and find out what is trending in the atmosphere; connect with our engineers and support, monitor release-notes, explore technical discussions, and more: Join our Slack channel Events \u00b6 The CAST AI team loves sharing knowledge and experience in webinars! If you would like to learn more about our journey and have your questions answered by our CTO and engineers LIVE - please check out our events calendar . Support \u00b6 Feel free to ask any questions on slack. You can also reach our support on the website chat or email: support@cast.ai","title":"Introduction"},{"location":"#welcome-to-cast-ai","text":"It is one platform that cuts your cloud costs, boosts DevOps, and prevents downtime.","title":"Welcome to CAST AI"},{"location":"#easy-to-start","text":"First, connect your cluster to CAST AI and see how much you can save by optimizing cluster configuration. After exploring the available savings and cost reports, you can onboard the cluster into CAST AI and set the Autoscaler policies, which will manage the cluster for you. It takes less than 5 minutes to fully onboard a cluster, please follow the guide to learn more . CAST AI abstracts layers of technical complexity from the user, so there is little knowledge required to use the product: basics of running Kubernetes cluster in the public cloud (AWS, GCP, AKS) working knowledge of kubectl command line utility for creating and managing Kubernetes clusters","title":"Easy to start"},{"location":"#community","text":"We invite you to join our community and find out what is trending in the atmosphere; connect with our engineers and support, monitor release-notes, explore technical discussions, and more: Join our Slack channel","title":"Community"},{"location":"#events","text":"The CAST AI team loves sharing knowledge and experience in webinars! If you would like to learn more about our journey and have your questions answered by our CTO and engineers LIVE - please check out our events calendar .","title":"Events"},{"location":"#support","text":"Feel free to ask any questions on slack. You can also reach our support on the website chat or email: support@cast.ai","title":"Support"},{"location":"administration/configure-payment-method/","text":"Setup payment method \u00b6 In order to set up a default payment method for your CAST AI subscription perform, take the following steps: From your account menu, select Manage subscription option. In the Manage subscriptions pop-up, choose Payment Methods . Enter all the required details and submit the form by pressing Add button. To learn more about the CAST AI subscription model, visit our pricing page .","title":"Configure payment details"},{"location":"administration/configure-payment-method/#setup-payment-method","text":"In order to set up a default payment method for your CAST AI subscription perform, take the following steps: From your account menu, select Manage subscription option. In the Manage subscriptions pop-up, choose Payment Methods . Enter all the required details and submit the form by pressing Add button. To learn more about the CAST AI subscription model, visit our pricing page .","title":"Setup payment method"},{"location":"api/authentication/","text":"Authentication \u00b6 Before you can use our API, either with your preferred REST client or via Terraform, you will need an API key. Obtaining API access key \u00b6 From the top menu in the CAST AI console, open API | API access keys , select create access key and name your key: We advise using the descriptive name for your intended purpose - it will be easier to distinguish which key is used for which integration if you add more keys later. Important When the key is created - save it because you will not be able to view the key again after this window is closed . The reason API key value is visible only at the time of creation is that we do not store the key in plain text on our system. For security reasons, CAST AI \"forgets\" key value after giving it to you, and later is only able to verify if the key is valid, but not to re-retrieve the value for you. If you lose your key, the only solution is to create a new key. CAST AI Swagger setup \u00b6 You can test your key directly in our API specification . Visit https://api.cast.ai/v1/spec/ , click \"Authorize\" and enter your key for X-API-Key field. After setting this up, you are now ready to use the \"Try it out\" button that is available for each endpoint. Using keys in API calls \u00b6 To authenticate, provide the key in X-API-Key HTTP header. For example, for curl this would be: curl -X GET \"https://api.cast.ai/v1/kubernetes/clusters\" -H \"X-API-Key: your-api-key-here\" | jq","title":"Authentication"},{"location":"api/authentication/#authentication","text":"Before you can use our API, either with your preferred REST client or via Terraform, you will need an API key.","title":"Authentication"},{"location":"api/authentication/#obtaining-api-access-key","text":"From the top menu in the CAST AI console, open API | API access keys , select create access key and name your key: We advise using the descriptive name for your intended purpose - it will be easier to distinguish which key is used for which integration if you add more keys later. Important When the key is created - save it because you will not be able to view the key again after this window is closed . The reason API key value is visible only at the time of creation is that we do not store the key in plain text on our system. For security reasons, CAST AI \"forgets\" key value after giving it to you, and later is only able to verify if the key is valid, but not to re-retrieve the value for you. If you lose your key, the only solution is to create a new key.","title":"Obtaining API access key"},{"location":"api/authentication/#cast-ai-swagger-setup","text":"You can test your key directly in our API specification . Visit https://api.cast.ai/v1/spec/ , click \"Authorize\" and enter your key for X-API-Key field. After setting this up, you are now ready to use the \"Try it out\" button that is available for each endpoint.","title":"CAST AI Swagger setup"},{"location":"api/authentication/#using-keys-in-api-calls","text":"To authenticate, provide the key in X-API-Key HTTP header. For example, for curl this would be: curl -X GET \"https://api.cast.ai/v1/kubernetes/clusters\" -H \"X-API-Key: your-api-key-here\" | jq","title":"Using keys in API calls"},{"location":"api/cli/","text":"CLI \u00b6 Your CAST.AI infrastructure can be automated via CAST AI Command Line Interface (CLI). Installation steps, examples, and releases are available at the repository: https://github.com/castai/cli .","title":"CLI"},{"location":"api/cli/#cli","text":"Your CAST.AI infrastructure can be automated via CAST AI Command Line Interface (CLI). Installation steps, examples, and releases are available at the repository: https://github.com/castai/cli .","title":"CLI"},{"location":"api/overview/","text":"Accessing CAST AI services via API \u00b6 Overview \u00b6 We build our services at CAST AI API-first; anything you can do in our console UI is available via REST API. You can use either your preferred way to call REST services directly or leverage our Terraform plugin to automate your infrastructure provisioning.","title":"Introduction"},{"location":"api/overview/#accessing-cast-ai-services-via-api","text":"","title":"Accessing CAST AI services via API"},{"location":"api/overview/#overview","text":"We build our services at CAST AI API-first; anything you can do in our console UI is available via REST API. You can use either your preferred way to call REST services directly or leverage our Terraform plugin to automate your infrastructure provisioning.","title":"Overview"},{"location":"api/specification/","text":"API specification \u00b6 Our API contract is published as OpenAPI v3 specification. You can check it on our Swagger UI: https://api.cast.ai/v1/spec/ This will bring you to our current specification. Here you will be able to familiarize yourself with available APIs and try functionality directly in the browser. To try out APIs in the browser you will need an API access key. See Authentication . We do not maintain any public SDKs but you can generate an API client for your programming language using many of the OpenAPI generators . Use below JSON as a spec: https://api.cast.ai/v1/spec/openapi.json OpenAPI is widely supported. Many tools (e.g. Postman) allow importing OpenAPI definitions as well. See documentation for your REST tooling to find out more.","title":"Specification"},{"location":"api/specification/#api-specification","text":"Our API contract is published as OpenAPI v3 specification. You can check it on our Swagger UI: https://api.cast.ai/v1/spec/ This will bring you to our current specification. Here you will be able to familiarize yourself with available APIs and try functionality directly in the browser. To try out APIs in the browser you will need an API access key. See Authentication . We do not maintain any public SDKs but you can generate an API client for your programming language using many of the OpenAPI generators . Use below JSON as a spec: https://api.cast.ai/v1/spec/openapi.json OpenAPI is widely supported. Many tools (e.g. Postman) allow importing OpenAPI definitions as well. See documentation for your REST tooling to find out more.","title":"API specification"},{"location":"api/terraform-provider/","text":"Terraform provider \u00b6 Your CAST.AI infrastructure can be automated via Terraform using terraform-provider-castai provider. Installation steps, example projects, and releases are available at the repository: https://github.com/castai/terraform-provider-castai .","title":"Terraform provider"},{"location":"api/terraform-provider/#terraform-provider","text":"Your CAST.AI infrastructure can be automated via Terraform using terraform-provider-castai provider. Installation steps, example projects, and releases are available at the repository: https://github.com/castai/terraform-provider-castai .","title":"Terraform provider"},{"location":"getting-started/overview/","text":"Overview \u00b6 CAST AI offers a variaty of advanced cost monitoring and autoscaling features to EKS, GKE, AKS and kOps clusters. By installing the CAST AI agent , you can start monitoring the running costs and potential savings of your cluster - and then enable the features that optimize your cluster. To get started, log into the console and navigate to the Connect cluster window. The script will install the agent that will run inside the cluster in read-only mode. After the installation, the agent will collect and analyze your cluster configuration to provide the most optimal setup along with a savings estimation for your current cloud environment. To start saving costs, turn the automatic optimization on when you're ready. Connect your cluster: AWS EKS GCP GKE Azure AKS kOps","title":"Overview"},{"location":"getting-started/overview/#overview","text":"CAST AI offers a variaty of advanced cost monitoring and autoscaling features to EKS, GKE, AKS and kOps clusters. By installing the CAST AI agent , you can start monitoring the running costs and potential savings of your cluster - and then enable the features that optimize your cluster. To get started, log into the console and navigate to the Connect cluster window. The script will install the agent that will run inside the cluster in read-only mode. After the installation, the agent will collect and analyze your cluster configuration to provide the most optimal setup along with a savings estimation for your current cloud environment. To start saving costs, turn the automatic optimization on when you're ready. Connect your cluster: AWS EKS GCP GKE Azure AKS kOps","title":"Overview"},{"location":"getting-started/aks/aks/","text":"Azure AKS \u00b6 Connect cluster \u00b6 To connect your cluster, log into the CAST AI console and navigate to the Connect cluster window, AKS tab. Copy the provided script and run it in your terminal or cloud shell. Make sure that kubectl is installed and can access your cluster. The script will create the following Kubernetes objects related to castai-agent agent: namespace and deployment, serviceaccount and secret, clusterrole and clusterrolebinding, role and rolebinding, resourcequota, configmap. After installation, your cluster's name will appear below the connection instructions as well as in the Cluster list . From there, you can open the cluster details and explore a detailed savings estimate based on your cluster configuration. The agent will run in a read-only mode, providing savings suggestions without applying any actual modifications. Credential onboarding \u00b6 To unlock all the benefits and enable automated cost optimization, CAST AI needs to have access to your cluster. The following section describes the steps required to onboard the AKS cluster on the CAST AI console. To make it less troublesome, we created a script that automates most of the steps. Prerequisites: az CLI - A command line tool for working with Azure services using commands in your command-line shell. For more information, see Installing az CLI . jq \u2013 a lightweight command line JSON processor. For more information about the tool, click here . Azure AD permissions \u2013 permissions to create App registration. The CAST AI agent has to be running on the cluster. Onboarding steps: To onboard your cluster, go to the Available Savings report and click on the Start saving or Enable optimization button. That\u2019s it! Your cluster is onboarded. Now you can enable the CAST AI Autoscaler to keep your cluster configuration optimal. Due to various reasons, it may sometimes take longer to validate the Azure credentials. Multiple re-runs of the onboarding script may be required. Actions performed by the onboarding script \u00b6 The script will perform the following actions: Create CASTAKSRole-*cluster-id* role used to manged onboarded AKS Cluster with following permissions ROLE_NAME = \"CastAKSRole- ${ CASTAI_CLUSTER_ID : 0 : 8 } \" ROLE_DEF = '{ \"Name\": \"' \" $ROLE_NAME \" '\", \"Description\": \"CAST.AI role used to manage ' \" $CLUSTER_NAME \" ' AKS cluster\", \"IsCustom\": true, \"Actions\": [ \"Microsoft.Compute/*/read\", \"Microsoft.Compute/virtualMachines/*\", \"Microsoft.Compute/virtualMachineScaleSets/*\", \"Microsoft.Compute/disks/write\", \"Microsoft.Compute/disks/delete\", \"Microsoft.Network/*/read\", \"Microsoft.Network/networkInterfaces/write\", \"Microsoft.Network/networkInterfaces/delete\", \"Microsoft.Network/networkInterfaces/join/action\", \"Microsoft.Network/networkSecurityGroups/join/action\", \"Microsoft.Network/publicIPAddresses/write\", \"Microsoft.Network/publicIPAddresses/delete\", \"Microsoft.Network/publicIPAddresses/join/action\", \"Microsoft.Network/virtualNetworks/subnets/join/action\", \"Microsoft.Network/virtualNetworks/subnets/write\", \"Microsoft.Network/applicationGateways/backendhealth/action\", \"Microsoft.Network/applicationGateways/backendAddressPools/join/action\", \"Microsoft.Network/applicationSecurityGroups/joinIpConfiguration/action\", \"Microsoft.Network/loadBalancers/backendAddressPools/write\", \"Microsoft.Network/loadBalancers/backendAddressPools/join/action\", \"Microsoft.ContainerService/*/read\", \"Microsoft.ContainerService/managedClusters/start/action\", \"Microsoft.ContainerService/managedClusters/stop/action\", \"Microsoft.ContainerService/managedClusters/runCommand/action\", \"Microsoft.ContainerService/managedClusters/agentPools/*\", \"Microsoft.Resources/*/read\", \"Microsoft.Resources/tags/write\", \"Microsoft.Authorization/locks/read\", \"Microsoft.Authorization/roleAssignments/read\", \"Microsoft.Authorization/roleDefinitions/read\", \"Microsoft.ManagedIdentity/userAssignedIdentities/assign/action\" ], \"AssignableScopes\": [ \"/subscriptions/' \" $SUBSCRIPTION_ID \" '/resourceGroups/' \" $CLUSTER_GROUP \" '\", \"/subscriptions/' \" $SUBSCRIPTION_ID \" '/resourceGroups/' \" $NODE_GROUP \" '\" ] }' Create app registration CAST.AI ${CLUSTER_NAME}-${CASTAI_CLUSTER_ID:0:8}\" which uses the role CastAKSRole-${CASTAI_CLUSTER_ID:0:8} All the Write permissions are scoped to a resource groups in which the cluster is running - it won't have access to resources of any other clusters in the Azure subscription. Kubernetes components required for a successful experience with CAST AI $ kubectl get deployments.apps -n castai-agent NAME READY UP-TO-DATE AVAILABLE AGE castai-agent 1 /1 1 1 3h26m castai-agent-cpvpa 1 /1 1 1 3h26m castai-cluster-controller 2 /2 2 2 3h26m castai-evictor 0 /0 0 0 3h26m $ kubectl get daemonsets.apps -n castai-agent NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE castai-aks-init-data 0 0 0 0 0 provisioner.cast.ai/aks-init-data = true 3h26m castai-spot-handler 0 0 0 0 0 scheduling.cast.ai/spot = true 3h26m Full overview of hosted components can be found here . Azure Agent Pools created by CAST AI \u00b6 After cluster is onboarded CAST AI will create two AKS agent pools : castpool - is used to run aks-init-data DaemonSet to gather necessary data required for CAST AI managed AKS nodes creation. More on aks-init-data DaemonSet can be found here . castworkers - is used as a container for CAST AI managed AKS nodes. Removal of this agent pool would result in removal of all CAST AI created nodes.","title":"Connect and onboard AKS"},{"location":"getting-started/aks/aks/#azure-aks","text":"","title":"Azure AKS"},{"location":"getting-started/aks/aks/#connect-cluster","text":"To connect your cluster, log into the CAST AI console and navigate to the Connect cluster window, AKS tab. Copy the provided script and run it in your terminal or cloud shell. Make sure that kubectl is installed and can access your cluster. The script will create the following Kubernetes objects related to castai-agent agent: namespace and deployment, serviceaccount and secret, clusterrole and clusterrolebinding, role and rolebinding, resourcequota, configmap. After installation, your cluster's name will appear below the connection instructions as well as in the Cluster list . From there, you can open the cluster details and explore a detailed savings estimate based on your cluster configuration. The agent will run in a read-only mode, providing savings suggestions without applying any actual modifications.","title":"Connect cluster"},{"location":"getting-started/aks/aks/#credential-onboarding","text":"To unlock all the benefits and enable automated cost optimization, CAST AI needs to have access to your cluster. The following section describes the steps required to onboard the AKS cluster on the CAST AI console. To make it less troublesome, we created a script that automates most of the steps. Prerequisites: az CLI - A command line tool for working with Azure services using commands in your command-line shell. For more information, see Installing az CLI . jq \u2013 a lightweight command line JSON processor. For more information about the tool, click here . Azure AD permissions \u2013 permissions to create App registration. The CAST AI agent has to be running on the cluster. Onboarding steps: To onboard your cluster, go to the Available Savings report and click on the Start saving or Enable optimization button. That\u2019s it! Your cluster is onboarded. Now you can enable the CAST AI Autoscaler to keep your cluster configuration optimal. Due to various reasons, it may sometimes take longer to validate the Azure credentials. Multiple re-runs of the onboarding script may be required.","title":"Credential onboarding"},{"location":"getting-started/aks/aks/#actions-performed-by-the-onboarding-script","text":"The script will perform the following actions: Create CASTAKSRole-*cluster-id* role used to manged onboarded AKS Cluster with following permissions ROLE_NAME = \"CastAKSRole- ${ CASTAI_CLUSTER_ID : 0 : 8 } \" ROLE_DEF = '{ \"Name\": \"' \" $ROLE_NAME \" '\", \"Description\": \"CAST.AI role used to manage ' \" $CLUSTER_NAME \" ' AKS cluster\", \"IsCustom\": true, \"Actions\": [ \"Microsoft.Compute/*/read\", \"Microsoft.Compute/virtualMachines/*\", \"Microsoft.Compute/virtualMachineScaleSets/*\", \"Microsoft.Compute/disks/write\", \"Microsoft.Compute/disks/delete\", \"Microsoft.Network/*/read\", \"Microsoft.Network/networkInterfaces/write\", \"Microsoft.Network/networkInterfaces/delete\", \"Microsoft.Network/networkInterfaces/join/action\", \"Microsoft.Network/networkSecurityGroups/join/action\", \"Microsoft.Network/publicIPAddresses/write\", \"Microsoft.Network/publicIPAddresses/delete\", \"Microsoft.Network/publicIPAddresses/join/action\", \"Microsoft.Network/virtualNetworks/subnets/join/action\", \"Microsoft.Network/virtualNetworks/subnets/write\", \"Microsoft.Network/applicationGateways/backendhealth/action\", \"Microsoft.Network/applicationGateways/backendAddressPools/join/action\", \"Microsoft.Network/applicationSecurityGroups/joinIpConfiguration/action\", \"Microsoft.Network/loadBalancers/backendAddressPools/write\", \"Microsoft.Network/loadBalancers/backendAddressPools/join/action\", \"Microsoft.ContainerService/*/read\", \"Microsoft.ContainerService/managedClusters/start/action\", \"Microsoft.ContainerService/managedClusters/stop/action\", \"Microsoft.ContainerService/managedClusters/runCommand/action\", \"Microsoft.ContainerService/managedClusters/agentPools/*\", \"Microsoft.Resources/*/read\", \"Microsoft.Resources/tags/write\", \"Microsoft.Authorization/locks/read\", \"Microsoft.Authorization/roleAssignments/read\", \"Microsoft.Authorization/roleDefinitions/read\", \"Microsoft.ManagedIdentity/userAssignedIdentities/assign/action\" ], \"AssignableScopes\": [ \"/subscriptions/' \" $SUBSCRIPTION_ID \" '/resourceGroups/' \" $CLUSTER_GROUP \" '\", \"/subscriptions/' \" $SUBSCRIPTION_ID \" '/resourceGroups/' \" $NODE_GROUP \" '\" ] }' Create app registration CAST.AI ${CLUSTER_NAME}-${CASTAI_CLUSTER_ID:0:8}\" which uses the role CastAKSRole-${CASTAI_CLUSTER_ID:0:8} All the Write permissions are scoped to a resource groups in which the cluster is running - it won't have access to resources of any other clusters in the Azure subscription. Kubernetes components required for a successful experience with CAST AI $ kubectl get deployments.apps -n castai-agent NAME READY UP-TO-DATE AVAILABLE AGE castai-agent 1 /1 1 1 3h26m castai-agent-cpvpa 1 /1 1 1 3h26m castai-cluster-controller 2 /2 2 2 3h26m castai-evictor 0 /0 0 0 3h26m $ kubectl get daemonsets.apps -n castai-agent NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE castai-aks-init-data 0 0 0 0 0 provisioner.cast.ai/aks-init-data = true 3h26m castai-spot-handler 0 0 0 0 0 scheduling.cast.ai/spot = true 3h26m Full overview of hosted components can be found here .","title":"Actions performed by the onboarding script"},{"location":"getting-started/aks/aks/#azure-agent-pools-created-by-cast-ai","text":"After cluster is onboarded CAST AI will create two AKS agent pools : castpool - is used to run aks-init-data DaemonSet to gather necessary data required for CAST AI managed AKS nodes creation. More on aks-init-data DaemonSet can be found here . castworkers - is used as a container for CAST AI managed AKS nodes. Removal of this agent pool would result in removal of all CAST AI created nodes.","title":"Azure Agent Pools created by CAST AI"},{"location":"getting-started/aks/remove-from-aks/","text":"Remove CAST resources from AKS cluster \u00b6 Based on the way how CAST was used on a cluster there are two options to remove CAST resources. Disconnect read only agent \u00b6 In order to disconnect your cluster from CAST AI click Disconnect cluster button in Clusters list and follow the guidance. Alternatively run following command from your terminal used to access the cluster: kubectl delete namespace castai-agent On top of that, also delete following kubernetes objects related to castai-agent agent: clusterrole.rbac.authorization.k8s.io/castai-agent clusterrolebinding.rbac.authorization.k8s.io/castai-agent Once cluster is disconnected its Status will change to Disconnected and you can choose to remove it from console by pressing Delete cluster button. Cluster will continue to run as normal, since Delete cluster action only removes it from CAST AI console. Removing CAST AI credentials and other resources \u00b6 When CAST was used to to optimize AKS cluster following resources were created: CAST AI agent deployment Cluster controller Spot handler Evictor Custom role App registration and secret Service principal To remove them follow the steps outlined below. Prerequisites \u00b6 In order to remove these resources first of all: Go to CAST AI console \u2192 Autoscaler page \u2192 Disable all CAST AI policies Disconnect the cluster by clicking Disconnect cluster button and following the guidance With above mentioned pre-requisites completed please follow next steps in Azure portal to remove CAST resources from your cluster: Delete node pools \u00b6 Go to Kubernetes services \u2192 your cluster \u2192 Node pools \u2192 find 2 node pools named \"castpool\" and \"castworkers\" and delete them. Delete app registration \u00b6 Go to App registrations \u2192 Search for \"CAST.AI cluster-name \" application and delete it.","title":"Remove CAST AI from AKS"},{"location":"getting-started/aks/remove-from-aks/#remove-cast-resources-from-aks-cluster","text":"Based on the way how CAST was used on a cluster there are two options to remove CAST resources.","title":"Remove CAST resources from AKS cluster"},{"location":"getting-started/aks/remove-from-aks/#disconnect-read-only-agent","text":"In order to disconnect your cluster from CAST AI click Disconnect cluster button in Clusters list and follow the guidance. Alternatively run following command from your terminal used to access the cluster: kubectl delete namespace castai-agent On top of that, also delete following kubernetes objects related to castai-agent agent: clusterrole.rbac.authorization.k8s.io/castai-agent clusterrolebinding.rbac.authorization.k8s.io/castai-agent Once cluster is disconnected its Status will change to Disconnected and you can choose to remove it from console by pressing Delete cluster button. Cluster will continue to run as normal, since Delete cluster action only removes it from CAST AI console.","title":"Disconnect read only agent"},{"location":"getting-started/aks/remove-from-aks/#removing-cast-ai-credentials-and-other-resources","text":"When CAST was used to to optimize AKS cluster following resources were created: CAST AI agent deployment Cluster controller Spot handler Evictor Custom role App registration and secret Service principal To remove them follow the steps outlined below.","title":"Removing CAST AI credentials and other resources"},{"location":"getting-started/aks/remove-from-aks/#prerequisites","text":"In order to remove these resources first of all: Go to CAST AI console \u2192 Autoscaler page \u2192 Disable all CAST AI policies Disconnect the cluster by clicking Disconnect cluster button and following the guidance With above mentioned pre-requisites completed please follow next steps in Azure portal to remove CAST resources from your cluster:","title":"Prerequisites"},{"location":"getting-started/aks/remove-from-aks/#delete-node-pools","text":"Go to Kubernetes services \u2192 your cluster \u2192 Node pools \u2192 find 2 node pools named \"castpool\" and \"castworkers\" and delete them.","title":"Delete node pools"},{"location":"getting-started/aks/remove-from-aks/#delete-app-registration","text":"Go to App registrations \u2192 Search for \"CAST.AI cluster-name \" application and delete it.","title":"Delete app registration"},{"location":"getting-started/eks/eks/","text":"AWS EKS \u00b6 Connect cluster \u00b6 To connect your cluster, log into the CAST AI console and navigate to the Connect cluster window, EKS tab. Copy the provided script and run it in your terminal or cloud shell. Make sure that kubectl is installed and can access your cluster. The script will create the following Kubernetes objects related to the castai-agent agent: namespace and deployment serviceaccount and secret clusterrole and clusterrolebinding role and rolebinding resourcequota configmap After installation, your cluster name will appear below connection instructions as well as in the Cluster list . From there, you can open the cluster details and explore a detailed savings estimate based on your cluster configuration. The agent will run in a read-only mode, providing savings suggestions without applying any modifications. Credential onboarding \u00b6 To unlock all the benefits and enable automated cost optimization, CAST AI needs to have access to your cluster. The following section describes the steps required to onboard the EKS cluster on the CAST AI console. To make it less troublesome, we created a script that automates most of the steps. Prerequisites: AWS CLI - A command line tool for working with AWS services using commands in your command-line shell. For more information, see Installing AWS CLI . jq \u2013 a lightweight command line JSON processor. For more information about the tool click here . IAM permissions \u2013 The IAM security principal that you're using must have permissions to work with AWS EKS, AWS IAM, and related resources. Additionally, you should have access to the EKS cluster that you wish to onboard on the CAST AI console. Example of least priveleged policy for administrator account (permissions needed to run onboarding script, used once per cluster during onboarding) { \"Action\" : [ \"iam:CreateRole\" , \"iam:CreatePolicy\" , \"iam:GetPolicy\" , \"iam:ListPolicyVersions\" , \"iam:PutRolePolicy\" , \"iam:AttachRolePolicy\" , \"iam:CreateInstanceProfile\" , \"iam:GetInstanceProfile\" , \"iam:AddRoleToInstanceProfile\" , \"iam:UpdateAssumeRolePolicy\" ] } The CAST AI agent has to be running on the cluster. Onboarding steps: To onboard your cluster, go to the Available Savings report and click on the Start saving or Enable CAST AI button. The following pop-up window contains the instructions for providing CAST AI with AWS access. By default, the script will create a AccessKeyId and SecretAccessKey . If Use cross-role IAM checkbox is selected, the script will create a role in your AWS account, with a trust policy to a CAST AI AWS user, allowing access through the STS AssumeRole API. That\u2019s it! Your cluster is onboarded. Now you can enable CAST AI Autoscaler to keep your cluster configuration optimal. Actions performed by the onboarding script \u00b6 The script will perform the following actions: Create cast-eks-*cluster-name* IAM user (if cross-account Role IAM is selected, an IAM role is created instead), with the required permissions to manage the cluster: AmazonEC2ReadOnlyAccess IAMReadOnlyAccess Manage instances in specified cluster restricted to cluster VPC Manage autoscaling groups in the specified cluster Manage EKS Node Groups in the specified cluster Create CastEKSPolicy policy used to manage EKS cluster. The policy contains the following permissions: Create & delete instance profiles Create & manage roles Create & manage EC2 security groups, key pairs, and tags Run EC2 instances Create following roles: cast-*cluster-name*-eks-####### to manage EKS nodes with following AWS managed permission policies applied : AmazonEKSWorkerNodePolicy AmazonEC2ContainterRegistryReadOnly AmazonEKS_CNI_Policy Modify aws-auth ConfigMap to map newly created IAM user to the cluster (skipped in case of cross-role IAM) If a cross-account role IAM was not selected, AWS AccessKeyId and SecretAccessKey are created and printed, which then can be added to the CAST AI console and assigned to the corresponding EKS cluster. The AccessKeyId and SecretAccessKey are used to by CAST to make programmatic calls to AWS and are stored in CAST AI's secret store that runs on Google's Secret manager solution . With cross-account role IAM selected, a Role ARN is printed and sent to CAST AI console, which is then used by CAST AI to assume the role when making AWS programmatic calls. All the Write permissions are scoped to a single EKS cluster - it won't have access to resources of any other clusters in the AWS account. Manual credential onboarding \u00b6 To complete the steps mentioned above manually (without our script), be aware that when you create an Amazon EKS cluster, the IAM entity user or role (such as a federated user that creates the cluster) is automatically granted a system:masters permissions in the cluster's RBAC configuration in the control plane. To grant additional AWS users or roles the ability to interact with your cluster, you need to edit the aws-auth ConfigMap within Kubernetes. For more information, see Managing users or IAM roles for your cluster . Usage of AWS services \u00b6 CAST AI relies on the agent runs inside customer's cluster. The following services are consumed during the operation: A portion of EC2 node resources from the customer's cluster. The CAST AI agent uses Cluster proportional vertical autoscaler to consume a minimum required resources depending on the size of the cluster Low amount of network traffic to communicate with CAST AI SaaS EC2 instances, their storage, and intra-cluster network traffic to manage Kubernetes cluster and perform autoscaling IAM resources as detailed in the onboarding section","title":"Connect and onboard EKS"},{"location":"getting-started/eks/eks/#aws-eks","text":"","title":"AWS EKS"},{"location":"getting-started/eks/eks/#connect-cluster","text":"To connect your cluster, log into the CAST AI console and navigate to the Connect cluster window, EKS tab. Copy the provided script and run it in your terminal or cloud shell. Make sure that kubectl is installed and can access your cluster. The script will create the following Kubernetes objects related to the castai-agent agent: namespace and deployment serviceaccount and secret clusterrole and clusterrolebinding role and rolebinding resourcequota configmap After installation, your cluster name will appear below connection instructions as well as in the Cluster list . From there, you can open the cluster details and explore a detailed savings estimate based on your cluster configuration. The agent will run in a read-only mode, providing savings suggestions without applying any modifications.","title":"Connect cluster"},{"location":"getting-started/eks/eks/#credential-onboarding","text":"To unlock all the benefits and enable automated cost optimization, CAST AI needs to have access to your cluster. The following section describes the steps required to onboard the EKS cluster on the CAST AI console. To make it less troublesome, we created a script that automates most of the steps. Prerequisites: AWS CLI - A command line tool for working with AWS services using commands in your command-line shell. For more information, see Installing AWS CLI . jq \u2013 a lightweight command line JSON processor. For more information about the tool click here . IAM permissions \u2013 The IAM security principal that you're using must have permissions to work with AWS EKS, AWS IAM, and related resources. Additionally, you should have access to the EKS cluster that you wish to onboard on the CAST AI console. Example of least priveleged policy for administrator account (permissions needed to run onboarding script, used once per cluster during onboarding) { \"Action\" : [ \"iam:CreateRole\" , \"iam:CreatePolicy\" , \"iam:GetPolicy\" , \"iam:ListPolicyVersions\" , \"iam:PutRolePolicy\" , \"iam:AttachRolePolicy\" , \"iam:CreateInstanceProfile\" , \"iam:GetInstanceProfile\" , \"iam:AddRoleToInstanceProfile\" , \"iam:UpdateAssumeRolePolicy\" ] } The CAST AI agent has to be running on the cluster. Onboarding steps: To onboard your cluster, go to the Available Savings report and click on the Start saving or Enable CAST AI button. The following pop-up window contains the instructions for providing CAST AI with AWS access. By default, the script will create a AccessKeyId and SecretAccessKey . If Use cross-role IAM checkbox is selected, the script will create a role in your AWS account, with a trust policy to a CAST AI AWS user, allowing access through the STS AssumeRole API. That\u2019s it! Your cluster is onboarded. Now you can enable CAST AI Autoscaler to keep your cluster configuration optimal.","title":"Credential onboarding"},{"location":"getting-started/eks/eks/#actions-performed-by-the-onboarding-script","text":"The script will perform the following actions: Create cast-eks-*cluster-name* IAM user (if cross-account Role IAM is selected, an IAM role is created instead), with the required permissions to manage the cluster: AmazonEC2ReadOnlyAccess IAMReadOnlyAccess Manage instances in specified cluster restricted to cluster VPC Manage autoscaling groups in the specified cluster Manage EKS Node Groups in the specified cluster Create CastEKSPolicy policy used to manage EKS cluster. The policy contains the following permissions: Create & delete instance profiles Create & manage roles Create & manage EC2 security groups, key pairs, and tags Run EC2 instances Create following roles: cast-*cluster-name*-eks-####### to manage EKS nodes with following AWS managed permission policies applied : AmazonEKSWorkerNodePolicy AmazonEC2ContainterRegistryReadOnly AmazonEKS_CNI_Policy Modify aws-auth ConfigMap to map newly created IAM user to the cluster (skipped in case of cross-role IAM) If a cross-account role IAM was not selected, AWS AccessKeyId and SecretAccessKey are created and printed, which then can be added to the CAST AI console and assigned to the corresponding EKS cluster. The AccessKeyId and SecretAccessKey are used to by CAST to make programmatic calls to AWS and are stored in CAST AI's secret store that runs on Google's Secret manager solution . With cross-account role IAM selected, a Role ARN is printed and sent to CAST AI console, which is then used by CAST AI to assume the role when making AWS programmatic calls. All the Write permissions are scoped to a single EKS cluster - it won't have access to resources of any other clusters in the AWS account.","title":"Actions performed by the onboarding script"},{"location":"getting-started/eks/eks/#manual-credential-onboarding","text":"To complete the steps mentioned above manually (without our script), be aware that when you create an Amazon EKS cluster, the IAM entity user or role (such as a federated user that creates the cluster) is automatically granted a system:masters permissions in the cluster's RBAC configuration in the control plane. To grant additional AWS users or roles the ability to interact with your cluster, you need to edit the aws-auth ConfigMap within Kubernetes. For more information, see Managing users or IAM roles for your cluster .","title":"Manual credential onboarding"},{"location":"getting-started/eks/eks/#usage-of-aws-services","text":"CAST AI relies on the agent runs inside customer's cluster. The following services are consumed during the operation: A portion of EC2 node resources from the customer's cluster. The CAST AI agent uses Cluster proportional vertical autoscaler to consume a minimum required resources depending on the size of the cluster Low amount of network traffic to communicate with CAST AI SaaS EC2 instances, their storage, and intra-cluster network traffic to manage Kubernetes cluster and perform autoscaling IAM resources as detailed in the onboarding section","title":"Usage of AWS services"},{"location":"getting-started/eks/remove-from-eks/","text":"Remove CAST resources from EKS cluster \u00b6 Based on the way how CAST was used on a cluster there are two options to remove CAST resources. Disconnect read only agent \u00b6 In order to disconnect your cluster from CAST AI click Disconnect cluster button in Clusters list and follow the guidance. Alternatively run following command from your terminal used to access the cluster: kubectl delete deployment castai-agent -n castai-agent On top of that, also delete following kubernetes objects related to castai-agent agent: namespace serviceaccount and secret clusterrole and clusterrolebinding role and rolebinding resourcequota configmap Once cluster is disconnected its Status will change to Disconnected and you can choose to remove it from console by pressing Delete cluster button. Cluster will continue to run as normal, since Delete cluster action only removes it from CAST AI console. Removing CAST AI credentials and other resources \u00b6 When CAST was used to to optimize EKS cluster following resources were created: CAST AI agent deployment User Policy Roles Instances Security group To remove them follow the steps outlined below. Prerequisites \u00b6 In order to remove these resources first of all: Go to CAST AI console \u2192 Autoscaler page \u2192 Disable all CAST AI policies Connect to your cluster from the terminal and run command: kubectl delete deployment castai-agent -n castai-agent With above mentioned pre-requisites completed please follow next steps in AWS Management Console to remove CAST resources from your running cluster: Delete user \u00b6 Go to AWS console \u2192 Identity and Access Management (IAM) \u2192 Users \u2192 find \u201ccast-eks- cluster-name \u201d user. Select the user and click Delete user. Delete policy \u00b6 Go to AWS console \u2192 Identity and Access Management (IAM) \u2192 Customer managed policies \u2192 find CastEKSPolicy. Mark it, go to Policy actions \u2192 Delete policy. Delete CAST role \u00b6 Go to AWS console \u2192 Identity and Access Management (IAM) \u2192 Roles \u2192 find role \"cast- cluster-name -eks-#######\", delete it. Delete instances \u00b6 Go to EC2 dashboard \u2192 Security groups \u2192 Instances \u2192 find CAST created instance(s) \u201c cluster-name -cast-#####\u2026\u201d. Terminate instance(s). If you have you used PODs with attached storage go to Volumes and delete Volumes attached to CAST AI created nodes. Delete security group \u00b6 Go to EC2 dashboard \u2192 Security groups \u2192 find \u201ccast- cluster-name -cluster/CastNodeSecurityGroup\u201d. Delete security group.","title":"Remove CAST AI from EKS"},{"location":"getting-started/eks/remove-from-eks/#remove-cast-resources-from-eks-cluster","text":"Based on the way how CAST was used on a cluster there are two options to remove CAST resources.","title":"Remove CAST resources from EKS cluster"},{"location":"getting-started/eks/remove-from-eks/#disconnect-read-only-agent","text":"In order to disconnect your cluster from CAST AI click Disconnect cluster button in Clusters list and follow the guidance. Alternatively run following command from your terminal used to access the cluster: kubectl delete deployment castai-agent -n castai-agent On top of that, also delete following kubernetes objects related to castai-agent agent: namespace serviceaccount and secret clusterrole and clusterrolebinding role and rolebinding resourcequota configmap Once cluster is disconnected its Status will change to Disconnected and you can choose to remove it from console by pressing Delete cluster button. Cluster will continue to run as normal, since Delete cluster action only removes it from CAST AI console.","title":"Disconnect read only agent"},{"location":"getting-started/eks/remove-from-eks/#removing-cast-ai-credentials-and-other-resources","text":"When CAST was used to to optimize EKS cluster following resources were created: CAST AI agent deployment User Policy Roles Instances Security group To remove them follow the steps outlined below.","title":"Removing CAST AI credentials and other resources"},{"location":"getting-started/eks/remove-from-eks/#prerequisites","text":"In order to remove these resources first of all: Go to CAST AI console \u2192 Autoscaler page \u2192 Disable all CAST AI policies Connect to your cluster from the terminal and run command: kubectl delete deployment castai-agent -n castai-agent With above mentioned pre-requisites completed please follow next steps in AWS Management Console to remove CAST resources from your running cluster:","title":"Prerequisites"},{"location":"getting-started/eks/remove-from-eks/#delete-user","text":"Go to AWS console \u2192 Identity and Access Management (IAM) \u2192 Users \u2192 find \u201ccast-eks- cluster-name \u201d user. Select the user and click Delete user.","title":"Delete user"},{"location":"getting-started/eks/remove-from-eks/#delete-policy","text":"Go to AWS console \u2192 Identity and Access Management (IAM) \u2192 Customer managed policies \u2192 find CastEKSPolicy. Mark it, go to Policy actions \u2192 Delete policy.","title":"Delete policy"},{"location":"getting-started/eks/remove-from-eks/#delete-cast-role","text":"Go to AWS console \u2192 Identity and Access Management (IAM) \u2192 Roles \u2192 find role \"cast- cluster-name -eks-#######\", delete it.","title":"Delete CAST role"},{"location":"getting-started/eks/remove-from-eks/#delete-instances","text":"Go to EC2 dashboard \u2192 Security groups \u2192 Instances \u2192 find CAST created instance(s) \u201c cluster-name -cast-#####\u2026\u201d. Terminate instance(s). If you have you used PODs with attached storage go to Volumes and delete Volumes attached to CAST AI created nodes.","title":"Delete instances"},{"location":"getting-started/eks/remove-from-eks/#delete-security-group","text":"Go to EC2 dashboard \u2192 Security groups \u2192 find \u201ccast- cluster-name -cluster/CastNodeSecurityGroup\u201d. Delete security group.","title":"Delete security group"},{"location":"getting-started/gke/gcp-marketplace-onboarding/","text":"How to start CAST AI cloud cost management platform on Google Cloud Marketplace \u00b6 The CAST AI platform is now available on the Google Cloud Marketplace. Using the new integration, customers can now access CAST AI directly from their cloud provider, eliminating the need to sign a new contract and taking advantage of their existing Google Cloud credits. Like other products available on the Google Cloud Marketplace, CAST AI meets Google\u2019s security and solution design requirements. How do you get started? Follow this step-by-step guide to access CAST AI from Google Cloud Marketplace. Requirements \u00b6 To onboard CAST AI using Google Cloud Marketplace, you need to have the following: access to Google Cloud Marketplace in your GCP project (with the rights to purchase solutions from the marketplace); access to CAST AI console. Finding the CAST AI offering in Google Cloud Marketplace \u00b6 Navigate to the Google Marketplace website and search for CAST AI. You can also use this link . This is the CAST AI offering: Subscribing to CAST AI in Google Cloud Marketplace \u00b6 Choose your plan and click SELECT. Agree to our terms of service and Subscribe. Before doing so, please make sure that your browser accepts popups from the Google Cloud Marketplace site. Registration in the CAST AI console \u00b6 The next step is registration inside CAST AI console page. You need click Register with CAST AI . After that, you should be redirected to the CAST AI console, where you need to log in to the organization to which the Google Marketplace subscription is assigned. Normal CAST AI login screen: Cluster view after successful login: Validation \u00b6 After completing all of the steps above, your CAST AI subscription should be in an active state. Quick guide to CAST AI pricing \u00b6 Users can choose from four different pricing plans on Google Cloud Marketplace: Cost reporting (free of charge and great to get started), Growth, Growth Pro, and Enterprise. Cost reporting - Free \u00b6 This is a free plan that lasts 1 month. It gives you real-time cost monitoring and reporting features, covering both current and historical cost data. You also get cost optimization recommendations to implement manually. Growth - USD 200.00/mo + usage fee \u00b6 This plan gives you access to cost monitoring and automated cost optimization powered by features such as automated rebalancing , real-time autoscaling , and spot instance automation with fallback. There\u2019s a limit on cluster number and CPU. Growth Pro - 1 year \u2013 USD 200.00/mo + usage fee \u00b6 This plan gives you all the features of the Growth plan without any limits around cluster number. There is a limit on CPUs. Enterprise - USD 5,000.00/mo + usage fee \u00b6 You get all of the cost monitoring and automated cost optimization features with no limits, with Platinum 24/7 support.We provide custom offers where monthly fee and tiered based pricing are subject to negotiation.","title":"Onboard Organization using GCP Marketplace"},{"location":"getting-started/gke/gcp-marketplace-onboarding/#how-to-start-cast-ai-cloud-cost-management-platform-on-google-cloud-marketplace","text":"The CAST AI platform is now available on the Google Cloud Marketplace. Using the new integration, customers can now access CAST AI directly from their cloud provider, eliminating the need to sign a new contract and taking advantage of their existing Google Cloud credits. Like other products available on the Google Cloud Marketplace, CAST AI meets Google\u2019s security and solution design requirements. How do you get started? Follow this step-by-step guide to access CAST AI from Google Cloud Marketplace.","title":"How to start CAST AI cloud cost management platform on Google Cloud Marketplace"},{"location":"getting-started/gke/gcp-marketplace-onboarding/#requirements","text":"To onboard CAST AI using Google Cloud Marketplace, you need to have the following: access to Google Cloud Marketplace in your GCP project (with the rights to purchase solutions from the marketplace); access to CAST AI console.","title":"Requirements"},{"location":"getting-started/gke/gcp-marketplace-onboarding/#finding-the-cast-ai-offering-in-google-cloud-marketplace","text":"Navigate to the Google Marketplace website and search for CAST AI. You can also use this link . This is the CAST AI offering:","title":"Finding the CAST AI offering in Google Cloud Marketplace"},{"location":"getting-started/gke/gcp-marketplace-onboarding/#subscribing-to-cast-ai-in-google-cloud-marketplace","text":"Choose your plan and click SELECT. Agree to our terms of service and Subscribe. Before doing so, please make sure that your browser accepts popups from the Google Cloud Marketplace site.","title":"Subscribing to CAST AI in Google Cloud Marketplace"},{"location":"getting-started/gke/gcp-marketplace-onboarding/#registration-in-the-cast-ai-console","text":"The next step is registration inside CAST AI console page. You need click Register with CAST AI . After that, you should be redirected to the CAST AI console, where you need to log in to the organization to which the Google Marketplace subscription is assigned. Normal CAST AI login screen: Cluster view after successful login:","title":"Registration in the CAST AI console"},{"location":"getting-started/gke/gcp-marketplace-onboarding/#validation","text":"After completing all of the steps above, your CAST AI subscription should be in an active state.","title":"Validation"},{"location":"getting-started/gke/gcp-marketplace-onboarding/#quick-guide-to-cast-ai-pricing","text":"Users can choose from four different pricing plans on Google Cloud Marketplace: Cost reporting (free of charge and great to get started), Growth, Growth Pro, and Enterprise.","title":"Quick guide to CAST AI pricing"},{"location":"getting-started/gke/gcp-marketplace-onboarding/#cost-reporting-free","text":"This is a free plan that lasts 1 month. It gives you real-time cost monitoring and reporting features, covering both current and historical cost data. You also get cost optimization recommendations to implement manually.","title":"Cost reporting - Free"},{"location":"getting-started/gke/gcp-marketplace-onboarding/#growth-usd-20000mo-usage-fee","text":"This plan gives you access to cost monitoring and automated cost optimization powered by features such as automated rebalancing , real-time autoscaling , and spot instance automation with fallback. There\u2019s a limit on cluster number and CPU.","title":"Growth - USD 200.00/mo + usage fee"},{"location":"getting-started/gke/gcp-marketplace-onboarding/#growth-pro-1-year-usd-20000mo-usage-fee","text":"This plan gives you all the features of the Growth plan without any limits around cluster number. There is a limit on CPUs.","title":"Growth Pro - 1 year \u2013 USD 200.00/mo + usage fee"},{"location":"getting-started/gke/gcp-marketplace-onboarding/#enterprise-usd-500000mo-usage-fee","text":"You get all of the cost monitoring and automated cost optimization features with no limits, with Platinum 24/7 support.We provide custom offers where monthly fee and tiered based pricing are subject to negotiation.","title":"Enterprise - USD 5,000.00/mo + usage fee"},{"location":"getting-started/gke/gke/","text":"GCP GKE \u00b6 Connect cluster \u00b6 To connect your cluster, log in to the CAST AI console and navigate to Connect cluster window, GKE tab. Copy the pre-generated script and run it inside your terminal or cloud shell. Make sure that kubectl is installed and can access your cluster. The script will create following kubernetes objects related to castai-agent agent: namespace and deployment serviceaccount and secret clusterrole and clusterrolebinding role and rolebinding resourcequota configmap After the installation, your cluster name will appear below connection instructions as well as in the Clusters list. From there, you can open the Available savings report and explore a detailed savings estimate based on your cluster configuration. The agent will run in read-only mode providing saving suggestions without any actual modifications. Credential onboarding \u00b6 To unlock all the benefits and enable automatic cost optimization, CAST AI must have access to your cluster. The following section describes the steps required to onboard the GKE cluster on the CAST AI console. To make it less troublesome, we created a script that automates most of the steps. Prerequisites: gcloud - A command line tool for working with GKE services using commands in your command-line shell. For more information, see Installing gcloud . IAM permissions \u2013 The IAM user that you're using must have: Access to the project where the cluster is created. Permissions to work with IAM, GKE, and compute resources. The CAST AI agent has to be running on the cluster. Onboarding steps: To onboard your cluster, go to the Available Savings report and click on the Start saving or Enable CAST AI button. The button's name will depend on the level of optimization available for your cluster. Copy the pre-generated script and run it inside your terminal or cloud shell. The script will create new GKE service account with the required roles. The generated user will have the following permissions: /roles/cast.gkeAccess (created by script) - access to get / update your GKE cluster and manage compute instances. roles/container.developer - access to resources within the Kubernetes cluster. That\u2019s it! Your cluster is onboarded. Now you can enable CAST AI Autoscaler to keep your cluster configuration optimal. Connect your cluster here Actions performed by the onboarding script \u00b6 The script will perform the following actions: Enables following GCP services and APIs for the project on which GKE cluster is running: GCP Service / API Group Description serviceusage.googleapis.com API to list, enable and disable GCP services iam.googleapis.com API to manage identity and access control for GCP resources cloudresourcemanager.googleapis.com API to create, read, and update metadata for GCP resource containers container.googleapis.com API to manage GKE compute.googleapis.com API to manage GCP virtual machines Creates a dedicated GCP service account castai-gke-<cluster-name-hash> used by CAST AI to request and manage GCP resources on customer's behalf. Creates a custom role castai.gkeAccess with following permissions: - compute.addresses.use - compute.disks.create - compute.disks.setLabels - compute.disks.use - compute.images.useReadOnly - compute.instanceGroupManagers.get - compute.instanceGroupManagers.update - compute.instanceGroups.get - compute.instanceTemplates.create - compute.instanceTemplates.delete - compute.instanceTemplates.get - compute.instanceTemplates.list - compute.instances.create - compute.instances.delete - compute.instances.get - compute.instances.list - compute.instances.setLabels - compute.instances.setMetadata - compute.instances.setServiceAccount - compute.instances.setTags - compute.instances.start - compute.instances.stop - compute.networks.use - compute.networks.useExternalIp - compute.subnetworks.get - compute.subnetworks.use - compute.subnetworks.useExternalIp - compute.zones.get - compute.zones.list - container.certificateSigningRequests.approve - container.clusters.get - container.clusters.update - container.operations.get - serviceusage.services.list Attaches following roles to castai-gke-<cluster-name-hash> service account: Role name Description castai.gkeAccess CAST AI managed role used to manage CAST AI add/delete node operations, full list of permissions listed above container.developer GCP managed role for full access to Kubernetes API objects inside Kubernetes cluster iam.serviceAccountUser GCP managed role to allow run operations as the service account Installs Kubernetes components required for a successful experience with CAST AI: $ kubectl get deployments.apps -n castai-agent NAME READY UP-TO-DATE AVAILABLE AGE castai-agent 1 /1 1 1 15m castai-agent-cpvpa 1 /1 1 1 15m castai-cluster-controller 2 /2 2 2 15m castai-evictor 0 /0 0 0 15m castai-kvisor 1 /1 1 1 15m $ kubectl get daemonsets.apps -n castai-agent NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE castai-spot-handler 0 0 0 0 0 scheduling.cast.ai/spot = true 15m Full overview of hosted components can be found here . GKE node pools created by CAST AI \u00b6 After cluster is onboarded CAST AI will create two GKE node pools: castpool - is used to gather necessary data required for CAST AI managed GKE x86 nodes creation castpool-arm - is used to gather necessary data required for CAST AI managed GKE ARM64 nodes creation. castpool-arm is created only if the cluster region support ARM64 VMs Disconnect GKE cluster \u00b6 In order to disconnect your cluster from CAST AI click Disconnect cluster button in Clusters list and follow the guidance. Alternatively, run the following command from your terminal used to access the cluster: kubectl delete deployment castai-agent -n castai-agent Once the cluster is disconnected, its Status will change to Disconnected and you can choose to remove it from the console by pressing the Delete cluster button. The cluster will continue to run as normal, since the Delete cluster action only removes it from CAST AI console.","title":"Connect and onboard GKE"},{"location":"getting-started/gke/gke/#gcp-gke","text":"","title":"GCP GKE"},{"location":"getting-started/gke/gke/#connect-cluster","text":"To connect your cluster, log in to the CAST AI console and navigate to Connect cluster window, GKE tab. Copy the pre-generated script and run it inside your terminal or cloud shell. Make sure that kubectl is installed and can access your cluster. The script will create following kubernetes objects related to castai-agent agent: namespace and deployment serviceaccount and secret clusterrole and clusterrolebinding role and rolebinding resourcequota configmap After the installation, your cluster name will appear below connection instructions as well as in the Clusters list. From there, you can open the Available savings report and explore a detailed savings estimate based on your cluster configuration. The agent will run in read-only mode providing saving suggestions without any actual modifications.","title":"Connect cluster"},{"location":"getting-started/gke/gke/#credential-onboarding","text":"To unlock all the benefits and enable automatic cost optimization, CAST AI must have access to your cluster. The following section describes the steps required to onboard the GKE cluster on the CAST AI console. To make it less troublesome, we created a script that automates most of the steps. Prerequisites: gcloud - A command line tool for working with GKE services using commands in your command-line shell. For more information, see Installing gcloud . IAM permissions \u2013 The IAM user that you're using must have: Access to the project where the cluster is created. Permissions to work with IAM, GKE, and compute resources. The CAST AI agent has to be running on the cluster. Onboarding steps: To onboard your cluster, go to the Available Savings report and click on the Start saving or Enable CAST AI button. The button's name will depend on the level of optimization available for your cluster. Copy the pre-generated script and run it inside your terminal or cloud shell. The script will create new GKE service account with the required roles. The generated user will have the following permissions: /roles/cast.gkeAccess (created by script) - access to get / update your GKE cluster and manage compute instances. roles/container.developer - access to resources within the Kubernetes cluster. That\u2019s it! Your cluster is onboarded. Now you can enable CAST AI Autoscaler to keep your cluster configuration optimal. Connect your cluster here","title":"Credential onboarding"},{"location":"getting-started/gke/gke/#actions-performed-by-the-onboarding-script","text":"The script will perform the following actions: Enables following GCP services and APIs for the project on which GKE cluster is running: GCP Service / API Group Description serviceusage.googleapis.com API to list, enable and disable GCP services iam.googleapis.com API to manage identity and access control for GCP resources cloudresourcemanager.googleapis.com API to create, read, and update metadata for GCP resource containers container.googleapis.com API to manage GKE compute.googleapis.com API to manage GCP virtual machines Creates a dedicated GCP service account castai-gke-<cluster-name-hash> used by CAST AI to request and manage GCP resources on customer's behalf. Creates a custom role castai.gkeAccess with following permissions: - compute.addresses.use - compute.disks.create - compute.disks.setLabels - compute.disks.use - compute.images.useReadOnly - compute.instanceGroupManagers.get - compute.instanceGroupManagers.update - compute.instanceGroups.get - compute.instanceTemplates.create - compute.instanceTemplates.delete - compute.instanceTemplates.get - compute.instanceTemplates.list - compute.instances.create - compute.instances.delete - compute.instances.get - compute.instances.list - compute.instances.setLabels - compute.instances.setMetadata - compute.instances.setServiceAccount - compute.instances.setTags - compute.instances.start - compute.instances.stop - compute.networks.use - compute.networks.useExternalIp - compute.subnetworks.get - compute.subnetworks.use - compute.subnetworks.useExternalIp - compute.zones.get - compute.zones.list - container.certificateSigningRequests.approve - container.clusters.get - container.clusters.update - container.operations.get - serviceusage.services.list Attaches following roles to castai-gke-<cluster-name-hash> service account: Role name Description castai.gkeAccess CAST AI managed role used to manage CAST AI add/delete node operations, full list of permissions listed above container.developer GCP managed role for full access to Kubernetes API objects inside Kubernetes cluster iam.serviceAccountUser GCP managed role to allow run operations as the service account Installs Kubernetes components required for a successful experience with CAST AI: $ kubectl get deployments.apps -n castai-agent NAME READY UP-TO-DATE AVAILABLE AGE castai-agent 1 /1 1 1 15m castai-agent-cpvpa 1 /1 1 1 15m castai-cluster-controller 2 /2 2 2 15m castai-evictor 0 /0 0 0 15m castai-kvisor 1 /1 1 1 15m $ kubectl get daemonsets.apps -n castai-agent NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE castai-spot-handler 0 0 0 0 0 scheduling.cast.ai/spot = true 15m Full overview of hosted components can be found here .","title":"Actions performed by the onboarding script"},{"location":"getting-started/gke/gke/#gke-node-pools-created-by-cast-ai","text":"After cluster is onboarded CAST AI will create two GKE node pools: castpool - is used to gather necessary data required for CAST AI managed GKE x86 nodes creation castpool-arm - is used to gather necessary data required for CAST AI managed GKE ARM64 nodes creation. castpool-arm is created only if the cluster region support ARM64 VMs","title":"GKE node pools created by CAST AI"},{"location":"getting-started/gke/gke/#disconnect-gke-cluster","text":"In order to disconnect your cluster from CAST AI click Disconnect cluster button in Clusters list and follow the guidance. Alternatively, run the following command from your terminal used to access the cluster: kubectl delete deployment castai-agent -n castai-agent Once the cluster is disconnected, its Status will change to Disconnected and you can choose to remove it from the console by pressing the Delete cluster button. The cluster will continue to run as normal, since the Delete cluster action only removes it from CAST AI console.","title":"Disconnect GKE cluster"},{"location":"getting-started/gke/remove-from-gke/","text":"Remove CAST resources from GKE cluster \u00b6 Based on the way how CAST was used on a cluster there are two options to remove CAST resources. Disconnect read only agent \u00b6 In order to disconnect your cluster from CAST AI click Disconnect cluster button in Clusters list and follow the guidance. Alternatively run following command from your terminal used to access the cluster: kubectl delete deployment castai-agent -n castai-agent On top of that, also delete following kubernetes objects related to castai-agent agent: namespace serviceaccount and secret clusterrole and clusterrolebinding role and rolebinding resourcequota configmap Once cluster is disconnected its Status will change to Disconnected and you can choose to remove it from console by pressing Delete cluster button. Cluster will continue to run as normal, since Delete cluster action only removes it from CAST AI console. Removing CAST AI credentials and other resources \u00b6 Prerequisites \u00b6 In order to remove these resources first of all: Go to CAST AI console \u2192 Autoscaler page \u2192 Disable all CAST AI policies Connect to your cluster from the terminal and run command: kubectl delete deployment castai-agent -n castai-agent With above mentioned pre-requisites completed please follow next steps in GCP Console to remove CAST resources from your running cluster: Delete Service Account \u00b6 Go to GCP console \u2192 Identity and Access Management (IAM) \u2192 Service Accounts \u2192 find Service account to manage *cluster-name* GKE cluster via CAST and delete it","title":"Remove CAST AI from GKE"},{"location":"getting-started/gke/remove-from-gke/#remove-cast-resources-from-gke-cluster","text":"Based on the way how CAST was used on a cluster there are two options to remove CAST resources.","title":"Remove CAST resources from GKE cluster"},{"location":"getting-started/gke/remove-from-gke/#disconnect-read-only-agent","text":"In order to disconnect your cluster from CAST AI click Disconnect cluster button in Clusters list and follow the guidance. Alternatively run following command from your terminal used to access the cluster: kubectl delete deployment castai-agent -n castai-agent On top of that, also delete following kubernetes objects related to castai-agent agent: namespace serviceaccount and secret clusterrole and clusterrolebinding role and rolebinding resourcequota configmap Once cluster is disconnected its Status will change to Disconnected and you can choose to remove it from console by pressing Delete cluster button. Cluster will continue to run as normal, since Delete cluster action only removes it from CAST AI console.","title":"Disconnect read only agent"},{"location":"getting-started/gke/remove-from-gke/#removing-cast-ai-credentials-and-other-resources","text":"","title":"Removing CAST AI credentials and other resources"},{"location":"getting-started/gke/remove-from-gke/#prerequisites","text":"In order to remove these resources first of all: Go to CAST AI console \u2192 Autoscaler page \u2192 Disable all CAST AI policies Connect to your cluster from the terminal and run command: kubectl delete deployment castai-agent -n castai-agent With above mentioned pre-requisites completed please follow next steps in GCP Console to remove CAST resources from your running cluster:","title":"Prerequisites"},{"location":"getting-started/gke/remove-from-gke/#delete-service-account","text":"Go to GCP console \u2192 Identity and Access Management (IAM) \u2192 Service Accounts \u2192 find Service account to manage *cluster-name* GKE cluster via CAST and delete it","title":"Delete Service Account"},{"location":"getting-started/kops/kops/","text":"kOps \u00b6 CAST AI supports clusters running following versions of kOps deployed on AWS: v1.11 v1.15 v1.17 v1.18 v1.20 v1.21 v1.22 Connect cluster \u00b6 To connect your cluster, log in to the CAST AI console and navigate to Connect cluster window, kOps tab. Copy the provided script and run it in your terminal or cloud shell. Make sure that kubectl is installed and can access your cluster. The script will create following kubernetes objects related to castai-agent agent: namespace and deployment serviceaccount and secret clusterrole and clusterrolebinding role and rolebinding resourcequota configmap After installation, your cluster name will appear below connection instructions as well as in the Cluster list . From there, you can open the cluster details and explore a detailed savings estimate based on your cluster configuration. The agent will run in a read-only mode, providing savings suggestions without applying any actual modifications. Credential onboarding \u00b6 To unlock all the benefits and enable automatic cost optimization, CAST AI needs to have access to your cluster. The following section describes the steps required to onboard the kOps cluster on the CAST AI console. To make it less troublesome, we created a script that automates most of the steps. Prerequisites: AWS CLI - A command line tool for working with AWS services using commands in your command-line shell. For more information, see Installing AWS CLI . jq \u2013 a lightweight command line JSON processor. For more information about the tool click here . IAM permissions \u2013 The IAM security principal that you're using must have permissions to work with AWS IAM, and related resources. Additionally, you should have access to the kOps cluster that you wish to onboard on the CAST AI console. The CAST AI agent has to be running on the cluster. Onboarding steps: To onboard your cluster, go to the Available Savings report and click on the Start saving or Enable CAST AI button. The button's name will depend on the number of optimizations available from your cluster. Follow the instruction in the pop-up window to create and use AWS AccessKeyId and SecretAccessKey That\u2019s it! Your cluster is onboarded. Now you can enable CAST AI Autoscaler to keep your cluster configuration optimal. Actions performed by the onboarding script \u00b6 The script will perform the following actions: Create cast-kops-*cluster-name* IAM user with the required permissions to manage the cluster: AmazonEC2ReadOnlyAccess IAMReadOnlyAccess Manage instances in specified cluster restricted to cluster VPC Manage autoscaling groups in the specified cluster Manage EC2 Node Groups in the specified cluster Create CASTKopsPolicyV2 managed policy used to manage kOps cluster. The policy contains the following permissions: Create & delete instance profiles Create & manage roles Create & manage EC2 security groups, key pairs, and tags Run EC2 instances Create CASTKopsRestrictedaccess inline policy to manage cluster specific resources. Modify aws-auth ConfigMap to map newly created IAM user to the cluster Create and print AWS AccessKeyId and SecretAccessKey , which then can be added to the CAST AI console and assigned to the corresponding kOps cluster. The AccessKeyId and SecretAccessKey are used to by CAST to make programmatic calls to AWS and are stored in CAST AI's secret store that runs on Google's Secret manager solution . All the Write permissions are scoped to a single kOps cluster - it won't have access to resources of any other clusters in the AWS account. Manual credential onboarding \u00b6 To complete the steps mentioned above manually (without our script), be aware that when you create a cluster, the IAM entity user or role (such as a federated user that creates the cluster) is automatically granted a system:masters permissions in the cluster's RBAC configuration in the control plane. To grant additional AWS users or roles the ability to interact with your cluster, you need to edit the aws-auth ConfigMap within Kubernetes. For more information, see Managing users or IAM roles for your cluster . Usage of AWS services \u00b6 CAST AI relies on the agent runs inside customer's cluster. The following services are consumed during the operation: A portion of EC2 node resources from the customer's cluster. The CAST AI agent uses Cluster proportional vertical autoscaler to consume a minimum required resources depending on the size of the cluster Low amount of network traffic to communicate with CAST AI SaaS EC2 instances, their storage, and intra-cluster network traffic to manage Kubernetes cluster and perform autoscaling IAM resources as detailed in the onboarding section Known issues \u00b6 Custom taints on kOps v1.17 with kube-router \u00b6 There's a known issue on kOps v1.17. Nodes with custom taints are not able to join the cluster when cluster is used with kube-router networking component. This happens because kube-router doesn't have required tolerations to start on nodes with custom taints. Impact: CAST.AI won't be able to add any nodes with custom taints (ex. Spot) for impacted clusters. Resolution: add following tolerations to kube-router daemonSet in your kOps v1.17 cluster's kube-system namespace. tolerations: - effect: NoSchedule operator: Exists - effect: NoExecute operator: Exists - key: CriticalAddonsOnly operator: Exists","title":"Connect and onboard kOps"},{"location":"getting-started/kops/kops/#kops","text":"CAST AI supports clusters running following versions of kOps deployed on AWS: v1.11 v1.15 v1.17 v1.18 v1.20 v1.21 v1.22","title":"kOps"},{"location":"getting-started/kops/kops/#connect-cluster","text":"To connect your cluster, log in to the CAST AI console and navigate to Connect cluster window, kOps tab. Copy the provided script and run it in your terminal or cloud shell. Make sure that kubectl is installed and can access your cluster. The script will create following kubernetes objects related to castai-agent agent: namespace and deployment serviceaccount and secret clusterrole and clusterrolebinding role and rolebinding resourcequota configmap After installation, your cluster name will appear below connection instructions as well as in the Cluster list . From there, you can open the cluster details and explore a detailed savings estimate based on your cluster configuration. The agent will run in a read-only mode, providing savings suggestions without applying any actual modifications.","title":"Connect cluster"},{"location":"getting-started/kops/kops/#credential-onboarding","text":"To unlock all the benefits and enable automatic cost optimization, CAST AI needs to have access to your cluster. The following section describes the steps required to onboard the kOps cluster on the CAST AI console. To make it less troublesome, we created a script that automates most of the steps. Prerequisites: AWS CLI - A command line tool for working with AWS services using commands in your command-line shell. For more information, see Installing AWS CLI . jq \u2013 a lightweight command line JSON processor. For more information about the tool click here . IAM permissions \u2013 The IAM security principal that you're using must have permissions to work with AWS IAM, and related resources. Additionally, you should have access to the kOps cluster that you wish to onboard on the CAST AI console. The CAST AI agent has to be running on the cluster. Onboarding steps: To onboard your cluster, go to the Available Savings report and click on the Start saving or Enable CAST AI button. The button's name will depend on the number of optimizations available from your cluster. Follow the instruction in the pop-up window to create and use AWS AccessKeyId and SecretAccessKey That\u2019s it! Your cluster is onboarded. Now you can enable CAST AI Autoscaler to keep your cluster configuration optimal.","title":"Credential onboarding"},{"location":"getting-started/kops/kops/#actions-performed-by-the-onboarding-script","text":"The script will perform the following actions: Create cast-kops-*cluster-name* IAM user with the required permissions to manage the cluster: AmazonEC2ReadOnlyAccess IAMReadOnlyAccess Manage instances in specified cluster restricted to cluster VPC Manage autoscaling groups in the specified cluster Manage EC2 Node Groups in the specified cluster Create CASTKopsPolicyV2 managed policy used to manage kOps cluster. The policy contains the following permissions: Create & delete instance profiles Create & manage roles Create & manage EC2 security groups, key pairs, and tags Run EC2 instances Create CASTKopsRestrictedaccess inline policy to manage cluster specific resources. Modify aws-auth ConfigMap to map newly created IAM user to the cluster Create and print AWS AccessKeyId and SecretAccessKey , which then can be added to the CAST AI console and assigned to the corresponding kOps cluster. The AccessKeyId and SecretAccessKey are used to by CAST to make programmatic calls to AWS and are stored in CAST AI's secret store that runs on Google's Secret manager solution . All the Write permissions are scoped to a single kOps cluster - it won't have access to resources of any other clusters in the AWS account.","title":"Actions performed by the onboarding script"},{"location":"getting-started/kops/kops/#manual-credential-onboarding","text":"To complete the steps mentioned above manually (without our script), be aware that when you create a cluster, the IAM entity user or role (such as a federated user that creates the cluster) is automatically granted a system:masters permissions in the cluster's RBAC configuration in the control plane. To grant additional AWS users or roles the ability to interact with your cluster, you need to edit the aws-auth ConfigMap within Kubernetes. For more information, see Managing users or IAM roles for your cluster .","title":"Manual credential onboarding"},{"location":"getting-started/kops/kops/#usage-of-aws-services","text":"CAST AI relies on the agent runs inside customer's cluster. The following services are consumed during the operation: A portion of EC2 node resources from the customer's cluster. The CAST AI agent uses Cluster proportional vertical autoscaler to consume a minimum required resources depending on the size of the cluster Low amount of network traffic to communicate with CAST AI SaaS EC2 instances, their storage, and intra-cluster network traffic to manage Kubernetes cluster and perform autoscaling IAM resources as detailed in the onboarding section","title":"Usage of AWS services"},{"location":"getting-started/kops/kops/#known-issues","text":"","title":"Known issues"},{"location":"getting-started/kops/kops/#custom-taints-on-kops-v117-with-kube-router","text":"There's a known issue on kOps v1.17. Nodes with custom taints are not able to join the cluster when cluster is used with kube-router networking component. This happens because kube-router doesn't have required tolerations to start on nodes with custom taints. Impact: CAST.AI won't be able to add any nodes with custom taints (ex. Spot) for impacted clusters. Resolution: add following tolerations to kube-router daemonSet in your kOps v1.17 cluster's kube-system namespace. tolerations: - effect: NoSchedule operator: Exists - effect: NoExecute operator: Exists - key: CriticalAddonsOnly operator: Exists","title":"Custom taints on kOps v1.17 with kube-router"},{"location":"getting-started/kops/remove-from-kops/","text":"Remove CAST resources from kOps cluster \u00b6 Based on the way how CAST was used on a cluster there are two options to remove CAST resources. Disconnect kOps cluster \u00b6 In order to disconnect your cluster from CAST AI click Disconnect cluster button in Clusters list and follow the guidance. Alternatively run following command from your terminal used to access the cluster: kubectl delete deployment castai-agent -n castai-agent On top of that, also delete following kubernetes objects related to castai-agent agent: namespace serviceaccount and secret clusterrole and clusterrolebinding role and rolebinding resourcequota configmap Once cluster is disconnected its Status will change to Disconnected and you can choose to remove it from console by pressing Delete cluster button. Cluster will continue to run as normal, since Delete cluster action only removes it from CAST AI console. Removing CAST AI credentials and other resources \u00b6 When CAST was used to to optimize kOps cluster following resources were created: CAST AI agent deployment User Policies Roles Instances Security group To remove them follow the steps outlined below. Prerequisites \u00b6 In order to remove these resources first of all: Go to CAST AI console \u2192 Autoscaler page \u2192 Disable all CAST AI policies Connect to your cluster from the terminal and run command: kubectl delete deployment castai-agent -n castai-agent With above mentioned pre-requisites completed please follow next steps in AWS Management Console to remove CAST resources from your running cluster: Delete user \u00b6 Go to AWS console \u2192 Identity and Access Management (IAM) \u2192 Users \u2192 find cast-kops-*cluster-name* user. Select the user and click Delete user. Delete policies \u00b6 Go to AWS console \u2192 Identity and Access Management (IAM) \u2192 Customer managed policies \u2192 find CASTKopsPolicyV2 and CASTKopsRestrictedaccess . Mark it, go to Policy actions \u2192 Delete policy. Delete instances \u00b6 Go to EC2 dashboard \u2192 Security groups \u2192 Instances \u2192 find CAST created instance(s) *cluster-name*-cast-#####\u2026 . Terminate instance(s). If you have you used PODs with attached storage go to Volumes and delete Volumes attached to CAST AI created nodes. Delete security group \u00b6 Go to EC2 dashboard \u2192 Security groups \u2192 find cast-*cluster-name*-cluster/CastNodeSecurityGroup . Delete security group.","title":"Remove CAST AI from kOps"},{"location":"getting-started/kops/remove-from-kops/#remove-cast-resources-from-kops-cluster","text":"Based on the way how CAST was used on a cluster there are two options to remove CAST resources.","title":"Remove CAST resources from kOps cluster"},{"location":"getting-started/kops/remove-from-kops/#disconnect-kops-cluster","text":"In order to disconnect your cluster from CAST AI click Disconnect cluster button in Clusters list and follow the guidance. Alternatively run following command from your terminal used to access the cluster: kubectl delete deployment castai-agent -n castai-agent On top of that, also delete following kubernetes objects related to castai-agent agent: namespace serviceaccount and secret clusterrole and clusterrolebinding role and rolebinding resourcequota configmap Once cluster is disconnected its Status will change to Disconnected and you can choose to remove it from console by pressing Delete cluster button. Cluster will continue to run as normal, since Delete cluster action only removes it from CAST AI console.","title":"Disconnect kOps cluster"},{"location":"getting-started/kops/remove-from-kops/#removing-cast-ai-credentials-and-other-resources","text":"When CAST was used to to optimize kOps cluster following resources were created: CAST AI agent deployment User Policies Roles Instances Security group To remove them follow the steps outlined below.","title":"Removing CAST AI credentials and other resources"},{"location":"getting-started/kops/remove-from-kops/#prerequisites","text":"In order to remove these resources first of all: Go to CAST AI console \u2192 Autoscaler page \u2192 Disable all CAST AI policies Connect to your cluster from the terminal and run command: kubectl delete deployment castai-agent -n castai-agent With above mentioned pre-requisites completed please follow next steps in AWS Management Console to remove CAST resources from your running cluster:","title":"Prerequisites"},{"location":"getting-started/kops/remove-from-kops/#delete-user","text":"Go to AWS console \u2192 Identity and Access Management (IAM) \u2192 Users \u2192 find cast-kops-*cluster-name* user. Select the user and click Delete user.","title":"Delete user"},{"location":"getting-started/kops/remove-from-kops/#delete-policies","text":"Go to AWS console \u2192 Identity and Access Management (IAM) \u2192 Customer managed policies \u2192 find CASTKopsPolicyV2 and CASTKopsRestrictedaccess . Mark it, go to Policy actions \u2192 Delete policy.","title":"Delete policies"},{"location":"getting-started/kops/remove-from-kops/#delete-instances","text":"Go to EC2 dashboard \u2192 Security groups \u2192 Instances \u2192 find CAST created instance(s) *cluster-name*-cast-#####\u2026 . Terminate instance(s). If you have you used PODs with attached storage go to Volumes and delete Volumes attached to CAST AI created nodes.","title":"Delete instances"},{"location":"getting-started/kops/remove-from-kops/#delete-security-group","text":"Go to EC2 dashboard \u2192 Security groups \u2192 find cast-*cluster-name*-cluster/CastNodeSecurityGroup . Delete security group.","title":"Delete security group"},{"location":"guides/agent-monitoring/","text":"CAST AI agent health monitoring \u00b6 The CAST AI agent deployed inside customer's cluster is a critical part of the solution, hence monitoring its status is vital. This document outlines the recommended techniques for monitoring and understanding the health of this agent. Agent logs \u00b6 To quickly assess the state of the agent, use the standard kubectl command to access the agent container logs: kubectl logs -n castai-agent -l app.kubernetes.io/name = castai-agent -c agent The expected outcome if the agent operations are healthy is as follows: time=\"2021-12-02T09:19:42Z\" level=info msg=\"delta with items[#] sent, response_code=204\" Prometheus metrics \u00b6 CAST AI exposes a number of Prometheus metrics, some of them can be used to assess the health of the CAST AI agent and alert in case of any issues. For example, if the agent is performing as expected it should send snapshots (deltas) containing metadata about the cluster every 15s back to the CAST AI console. If snapshots are not received for a sustained period of time, this should be cause of concern and investigation. To monitor and alert against such scenarios, we propose using Prometheus metrics and alerting as described here . Advanced monitoring using kube-state-metrics and Prometheus \u00b6 We suggest building kube-state-metrics and Prometheus setup for advanced monitoring capability. This would enable users to capture cases like: Agent pod not transitioning into Running status, Agent pod constantly restarting. Examples of Prometheus alerting rules that cover the mentioned scenarions are presented below: alerting_rules.yml : groups : - name : castai-agent rules : - alert : CastaiAgentFailedToRun expr : | sum by (namespace, pod) ( max by(namespace, pod) ( kube_pod_status_phase{phase=~\"Pending|Unknown|Failed\",namespace=\"castai-agent\"} ) * on(namespace, pod) group_left(owner_kind) topk by(namespace, pod) ( 1, max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!=\"Job\"}) ) ) > 0 for : 5m labels : severity : page annotations : summary : \"Kubernetes pod {{ $labels.pod }} cannot transition to Running phase.\" description : \"Checks the Kubernetes pod status phase metric and alerts when phase Running has not been reached in at least 5 minutes. Tip: phase=Running does not mean that the pod is running without any issues, i.e. when phase=Running, pod can have status CrashLoopBackOff, it only means that the pod was successfully scheduled.\" value : \"{{`{{ $value }}`}}\" - alert : CastaiAgentCrashLooping expr : | increase(kube_pod_container_status_restarts_total{namespace=\"castai-agent\"}[1h]) > 5 for : 0s labels : severity : page annotations : summary : \"Kubernetes pod {{ $labels.pod }} crash looping.\" description : \"Checks the total number of pod restarts in the last hour and alerts when there were at least 5 restarts.\" value : \"{{ $value }}\"","title":"Agent status monitoring"},{"location":"guides/agent-monitoring/#cast-ai-agent-health-monitoring","text":"The CAST AI agent deployed inside customer's cluster is a critical part of the solution, hence monitoring its status is vital. This document outlines the recommended techniques for monitoring and understanding the health of this agent.","title":"CAST AI agent health monitoring"},{"location":"guides/agent-monitoring/#agent-logs","text":"To quickly assess the state of the agent, use the standard kubectl command to access the agent container logs: kubectl logs -n castai-agent -l app.kubernetes.io/name = castai-agent -c agent The expected outcome if the agent operations are healthy is as follows: time=\"2021-12-02T09:19:42Z\" level=info msg=\"delta with items[#] sent, response_code=204\"","title":"Agent logs"},{"location":"guides/agent-monitoring/#prometheus-metrics","text":"CAST AI exposes a number of Prometheus metrics, some of them can be used to assess the health of the CAST AI agent and alert in case of any issues. For example, if the agent is performing as expected it should send snapshots (deltas) containing metadata about the cluster every 15s back to the CAST AI console. If snapshots are not received for a sustained period of time, this should be cause of concern and investigation. To monitor and alert against such scenarios, we propose using Prometheus metrics and alerting as described here .","title":"Prometheus metrics"},{"location":"guides/agent-monitoring/#advanced-monitoring-using-kube-state-metrics-and-prometheus","text":"We suggest building kube-state-metrics and Prometheus setup for advanced monitoring capability. This would enable users to capture cases like: Agent pod not transitioning into Running status, Agent pod constantly restarting. Examples of Prometheus alerting rules that cover the mentioned scenarions are presented below: alerting_rules.yml : groups : - name : castai-agent rules : - alert : CastaiAgentFailedToRun expr : | sum by (namespace, pod) ( max by(namespace, pod) ( kube_pod_status_phase{phase=~\"Pending|Unknown|Failed\",namespace=\"castai-agent\"} ) * on(namespace, pod) group_left(owner_kind) topk by(namespace, pod) ( 1, max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!=\"Job\"}) ) ) > 0 for : 5m labels : severity : page annotations : summary : \"Kubernetes pod {{ $labels.pod }} cannot transition to Running phase.\" description : \"Checks the Kubernetes pod status phase metric and alerts when phase Running has not been reached in at least 5 minutes. Tip: phase=Running does not mean that the pod is running without any issues, i.e. when phase=Running, pod can have status CrashLoopBackOff, it only means that the pod was successfully scheduled.\" value : \"{{`{{ $value }}`}}\" - alert : CastaiAgentCrashLooping expr : | increase(kube_pod_container_status_restarts_total{namespace=\"castai-agent\"}[1h]) > 5 for : 0s labels : severity : page annotations : summary : \"Kubernetes pod {{ $labels.pod }} crash looping.\" description : \"Checks the total number of pod restarts in the last hour and alerts when there were at least 5 restarts.\" value : \"{{ $value }}\"","title":"Advanced monitoring using kube-state-metrics and Prometheus"},{"location":"guides/autoscaling-policies/","text":"Autoscaling policies \u00b6 Autoscaling policies define the set of rules based on which your cluster is monitored and scaled to maintain steady performance at the lowest possible cost. This topic describes the available policy configuration options and provides guidance on how to configure them. Prerequisites \u00b6 Onboard a cluster - see connect cluster Once you've connected a cluster, select it and navigate to the Autoscaler menu. Scoped autoscaler mode \u00b6 Autoscaler features described below can be made to act only on a subset of your cluster. By marking specific workloads for autoscaling, only that subset will be considered by the unscheduled pods policy, and the empty nodes policy will only clean up nodes that the autoscaler has previously created. While this mode is turned on, the autoscaler-created nodes will have a specific taint: scheduling.cast.ai/scoped-autoscaler=true:NoSchedule . This ensures that only the subset of workloads specifically meant for the scoped autoscaler will be scheduled on these nodes. For pods that you wish to be included, update your relevant deployments to contain this configuration: apiVersion : apps/v1 kind : Deployment spec : template : spec : nodeSelector : provisioner.cast.ai/managed-by : cast.ai tolerations : - key : \"scheduling.cast.ai/scoped-autoscaler\" operator : \"Exists\" effect : \"NoSchedule\" The node selector will ensure that pods only schedule on CAST AI-provisioned nodes. Also, this specific selector is what the scoped autoscaler is looking for when deciding which unsheduled pods are within the scope. Toleration is required for the above-described reasons: we want the pods to actually be able to be scheduled on provisioned nodes. If toleration is not present, this will be treated as misconfiguration and the pod will be ignored. Evictor also needs to be configured to run in scoped mode. Call PUT /v1/kubernetes/clusters/{clusterId}/policies API by supplying full policy config with evictor settings updated, partial snippet bellow (with scopesMode: True) ... \"evictor\" : { \"enabled\" : true , \"dryRun\" : false , \"aggressiveMode\" : false , \"scopedMode\" : true , \"cycleInterval\" : \"5s\" , \"allowed\" : true , \"nodeGracePeriodMinutes\" : 2 } ... Cluster CPU limits policy \u00b6 Each CAST AI cluster size can be limited by the total amount of vCPUs available on all the worker nodes used to run workloads. If disabled, the cluster can upscale indefinitely and downscale to 0 worker nodes, depending on the actual resource consumption. Configuring CPU limits policy \u00b6 You can adjust a cluster's CPU limits settings either via the CAST AI console: or via the CAST AI policies API endpoint by setting values for \"clusterLimits\" : { \"cpu\" : { \"maxCores\" : <value> , \"minCores\" : <value> }, \"enabled\" : <value> } The new settings will propagate immediately. Horizontal Pod Autoscaler (HPA) policy \u00b6 For now, the HPA policy is only supported on clusters created in CAST AI. See HPA documentation for a detailed overview. Spot/Preemptive Instances policy \u00b6 This policy instructs the CAST AI optimization engine to purchase Spot / Preemptive instances and place specifically labelled pods on those instances. CAST AI automatically handles instance interruptions and replaces instances when they are terminated by the CSP. You can find a detailed guide on how to configure your workloads to run on Spot instances here . The following configuration settings can be applied to the policy. Interruption tolerance \u00b6 Using the interruption tolerance setting, you can restrict which instances types the autoscaler should consider when choosing the Spot instance type. The default \"Cost efficient\" option means that the autoscaler will choose the cheapest option, regardless of the selected instance type's reliability; choosing \"Least interrupted\" will ensure that selection will be done only from most reliable instances. Spot fallback \u00b6 By using this feature, you can guarantee that workloads designated for spot instances have capacity to run even if spot inventory is temporarily not available. To mitigate the impact of spot drought, CAST AI provisions fallback node (i.e. a temporary on-demand node) and use it to schedule the impacted workloads. Once the configured time expires, CAST AI will automatically attempt to find the suitable spot node. If it is available, it will get provisioned - then the fallback node will be drained and deleted. As a result, the impacted workloads will get scheduled on the spot node in the same way as prior to the spot drought event. Unscheduled pods policy \u00b6 A pod becomes unschedulable when the Kubernetes scheduler cannot find a node to which it can assign the pod. For instance, a pod can request more CPU or memory than the resources available on any of the worker nodes. In many such cases, this indicates the need to scale up by adding additional nodes to the cluster. The CAST AI autoscaler is equipped with a mechanism to handle this. Headroom attributes \u00b6 Headroom is a buffer of spare capacity (in terms of both memory and CPU) to ensure that cluster can meet suddenly increased demand for resources. It is based on the currently available total worker nodes resource capacity. For example, if headroom for memory and CPU are both set to 10%, and the cluster consists of 2 worker nodes equipped with 2 cores and 4GB RAM each, a total of 0.4 cores and 819MB would be considered as headroom in the next cluster size increase phase. Node constraints \u00b6 Node constraints limit the possible node pool for CAST AI to choose from when adding a node to a cluster. Entered Min CPU and Max CPU as well as Min memory and Max memory values are respected when selecting the next node to be added to the cluster, this way user influences the composition of the cluster in terms of size and number of nodes. Since this decision is driven by the customer and not by CAST AI optimization engine, it might not result in the most cost effective node added. However, this feature is particularly beneficial when migrating into CAST AI selected nodes and securing additional capacity upfront (i.e. adding larger nodes, but knowing that during migration they would be filled up). The system supports the following CPU to Memory ratios: 1:2, 1:4 and 1:8. Based on the example presented in the picture, CAST AI would consider 4CPU16GiB RAM and 4CPU 32GiB RAM instances Provisioning decision \u00b6 After receiving the unschedulable pods event, the CAST AI recommendation engine will select the best price/performance ratio node capable of accommodating all of the currently unschedulable pods plus headroom or respecting node constraints. CAST AI will then provision it and join with the cluster. This process usually takes a few minutes, depending on the cloud service provider of your choice. Currently, only a single node will be added at a time. If any unschedulable pods still remain, the cycle is repeated until all the pods are scheduled (provided that the reason was insufficient resources). Configuring the unscheduled pod's policy \u00b6 You can enable/disable the unschedulable pod's policy and set headroom settings as well as node constraints either on the CAST AI console or via the CAST AI policies API endpoint by setting values for headroom: \"unschedulablePods\" : { \"enabled\" : <value> , \"headroom\" : { \"cpuPercentage\" : <value> , \"enabled\" : <value> , \"memoryPercentage\" : <value> } } or for node constraints: \"unschedulablePods\" : { \"enabled\" : <value> , \"headroom\" : { \"enabled\" : <value> , \"maxCpuCores\" : <value> , \"maxRamMib\" : <value> , \"minCpuCores\" : <value> , \"minRamMib\" : <value> } } It may take a few minutes for the new settings to propagate. Node deletion policy \u00b6 This policy will automatically remove nodes from your cluster when they no longer have scheduled workloads. This allows your cluster to maintain a minimal footprint and reduce cloud costs. If Keep empty node alive for is set to 0, CAST AI moves to delete an empty node as soon as it is detected. In cases when a user wants to keep the empty node in the cluster for a period of time, a time parameter can be set, effectively delaying the deletion. Disable deletion of specific node(s) \u00b6 If you annotate or label a node with autoscaling.cast.ai/removal-disabled=\"true\" , the Node deletion policy won't delete it even if it is completely empty. Labeling the node(s) \u00b6 You can label nodes using kubectl in the following fashion: Specific node(s): # replace <node_name> with your node name of choice kubectl label node <node_name> [ <node_name> ... ] autoscaling.cast.ai/removal-disabled = true # e.g. to label node `myclusternode-e359fefa-d3a2` run this command: kubectl label node myclusternode-e359fefa-d3a2 autoscaling.cast.ai/removal-disabled = true # e.g. to label two nodes `myclusternode-e359fefa-d3a2` and myclusternode-anothernode run this command: kubectl label node myclusternode-e359fefa-d3a2 myclusternode-anothernode autoscaling.cast.ai/removal-disabled = true Many nodes using label selector # replace <label> with your node name of choice kubectl label node -l <label> autoscaling.cast.ai/removal-disabled = true # e.g. to label nodes in availability zone `europe-west3-c` run this command: kubectl label node -l topology.kubernetes.io/zone = europe-west3-c autoscaling.cast.ai/removal-disabled = true All nodes kubectl label node --all autoscaling.cast.ai/removal-disabled = true Removing the label \u00b6 In order to instruct policy to delete the node, you need to remove the label. Using previously described methods, instruct kubectl to label a node with autoscaling.cast.ai/removal-disabled- (note the - symbol instead of =true ). Evictor \u00b6 CAST AI Evictor also respects this label or annotation so it won't try to evict marked nodes. Policies precedence rules \u00b6 If multiple policies are enabled and multiple rules are triggered during the same evaluation period, they will be handled in the following order: Cluster CPU limits policy Horizontal Pod Autoscaler (HPA) policy Unscheduled pods policy Node deletion policy","title":"Autoscaling policies"},{"location":"guides/autoscaling-policies/#autoscaling-policies","text":"Autoscaling policies define the set of rules based on which your cluster is monitored and scaled to maintain steady performance at the lowest possible cost. This topic describes the available policy configuration options and provides guidance on how to configure them.","title":"Autoscaling policies"},{"location":"guides/autoscaling-policies/#prerequisites","text":"Onboard a cluster - see connect cluster Once you've connected a cluster, select it and navigate to the Autoscaler menu.","title":"Prerequisites"},{"location":"guides/autoscaling-policies/#scoped-autoscaler-mode","text":"Autoscaler features described below can be made to act only on a subset of your cluster. By marking specific workloads for autoscaling, only that subset will be considered by the unscheduled pods policy, and the empty nodes policy will only clean up nodes that the autoscaler has previously created. While this mode is turned on, the autoscaler-created nodes will have a specific taint: scheduling.cast.ai/scoped-autoscaler=true:NoSchedule . This ensures that only the subset of workloads specifically meant for the scoped autoscaler will be scheduled on these nodes. For pods that you wish to be included, update your relevant deployments to contain this configuration: apiVersion : apps/v1 kind : Deployment spec : template : spec : nodeSelector : provisioner.cast.ai/managed-by : cast.ai tolerations : - key : \"scheduling.cast.ai/scoped-autoscaler\" operator : \"Exists\" effect : \"NoSchedule\" The node selector will ensure that pods only schedule on CAST AI-provisioned nodes. Also, this specific selector is what the scoped autoscaler is looking for when deciding which unsheduled pods are within the scope. Toleration is required for the above-described reasons: we want the pods to actually be able to be scheduled on provisioned nodes. If toleration is not present, this will be treated as misconfiguration and the pod will be ignored. Evictor also needs to be configured to run in scoped mode. Call PUT /v1/kubernetes/clusters/{clusterId}/policies API by supplying full policy config with evictor settings updated, partial snippet bellow (with scopesMode: True) ... \"evictor\" : { \"enabled\" : true , \"dryRun\" : false , \"aggressiveMode\" : false , \"scopedMode\" : true , \"cycleInterval\" : \"5s\" , \"allowed\" : true , \"nodeGracePeriodMinutes\" : 2 } ...","title":"Scoped autoscaler mode"},{"location":"guides/autoscaling-policies/#cluster-cpu-limits-policy","text":"Each CAST AI cluster size can be limited by the total amount of vCPUs available on all the worker nodes used to run workloads. If disabled, the cluster can upscale indefinitely and downscale to 0 worker nodes, depending on the actual resource consumption.","title":"Cluster CPU limits policy"},{"location":"guides/autoscaling-policies/#configuring-cpu-limits-policy","text":"You can adjust a cluster's CPU limits settings either via the CAST AI console: or via the CAST AI policies API endpoint by setting values for \"clusterLimits\" : { \"cpu\" : { \"maxCores\" : <value> , \"minCores\" : <value> }, \"enabled\" : <value> } The new settings will propagate immediately.","title":"Configuring CPU limits policy"},{"location":"guides/autoscaling-policies/#horizontal-pod-autoscaler-hpa-policy","text":"For now, the HPA policy is only supported on clusters created in CAST AI. See HPA documentation for a detailed overview.","title":"Horizontal Pod Autoscaler (HPA) policy"},{"location":"guides/autoscaling-policies/#spotpreemptive-instances-policy","text":"This policy instructs the CAST AI optimization engine to purchase Spot / Preemptive instances and place specifically labelled pods on those instances. CAST AI automatically handles instance interruptions and replaces instances when they are terminated by the CSP. You can find a detailed guide on how to configure your workloads to run on Spot instances here . The following configuration settings can be applied to the policy.","title":"Spot/Preemptive Instances policy"},{"location":"guides/autoscaling-policies/#interruption-tolerance","text":"Using the interruption tolerance setting, you can restrict which instances types the autoscaler should consider when choosing the Spot instance type. The default \"Cost efficient\" option means that the autoscaler will choose the cheapest option, regardless of the selected instance type's reliability; choosing \"Least interrupted\" will ensure that selection will be done only from most reliable instances.","title":"Interruption tolerance"},{"location":"guides/autoscaling-policies/#spot-fallback","text":"By using this feature, you can guarantee that workloads designated for spot instances have capacity to run even if spot inventory is temporarily not available. To mitigate the impact of spot drought, CAST AI provisions fallback node (i.e. a temporary on-demand node) and use it to schedule the impacted workloads. Once the configured time expires, CAST AI will automatically attempt to find the suitable spot node. If it is available, it will get provisioned - then the fallback node will be drained and deleted. As a result, the impacted workloads will get scheduled on the spot node in the same way as prior to the spot drought event.","title":"Spot fallback"},{"location":"guides/autoscaling-policies/#unscheduled-pods-policy","text":"A pod becomes unschedulable when the Kubernetes scheduler cannot find a node to which it can assign the pod. For instance, a pod can request more CPU or memory than the resources available on any of the worker nodes. In many such cases, this indicates the need to scale up by adding additional nodes to the cluster. The CAST AI autoscaler is equipped with a mechanism to handle this.","title":"Unscheduled pods policy"},{"location":"guides/autoscaling-policies/#headroom-attributes","text":"Headroom is a buffer of spare capacity (in terms of both memory and CPU) to ensure that cluster can meet suddenly increased demand for resources. It is based on the currently available total worker nodes resource capacity. For example, if headroom for memory and CPU are both set to 10%, and the cluster consists of 2 worker nodes equipped with 2 cores and 4GB RAM each, a total of 0.4 cores and 819MB would be considered as headroom in the next cluster size increase phase.","title":"Headroom attributes"},{"location":"guides/autoscaling-policies/#node-constraints","text":"Node constraints limit the possible node pool for CAST AI to choose from when adding a node to a cluster. Entered Min CPU and Max CPU as well as Min memory and Max memory values are respected when selecting the next node to be added to the cluster, this way user influences the composition of the cluster in terms of size and number of nodes. Since this decision is driven by the customer and not by CAST AI optimization engine, it might not result in the most cost effective node added. However, this feature is particularly beneficial when migrating into CAST AI selected nodes and securing additional capacity upfront (i.e. adding larger nodes, but knowing that during migration they would be filled up). The system supports the following CPU to Memory ratios: 1:2, 1:4 and 1:8. Based on the example presented in the picture, CAST AI would consider 4CPU16GiB RAM and 4CPU 32GiB RAM instances","title":"Node constraints"},{"location":"guides/autoscaling-policies/#provisioning-decision","text":"After receiving the unschedulable pods event, the CAST AI recommendation engine will select the best price/performance ratio node capable of accommodating all of the currently unschedulable pods plus headroom or respecting node constraints. CAST AI will then provision it and join with the cluster. This process usually takes a few minutes, depending on the cloud service provider of your choice. Currently, only a single node will be added at a time. If any unschedulable pods still remain, the cycle is repeated until all the pods are scheduled (provided that the reason was insufficient resources).","title":"Provisioning decision"},{"location":"guides/autoscaling-policies/#configuring-the-unscheduled-pods-policy","text":"You can enable/disable the unschedulable pod's policy and set headroom settings as well as node constraints either on the CAST AI console or via the CAST AI policies API endpoint by setting values for headroom: \"unschedulablePods\" : { \"enabled\" : <value> , \"headroom\" : { \"cpuPercentage\" : <value> , \"enabled\" : <value> , \"memoryPercentage\" : <value> } } or for node constraints: \"unschedulablePods\" : { \"enabled\" : <value> , \"headroom\" : { \"enabled\" : <value> , \"maxCpuCores\" : <value> , \"maxRamMib\" : <value> , \"minCpuCores\" : <value> , \"minRamMib\" : <value> } } It may take a few minutes for the new settings to propagate.","title":"Configuring the unscheduled pod's policy"},{"location":"guides/autoscaling-policies/#node-deletion-policy","text":"This policy will automatically remove nodes from your cluster when they no longer have scheduled workloads. This allows your cluster to maintain a minimal footprint and reduce cloud costs. If Keep empty node alive for is set to 0, CAST AI moves to delete an empty node as soon as it is detected. In cases when a user wants to keep the empty node in the cluster for a period of time, a time parameter can be set, effectively delaying the deletion.","title":"Node deletion policy"},{"location":"guides/autoscaling-policies/#disable-deletion-of-specific-nodes","text":"If you annotate or label a node with autoscaling.cast.ai/removal-disabled=\"true\" , the Node deletion policy won't delete it even if it is completely empty.","title":"Disable deletion of specific node(s)"},{"location":"guides/autoscaling-policies/#labeling-the-nodes","text":"You can label nodes using kubectl in the following fashion: Specific node(s): # replace <node_name> with your node name of choice kubectl label node <node_name> [ <node_name> ... ] autoscaling.cast.ai/removal-disabled = true # e.g. to label node `myclusternode-e359fefa-d3a2` run this command: kubectl label node myclusternode-e359fefa-d3a2 autoscaling.cast.ai/removal-disabled = true # e.g. to label two nodes `myclusternode-e359fefa-d3a2` and myclusternode-anothernode run this command: kubectl label node myclusternode-e359fefa-d3a2 myclusternode-anothernode autoscaling.cast.ai/removal-disabled = true Many nodes using label selector # replace <label> with your node name of choice kubectl label node -l <label> autoscaling.cast.ai/removal-disabled = true # e.g. to label nodes in availability zone `europe-west3-c` run this command: kubectl label node -l topology.kubernetes.io/zone = europe-west3-c autoscaling.cast.ai/removal-disabled = true All nodes kubectl label node --all autoscaling.cast.ai/removal-disabled = true","title":"Labeling the node(s)"},{"location":"guides/autoscaling-policies/#removing-the-label","text":"In order to instruct policy to delete the node, you need to remove the label. Using previously described methods, instruct kubectl to label a node with autoscaling.cast.ai/removal-disabled- (note the - symbol instead of =true ).","title":"Removing the label"},{"location":"guides/autoscaling-policies/#evictor","text":"CAST AI Evictor also respects this label or annotation so it won't try to evict marked nodes.","title":"Evictor"},{"location":"guides/autoscaling-policies/#policies-precedence-rules","text":"If multiple policies are enabled and multiple rules are triggered during the same evaluation period, they will be handled in the following order: Cluster CPU limits policy Horizontal Pod Autoscaler (HPA) policy Unscheduled pods policy Node deletion policy","title":"Policies precedence rules"},{"location":"guides/cloud-permissions/","text":"Permissions Setup Used In Cloud Providers (AWS / GCP / Azure) \u00b6 When cluster is promoted to Phase 2 (cost optimisation is enabled) then CAST AI central system is able to perform operations on Cloud Provider (AWS / GCP / Azure) level (like for example request a node and add it to a cluster). Such operations require relevant Cloud Provider specific credentials and permissions. Below there is a description of the permission setup done for AWS and GCP Cloud Providers (similar description for Azure will be released shortly as well). AWS \u00b6 AWS User used by CAST AI \u00b6 Phase 2 on-boarding script creates a dedicated AWS user used by CAST AI to request and manage AWS resources on customer's behalf. This user follows cast-eks-<cluster name> convention: \u00bb aws iam list-users --output text | grep cast-eks- USERS arn:aws:iam::123456789012:user/cast-eks-some-cluster 2022 -05-12T12:48:47+00:00 / 123456789012345678901 cast-eks-some-cluster AWS permissions used by CAST AI \u00b6 Once user is created, following policies are attached to the AWS user: API Group Type Description AmazonEC2ReadOnlyAccess AWS managed policy Used to fetch details about Virtual Machines IAMReadOnlyAccess AWS managed policy Used to fetch required data from IAM CastEKSPolicy Managed policy CAST AI policy for creating and removing Virtual Machines when managing Cluster nodes CastEKSRestrictedAccess Inline policy CAST AI policy for Cluster Pause / Resume functionality These policies may be validated by combining results from the following commands (please look up AWS documentation about the details how to used that): aws iam list-user-policies --user-name <user name> aws iam list-attached-user-policies --user-name <user name> aws iam list-groups-for-user --user-name <user name> The result also contains policies' arn's which is required for inspecting permissions, which can be done using following commands: \u00bb aws iam list-policy-versions --policy-arn arn:aws:iam::123456789012:policy/CastEKSPolicy { \"Versions\" : [ { \"VersionId\" : \"v83\" , \"IsDefaultVersion\" : true, \"CreateDate\" : \"2022-05-12T12:49:01+00:00\" } , { \"VersionId\" : \"v82\" , \"IsDefaultVersion\" : false, \"CreateDate\" : \"2022-05-12T09:53:58+00:00\" } ] } ... and then: \u00bb aws iam get-policy-version --policy-arn arn:aws:iam::123456789012:policy/CastEKSPolicy --version-id v83 { \"PolicyVersion\" : { \"Document\" : { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"PassRoleEC2\" , \"Action\" : \"iam:PassRole\" , \"Effect\" : \"Allow\" , \"Resource\" : \"arn:aws:iam::*:role/*\" , \"Condition\" : { \"StringEquals\" : { \"iam:PassedToService\" : \"ec2.amazonaws.com\" } } } , { \"Sid\" : \"NonResourcePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"iam:DeleteInstanceProfile\" , \"iam:RemoveRoleFromInstanceProfile\" , \"iam:DeleteRole\" , \"iam:DetachRolePolicy\" , \"iam:CreateServiceLinkedRole\" , \"iam:DeleteServiceLinkedRole\" , \"ec2:CreateSecurityGroup\" , \"ec2:CreateKeyPair\" , \"ec2:DeleteKeyPair\" , \"ec2:CreateTags\" , \"ec2:ImportKeyPair\" ] , \"Resource\" : \"*\" } , { \"Sid\" : \"RunInstancesPermissions\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : [ \"arn:aws:ec2:*:028075177508:network-interface/*\" , \"arn:aws:ec2:*:028075177508:security-group/*\" , \"arn:aws:ec2:*:028075177508:volume/*\" , \"arn:aws:ec2:*:028075177508:key-pair/*\" , \"arn:aws:ec2:*::image/*\" ] } ] } , \"VersionId\" : \"v83\" , \"IsDefaultVersion\" : true, \"CreateDate\" : \"2022-05-12T12:49:01+00:00\" } } AWS permissions when access is granted using Cross-account IAM role \u00b6 When enabling cost optimisation (Phase 2) for a connected cluster, there is an option to grant permissions using Cross-account IAM role. This feature allows creating a dedicated cluster user in CAST AI AWS account with a trust policy to be able to 'assume role' defined in customer's AWS account. Keeping role definition and users in separate AWS accounts allows keeping user's credentials on CAST AI side without handing them over when running on-boarding script, which provides higher security level. From customer perspective used role contains the same set of permissions as in case of regular flow (when user is created in customer's AWS account), this can be verified using following command: aws iam list-attached-role-policies --role-name <role name> aws iam list-role-policies --role-name <role name> Additionally, a trust relationship is created as follows: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::123456789012:user/cast-crossrole-f8f82b9c-d375-40d2-9483-123456789012\" }, \"Action\" : \"sts:AssumeRole\" } ] } GCP \u00b6 Overview of GCP permissions used by CAST AI \u00b6 Phase 2 on-boarding script performs several actions to get required permissions to manage GKE and GCP resources on customer's behalf: Enables required GCP services and APIs for the project Creates IAM service account and assigns required roles to it Generates IAM service account key which is used by CAST AI components to manage GKE and GCP resources on customer's behalf GCP Services and APIs used by CAST AI \u00b6 CAST AI enables following GCP services and APIs for the project on which customer's GKE cluster is running: GCP Service / API Group Description serviceusage.googleapis.com API to list, enable and disable GCP services iam.googleapis.com API to manage identity and access control for GCP resources cloudresourcemanager.googleapis.com API to create, read, and update metadata for GCP resource containers container.googleapis.com API to manage GKE compute.googleapis.com API to manage GCP virtual machines GCP Service Account used by CAST AI \u00b6 Phase 2 on-boarding script creates a dedicated GCP service account used by CAST AI to request and manage GCP resources on customer's behalf. The Service Account follows castai-gke-<cluster-name-hash> convention. Service account can be verified by: gcloud iam service-accounts describe castai-gke-<cluster-name-hash>@<your-gcp-project>.iam.gserviceaccount.com CAST AI created Service Account has the following roles attached: Role name Description castai.gkeAccess CAST AI managed role used to manage CAST AI add/delete node operations, full list of permissions can be found below container.developer GCP managed role for full access to Kubernetes API objects inside Kubernetes cluster iam.serviceAccountUser GCP managed role to allow run operations as the service account Full list of castai.gkeAccess role permissions: \u00bb gcloud iam roles describe --project = <your-project-name> castai.gkeAccess description: Role to manage GKE cluster via CAST AI etag: example-tag includedPermissions: - compute.addresses.use - compute.disks.create - compute.disks.setLabels - compute.disks.use - compute.images.useReadOnly - compute.instanceGroupManagers.get - compute.instanceGroupManagers.update - compute.instanceGroups.get - compute.instanceTemplates.create - compute.instanceTemplates.delete - compute.instanceTemplates.get - compute.instanceTemplates.list - compute.instances.create - compute.instances.delete - compute.instances.get - compute.instances.list - compute.instances.setLabels - compute.instances.setMetadata - compute.instances.setServiceAccount - compute.instances.setTags - compute.instances.start - compute.instances.stop - compute.networks.use - compute.networks.useExternalIp - compute.subnetworks.get - compute.subnetworks.use - compute.subnetworks.useExternalIp - compute.zones.get - compute.zones.list - container.certificateSigningRequests.approve - container.clusters.get - container.clusters.update - container.operations.get - serviceusage.services.list name: projects/<your-project-name>/roles/castai.gkeAccess stage: ALPHA title: Role to manage GKE cluster via CAST AI","title":"Cloud permissions"},{"location":"guides/cloud-permissions/#permissions-setup-used-in-cloud-providers-aws-gcp-azure","text":"When cluster is promoted to Phase 2 (cost optimisation is enabled) then CAST AI central system is able to perform operations on Cloud Provider (AWS / GCP / Azure) level (like for example request a node and add it to a cluster). Such operations require relevant Cloud Provider specific credentials and permissions. Below there is a description of the permission setup done for AWS and GCP Cloud Providers (similar description for Azure will be released shortly as well).","title":"Permissions Setup Used In Cloud Providers (AWS / GCP / Azure)"},{"location":"guides/cloud-permissions/#aws","text":"","title":"AWS"},{"location":"guides/cloud-permissions/#aws-user-used-by-cast-ai","text":"Phase 2 on-boarding script creates a dedicated AWS user used by CAST AI to request and manage AWS resources on customer's behalf. This user follows cast-eks-<cluster name> convention: \u00bb aws iam list-users --output text | grep cast-eks- USERS arn:aws:iam::123456789012:user/cast-eks-some-cluster 2022 -05-12T12:48:47+00:00 / 123456789012345678901 cast-eks-some-cluster","title":"AWS User used by CAST AI"},{"location":"guides/cloud-permissions/#aws-permissions-used-by-cast-ai","text":"Once user is created, following policies are attached to the AWS user: API Group Type Description AmazonEC2ReadOnlyAccess AWS managed policy Used to fetch details about Virtual Machines IAMReadOnlyAccess AWS managed policy Used to fetch required data from IAM CastEKSPolicy Managed policy CAST AI policy for creating and removing Virtual Machines when managing Cluster nodes CastEKSRestrictedAccess Inline policy CAST AI policy for Cluster Pause / Resume functionality These policies may be validated by combining results from the following commands (please look up AWS documentation about the details how to used that): aws iam list-user-policies --user-name <user name> aws iam list-attached-user-policies --user-name <user name> aws iam list-groups-for-user --user-name <user name> The result also contains policies' arn's which is required for inspecting permissions, which can be done using following commands: \u00bb aws iam list-policy-versions --policy-arn arn:aws:iam::123456789012:policy/CastEKSPolicy { \"Versions\" : [ { \"VersionId\" : \"v83\" , \"IsDefaultVersion\" : true, \"CreateDate\" : \"2022-05-12T12:49:01+00:00\" } , { \"VersionId\" : \"v82\" , \"IsDefaultVersion\" : false, \"CreateDate\" : \"2022-05-12T09:53:58+00:00\" } ] } ... and then: \u00bb aws iam get-policy-version --policy-arn arn:aws:iam::123456789012:policy/CastEKSPolicy --version-id v83 { \"PolicyVersion\" : { \"Document\" : { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"PassRoleEC2\" , \"Action\" : \"iam:PassRole\" , \"Effect\" : \"Allow\" , \"Resource\" : \"arn:aws:iam::*:role/*\" , \"Condition\" : { \"StringEquals\" : { \"iam:PassedToService\" : \"ec2.amazonaws.com\" } } } , { \"Sid\" : \"NonResourcePermissions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"iam:DeleteInstanceProfile\" , \"iam:RemoveRoleFromInstanceProfile\" , \"iam:DeleteRole\" , \"iam:DetachRolePolicy\" , \"iam:CreateServiceLinkedRole\" , \"iam:DeleteServiceLinkedRole\" , \"ec2:CreateSecurityGroup\" , \"ec2:CreateKeyPair\" , \"ec2:DeleteKeyPair\" , \"ec2:CreateTags\" , \"ec2:ImportKeyPair\" ] , \"Resource\" : \"*\" } , { \"Sid\" : \"RunInstancesPermissions\" , \"Effect\" : \"Allow\" , \"Action\" : \"ec2:RunInstances\" , \"Resource\" : [ \"arn:aws:ec2:*:028075177508:network-interface/*\" , \"arn:aws:ec2:*:028075177508:security-group/*\" , \"arn:aws:ec2:*:028075177508:volume/*\" , \"arn:aws:ec2:*:028075177508:key-pair/*\" , \"arn:aws:ec2:*::image/*\" ] } ] } , \"VersionId\" : \"v83\" , \"IsDefaultVersion\" : true, \"CreateDate\" : \"2022-05-12T12:49:01+00:00\" } }","title":"AWS permissions used by CAST AI"},{"location":"guides/cloud-permissions/#aws-permissions-when-access-is-granted-using-cross-account-iam-role","text":"When enabling cost optimisation (Phase 2) for a connected cluster, there is an option to grant permissions using Cross-account IAM role. This feature allows creating a dedicated cluster user in CAST AI AWS account with a trust policy to be able to 'assume role' defined in customer's AWS account. Keeping role definition and users in separate AWS accounts allows keeping user's credentials on CAST AI side without handing them over when running on-boarding script, which provides higher security level. From customer perspective used role contains the same set of permissions as in case of regular flow (when user is created in customer's AWS account), this can be verified using following command: aws iam list-attached-role-policies --role-name <role name> aws iam list-role-policies --role-name <role name> Additionally, a trust relationship is created as follows: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"arn:aws:iam::123456789012:user/cast-crossrole-f8f82b9c-d375-40d2-9483-123456789012\" }, \"Action\" : \"sts:AssumeRole\" } ] }","title":"AWS permissions when access is granted using Cross-account IAM role"},{"location":"guides/cloud-permissions/#gcp","text":"","title":"GCP"},{"location":"guides/cloud-permissions/#overview-of-gcp-permissions-used-by-cast-ai","text":"Phase 2 on-boarding script performs several actions to get required permissions to manage GKE and GCP resources on customer's behalf: Enables required GCP services and APIs for the project Creates IAM service account and assigns required roles to it Generates IAM service account key which is used by CAST AI components to manage GKE and GCP resources on customer's behalf","title":"Overview of GCP permissions used by CAST AI"},{"location":"guides/cloud-permissions/#gcp-services-and-apis-used-by-cast-ai","text":"CAST AI enables following GCP services and APIs for the project on which customer's GKE cluster is running: GCP Service / API Group Description serviceusage.googleapis.com API to list, enable and disable GCP services iam.googleapis.com API to manage identity and access control for GCP resources cloudresourcemanager.googleapis.com API to create, read, and update metadata for GCP resource containers container.googleapis.com API to manage GKE compute.googleapis.com API to manage GCP virtual machines","title":"GCP Services and APIs used by CAST AI"},{"location":"guides/cloud-permissions/#gcp-service-account-used-by-cast-ai","text":"Phase 2 on-boarding script creates a dedicated GCP service account used by CAST AI to request and manage GCP resources on customer's behalf. The Service Account follows castai-gke-<cluster-name-hash> convention. Service account can be verified by: gcloud iam service-accounts describe castai-gke-<cluster-name-hash>@<your-gcp-project>.iam.gserviceaccount.com CAST AI created Service Account has the following roles attached: Role name Description castai.gkeAccess CAST AI managed role used to manage CAST AI add/delete node operations, full list of permissions can be found below container.developer GCP managed role for full access to Kubernetes API objects inside Kubernetes cluster iam.serviceAccountUser GCP managed role to allow run operations as the service account Full list of castai.gkeAccess role permissions: \u00bb gcloud iam roles describe --project = <your-project-name> castai.gkeAccess description: Role to manage GKE cluster via CAST AI etag: example-tag includedPermissions: - compute.addresses.use - compute.disks.create - compute.disks.setLabels - compute.disks.use - compute.images.useReadOnly - compute.instanceGroupManagers.get - compute.instanceGroupManagers.update - compute.instanceGroups.get - compute.instanceTemplates.create - compute.instanceTemplates.delete - compute.instanceTemplates.get - compute.instanceTemplates.list - compute.instances.create - compute.instances.delete - compute.instances.get - compute.instances.list - compute.instances.setLabels - compute.instances.setMetadata - compute.instances.setServiceAccount - compute.instances.setTags - compute.instances.start - compute.instances.stop - compute.networks.use - compute.networks.useExternalIp - compute.subnetworks.get - compute.subnetworks.use - compute.subnetworks.useExternalIp - compute.zones.get - compute.zones.list - container.certificateSigningRequests.approve - container.clusters.get - container.clusters.update - container.operations.get - serviceusage.services.list name: projects/<your-project-name>/roles/castai.gkeAccess stage: ALPHA title: Role to manage GKE cluster via CAST AI","title":"GCP Service Account used by CAST AI"},{"location":"guides/cluster-controller/","text":"CAST AI cluster-controller \u00b6 Cluster controller is responsible for handling certain Kubernetes actions such as draining and deleting nodes, adding labels, approving CSR requests. It's open source and can be found on github https://github.com/castai/cluster-controller Install cluster-controller \u00b6 By default cluster controller is installed during your cluster onboarding using helm chart https://github.com/castai/helm-charts/tree/main/charts/castai-cluster-controller If for some reasons it was uninstalled you can install it manually. Add CAST AI helm charts repository. helm repo add castai-helm https://castai.github.io/helm-charts helm repo update You can list all available components and versions. helm search repo castai-helm Expected example output NAME CHART VERSION APP VERSION DESCRIPTION castai-helm/castai-agent 0.18.0 v0.23.0 CAST AI agent deployment chart. castai-helm/castai-cluster-controller 0.17.0 v0.14.0 CAST AI cluster controller deployment chart. castai-helm/castai-evictor 0.10.0 0.5.1 Cluster utilization defragmentation tool castai-helm/castai-spot-handler 0.3.0 v0.3.0 CAST AI spot handler daemonset chart. Now let's install it. helm upgrade --install cluster-controller castai-helm/castai-cluster-controller -n castai-agent \\ --set castai.apiKey = <your-api-token> \\ --set castai.clusterID = <your-cluster-id> For AKS clusters you should also pass --set aks.enabled=true You can create api key via CAST AI console UI. You can find your cluster ID in CAST AI console UI. Upgrade cluster-controller \u00b6 Cluster controller supports auto-update out of the box and is enabled by default. However sometimes it cannot be updated due to changes in RBAC and requires manual upgrade. Upgrade to latest version. helm repo update helm upgrade cluster-controller castai-helm/castai-cluster-controller --reuse-values -n castai-agent \\ --set image.repository = us-docker.pkg.dev/castai-hub/library/cluster-controller Troubleshooting \u00b6 Check cluster-controller logs kubectl logs -l app.kubernetes.io/name = castai-cluster-controller -n castai-agent Auto updates \u00b6 By default cluster-controller can update itself by receiving update action (scheduled by CAST AI). However, it cannot update other components such as castai-evictor, castai-spot-handler or castai-agent. You can explicitly bind role such as cluster-admin to castai-cluster-controller service account. This will allow cluster-controller to manage all other CAST AI components automatically. cat <<EOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: castai-cluster-controller-admin roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: castai-cluster-controller namespace: castai-agent EOF","title":"Cluster controller"},{"location":"guides/cluster-controller/#cast-ai-cluster-controller","text":"Cluster controller is responsible for handling certain Kubernetes actions such as draining and deleting nodes, adding labels, approving CSR requests. It's open source and can be found on github https://github.com/castai/cluster-controller","title":"CAST AI cluster-controller"},{"location":"guides/cluster-controller/#install-cluster-controller","text":"By default cluster controller is installed during your cluster onboarding using helm chart https://github.com/castai/helm-charts/tree/main/charts/castai-cluster-controller If for some reasons it was uninstalled you can install it manually. Add CAST AI helm charts repository. helm repo add castai-helm https://castai.github.io/helm-charts helm repo update You can list all available components and versions. helm search repo castai-helm Expected example output NAME CHART VERSION APP VERSION DESCRIPTION castai-helm/castai-agent 0.18.0 v0.23.0 CAST AI agent deployment chart. castai-helm/castai-cluster-controller 0.17.0 v0.14.0 CAST AI cluster controller deployment chart. castai-helm/castai-evictor 0.10.0 0.5.1 Cluster utilization defragmentation tool castai-helm/castai-spot-handler 0.3.0 v0.3.0 CAST AI spot handler daemonset chart. Now let's install it. helm upgrade --install cluster-controller castai-helm/castai-cluster-controller -n castai-agent \\ --set castai.apiKey = <your-api-token> \\ --set castai.clusterID = <your-cluster-id> For AKS clusters you should also pass --set aks.enabled=true You can create api key via CAST AI console UI. You can find your cluster ID in CAST AI console UI.","title":"Install cluster-controller"},{"location":"guides/cluster-controller/#upgrade-cluster-controller","text":"Cluster controller supports auto-update out of the box and is enabled by default. However sometimes it cannot be updated due to changes in RBAC and requires manual upgrade. Upgrade to latest version. helm repo update helm upgrade cluster-controller castai-helm/castai-cluster-controller --reuse-values -n castai-agent \\ --set image.repository = us-docker.pkg.dev/castai-hub/library/cluster-controller","title":"Upgrade cluster-controller"},{"location":"guides/cluster-controller/#troubleshooting","text":"Check cluster-controller logs kubectl logs -l app.kubernetes.io/name = castai-cluster-controller -n castai-agent","title":"Troubleshooting"},{"location":"guides/cluster-controller/#auto-updates","text":"By default cluster-controller can update itself by receiving update action (scheduled by CAST AI). However, it cannot update other components such as castai-evictor, castai-spot-handler or castai-agent. You can explicitly bind role such as cluster-admin to castai-cluster-controller service account. This will allow cluster-controller to manage all other CAST AI components automatically. cat <<EOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: castai-cluster-controller-admin roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: castai-cluster-controller namespace: castai-agent EOF","title":"Auto updates"},{"location":"guides/connect-to-node/","text":"Connect to node \u00b6 This guide describes how to connect to your cluster node via Kubernetes or native ssh. Connect via node-shell Kubernetes plugin \u00b6 With node-shell Kubernetes plugin you can connect into your node via Kubernetes API Server as a proxy. Install node-shell \u00b6 using krew : kubectl krew index add kvaps <a href = \"https://github.com/kvaps/krew-index\" >https://github.com/kvaps/krew-index</a> kubectl krew install kvaps/node-shell or using curl: curl -LO https://github.com/kvaps/kubectl-node-shell/raw/master/kubectl-node_shell chmod +x ./kubectl-node_shell sudo mv ./kubectl-node_shell /usr/local/bin/kubectl-node_shell Example node-shell usages \u00b6 # Get standard bash shell kubectl node-shell <node> # Execute custom command kubectl node-shell <node> -- echo 123 # Use stdin cat /etc/passwd | kubectl node-shell <node> -- sh -c 'cat > /tmp/passwd' # Run oneliner script kubectl node-shell <node> -- sh -c 'cat /tmp/passwd; rm -f /tmp/passwd' You need to be able to start privileged containers for that. Connect via Lens UI \u00b6 Lens is a great Kubernetes UI tool which has builtin functionality to connect into cluster node. Connect via native ssh \u00b6 With native SSH you can connect directly into your node without Kubernetes API. Install CAST CLI \u00b6 Install official CAST CLI Example CAST CLI usage \u00b6 cast -c=cluster-name node ssh my-node-name When to use native ssh with CAST CLI? Your Kubernetes cluster is not working properly (Kubernetes API Server is not accessible etc.) You need native SSH performance, eg: packet tracing with tcpdump etc. Kubernetes node-shell plugin spin ups a new pod with root access and proxies to Kubernetes API Server which is slower that direct SSH connection.","title":"Connect to node"},{"location":"guides/connect-to-node/#connect-to-node","text":"This guide describes how to connect to your cluster node via Kubernetes or native ssh.","title":"Connect to node"},{"location":"guides/connect-to-node/#connect-via-node-shell-kubernetes-plugin","text":"With node-shell Kubernetes plugin you can connect into your node via Kubernetes API Server as a proxy.","title":"Connect via node-shell Kubernetes plugin"},{"location":"guides/connect-to-node/#install-node-shell","text":"using krew : kubectl krew index add kvaps <a href = \"https://github.com/kvaps/krew-index\" >https://github.com/kvaps/krew-index</a> kubectl krew install kvaps/node-shell or using curl: curl -LO https://github.com/kvaps/kubectl-node-shell/raw/master/kubectl-node_shell chmod +x ./kubectl-node_shell sudo mv ./kubectl-node_shell /usr/local/bin/kubectl-node_shell","title":"Install node-shell"},{"location":"guides/connect-to-node/#example-node-shell-usages","text":"# Get standard bash shell kubectl node-shell <node> # Execute custom command kubectl node-shell <node> -- echo 123 # Use stdin cat /etc/passwd | kubectl node-shell <node> -- sh -c 'cat > /tmp/passwd' # Run oneliner script kubectl node-shell <node> -- sh -c 'cat /tmp/passwd; rm -f /tmp/passwd' You need to be able to start privileged containers for that.","title":"Example node-shell usages"},{"location":"guides/connect-to-node/#connect-via-lens-ui","text":"Lens is a great Kubernetes UI tool which has builtin functionality to connect into cluster node.","title":"Connect via Lens UI"},{"location":"guides/connect-to-node/#connect-via-native-ssh","text":"With native SSH you can connect directly into your node without Kubernetes API.","title":"Connect via native ssh"},{"location":"guides/connect-to-node/#install-cast-cli","text":"Install official CAST CLI","title":"Install CAST CLI"},{"location":"guides/connect-to-node/#example-cast-cli-usage","text":"cast -c=cluster-name node ssh my-node-name When to use native ssh with CAST CLI? Your Kubernetes cluster is not working properly (Kubernetes API Server is not accessible etc.) You need native SSH performance, eg: packet tracing with tcpdump etc. Kubernetes node-shell plugin spin ups a new pod with root access and proxies to Kubernetes API Server which is slower that direct SSH connection.","title":"Example CAST CLI usage"},{"location":"guides/custom-secret-management/","text":"Custom Secret Management \u00b6 There's plenty of technologies used to manage Secrets in GitOps. Some store the encrypted Secret data in a git repository and use a cluster addon to decrypt the data during deployment, some use a reference to an external secret manager/vault. To enable the use of CAST AI Agent with custom secret managers, the Agent helm chart provides the parameter apiKeySecretRef . # Name of secret with Token to be used for authorizing agent access to the API # apiKey and apiKeySecretRef are mutually exclusive # The referenced secret must provide the token in .data[\"API_KEY\"] apiKeySecretRef : \"\" Example \u00b6 CAST AI Agent \u00b6 An example of using CAST AI Agent helm chart with custom secret: helm repo add castai-helm https://castai.github.io/helm-charts helm repo update helm upgrade --install castai-agent castai-helm/castai-agent -n castai-agent \\ --set apiKeySecretRef = <your-custom-secret> \\ --set clusterID = <your-cluster-id> CAST AI Cluster Controller \u00b6 An example of using CAST AI Cluster Controller helm chart with custom secret: helm repo add castai-helm https://castai.github.io/helm-charts helm repo update helm upgrade --install castai-agent castai-helm/castai-cluster-controller -n castai-agent \\ --set castai.apiKeySecretRef = <your-custom-secret> \\ --set castai.clusterID = <your-cluster-id>","title":"Custom secret management"},{"location":"guides/custom-secret-management/#custom-secret-management","text":"There's plenty of technologies used to manage Secrets in GitOps. Some store the encrypted Secret data in a git repository and use a cluster addon to decrypt the data during deployment, some use a reference to an external secret manager/vault. To enable the use of CAST AI Agent with custom secret managers, the Agent helm chart provides the parameter apiKeySecretRef . # Name of secret with Token to be used for authorizing agent access to the API # apiKey and apiKeySecretRef are mutually exclusive # The referenced secret must provide the token in .data[\"API_KEY\"] apiKeySecretRef : \"\"","title":"Custom Secret Management"},{"location":"guides/custom-secret-management/#example","text":"","title":"Example"},{"location":"guides/custom-secret-management/#cast-ai-agent","text":"An example of using CAST AI Agent helm chart with custom secret: helm repo add castai-helm https://castai.github.io/helm-charts helm repo update helm upgrade --install castai-agent castai-helm/castai-agent -n castai-agent \\ --set apiKeySecretRef = <your-custom-secret> \\ --set clusterID = <your-cluster-id>","title":"CAST AI Agent"},{"location":"guides/custom-secret-management/#cast-ai-cluster-controller","text":"An example of using CAST AI Cluster Controller helm chart with custom secret: helm repo add castai-helm https://castai.github.io/helm-charts helm repo update helm upgrade --install castai-agent castai-helm/castai-cluster-controller -n castai-agent \\ --set castai.apiKeySecretRef = <your-custom-secret> \\ --set castai.clusterID = <your-cluster-id>","title":"CAST AI Cluster Controller"},{"location":"guides/evictor/","text":"Evictor \u00b6 Install Evictor \u00b6 Evictor will compact your pods into fewer nodes, creating empty nodes that will be removed by the Node deletion policy. To install Evictor fo the first time run this command: helm repo add castai-helm https://castai.github.io/helm-charts helm upgrade --install castai-evictor castai-helm/castai-evictor -n castai-agent --set dryRun = false This process will take some time. Also, by default, Evictor will not cause any downtime to single replica deployments / StatefulSets, pods without ReplicaSet, meaning that those nodes can't be removed gracefully. Familiarize with rules and available overrides in order to setup Evictor to meet your needs. In order for evictor to run in more aggressive mode (start considering applications with single replica), you should pass the following parameters: --set dryRun = false,aggressiveMode = true In order for evictor to run in scoped mode (only removing nodes created by CAST AI when using scoped autoscaler), you should pass the following parameters: --set dryRun = false,scopedMode = true Evictor by default will only impact nodes older than 5 minutes, if you wish to change the grace period before a node can be considered for eviction set the nodeGracePeriodMinutes parameter to the desired time in minutes. This is useful for slow to start nodes to prevent them from being marked for eviction before they can start taking workloads. --set dryRun = false,nodeGracePeriodMinutes = 8 Upgrading Evictor \u00b6 Check the Evictor version you are currently using: helm ls -n castai-agent Update the helm chart repository to make sure that your helm command is aware of the latest charts: helm repo update Install the latest Evictor version: helm upgrade --install castai-evictor castai-helm/castai-evictor -n castai-agent --set dryRun = false Check whether the Evictor version was changed: helm ls -n castai-agent Avoiding downtime during Bin-Packing \u00b6 Evictor follows certain rules to avoid downtime. In order for the node to be considered for possible removal due to bin-packing, all of the pods running on the node must meet following criteria: A pod must be replicated: it should be managed by a Controller (e.g. ReplicaSet , ReplicationController , Deployment ), which has more than one replicas (see Overrides ) A pod is not part of StatefulSet A pod must not be marked as non-evictable (see Overrides ) All static pods (YAMLs defined in node's /etc/kubernetes/manifests by default) are considered evictable All DaemonSet -controller pods are considered evictable Rules override for specific pods or nodes \u00b6 Name Value Type ( Annotation or Label ) Location ( Pod or Node ) Effect autoscaling.cast.ai/removal-disabled \"true\" Annotation on Pod , but can be both label and annotation on Node Both Pod and Node Evictor won't try to Evict a Node with this Annotation or Node running Pod annotated with this Annotation. beta.evictor.cast.ai/disposable \"true\" Annotation Pod Evictor will treat this Pod as Evictable despite any of the other rules defined in Rules beta.evictor.cast.ai/eviction-disabled Note: Deprected. Use autoscaling.cast.ai/removal-disabled instead. \"true\" Annotation on Pod , but can be both label and annotation on Node Both Pod and Node Evictor won't try to Evict a Node with this Annotation or Node running Pod annotated with this Annotation. Examples of override applications \u00b6 Label or annotate a pod, so Evictor won't evict a node running an annotated pod (can be applied on a node as well). kubectl label pods <pod-name> beta.evictor.cast.ai/eviction-disabled = \"true\" kubectl annotate pods <pod-name> beta.evictor.cast.ai/eviction-disabled = \"true\" Label or annotate a node, to prevent eviction of pods as well as removal of the node (even when it's empty): kubectl label nodes <node-name> autoscaling.cast.ai/removal-disabled = \"true\" kubectl annotate nodes <node-name> autoscaling.cast.ai/removal-disabled = \"true\" You can also annotate a pod to make it dispossable, irrespective of other criteria that would normally make the pod un-evictable. Here is an example of a disposable pod manifest: kind: Pod metadata: name: disposable-pod annotations: beta.evictor.cast.ai/disposable: \"true\" spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 resources: requests: cpu: '1' limits: cpu: '1' Due to applied annotation, pod will be targeted for eviction even though it is not replicated. Troubleshooting \u00b6 Evictor policy is not allowed to be turned on \u00b6 The reasons why Evictor is unavailable in the policies page is that CAST AI has detected an already existing Evictor installation. If you want CAST AI to manage the Evictor instead, then you need to remove the current installation first. After you remove the evictor, follow installation steps above. How to check the logs \u00b6 To check Evictor logs, run the following command: kubectl logs -l app.kubernetes.io/name = castai-evictor -n castai-agent","title":"Evictor"},{"location":"guides/evictor/#evictor","text":"","title":"Evictor"},{"location":"guides/evictor/#install-evictor","text":"Evictor will compact your pods into fewer nodes, creating empty nodes that will be removed by the Node deletion policy. To install Evictor fo the first time run this command: helm repo add castai-helm https://castai.github.io/helm-charts helm upgrade --install castai-evictor castai-helm/castai-evictor -n castai-agent --set dryRun = false This process will take some time. Also, by default, Evictor will not cause any downtime to single replica deployments / StatefulSets, pods without ReplicaSet, meaning that those nodes can't be removed gracefully. Familiarize with rules and available overrides in order to setup Evictor to meet your needs. In order for evictor to run in more aggressive mode (start considering applications with single replica), you should pass the following parameters: --set dryRun = false,aggressiveMode = true In order for evictor to run in scoped mode (only removing nodes created by CAST AI when using scoped autoscaler), you should pass the following parameters: --set dryRun = false,scopedMode = true Evictor by default will only impact nodes older than 5 minutes, if you wish to change the grace period before a node can be considered for eviction set the nodeGracePeriodMinutes parameter to the desired time in minutes. This is useful for slow to start nodes to prevent them from being marked for eviction before they can start taking workloads. --set dryRun = false,nodeGracePeriodMinutes = 8","title":"Install Evictor"},{"location":"guides/evictor/#upgrading-evictor","text":"Check the Evictor version you are currently using: helm ls -n castai-agent Update the helm chart repository to make sure that your helm command is aware of the latest charts: helm repo update Install the latest Evictor version: helm upgrade --install castai-evictor castai-helm/castai-evictor -n castai-agent --set dryRun = false Check whether the Evictor version was changed: helm ls -n castai-agent","title":"Upgrading Evictor"},{"location":"guides/evictor/#avoiding-downtime-during-bin-packing","text":"Evictor follows certain rules to avoid downtime. In order for the node to be considered for possible removal due to bin-packing, all of the pods running on the node must meet following criteria: A pod must be replicated: it should be managed by a Controller (e.g. ReplicaSet , ReplicationController , Deployment ), which has more than one replicas (see Overrides ) A pod is not part of StatefulSet A pod must not be marked as non-evictable (see Overrides ) All static pods (YAMLs defined in node's /etc/kubernetes/manifests by default) are considered evictable All DaemonSet -controller pods are considered evictable","title":"Avoiding downtime during Bin-Packing"},{"location":"guides/evictor/#rules-override-for-specific-pods-or-nodes","text":"Name Value Type ( Annotation or Label ) Location ( Pod or Node ) Effect autoscaling.cast.ai/removal-disabled \"true\" Annotation on Pod , but can be both label and annotation on Node Both Pod and Node Evictor won't try to Evict a Node with this Annotation or Node running Pod annotated with this Annotation. beta.evictor.cast.ai/disposable \"true\" Annotation Pod Evictor will treat this Pod as Evictable despite any of the other rules defined in Rules beta.evictor.cast.ai/eviction-disabled Note: Deprected. Use autoscaling.cast.ai/removal-disabled instead. \"true\" Annotation on Pod , but can be both label and annotation on Node Both Pod and Node Evictor won't try to Evict a Node with this Annotation or Node running Pod annotated with this Annotation.","title":"Rules override for specific pods or nodes"},{"location":"guides/evictor/#examples-of-override-applications","text":"Label or annotate a pod, so Evictor won't evict a node running an annotated pod (can be applied on a node as well). kubectl label pods <pod-name> beta.evictor.cast.ai/eviction-disabled = \"true\" kubectl annotate pods <pod-name> beta.evictor.cast.ai/eviction-disabled = \"true\" Label or annotate a node, to prevent eviction of pods as well as removal of the node (even when it's empty): kubectl label nodes <node-name> autoscaling.cast.ai/removal-disabled = \"true\" kubectl annotate nodes <node-name> autoscaling.cast.ai/removal-disabled = \"true\" You can also annotate a pod to make it dispossable, irrespective of other criteria that would normally make the pod un-evictable. Here is an example of a disposable pod manifest: kind: Pod metadata: name: disposable-pod annotations: beta.evictor.cast.ai/disposable: \"true\" spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 resources: requests: cpu: '1' limits: cpu: '1' Due to applied annotation, pod will be targeted for eviction even though it is not replicated.","title":"Examples of override applications"},{"location":"guides/evictor/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"guides/evictor/#evictor-policy-is-not-allowed-to-be-turned-on","text":"The reasons why Evictor is unavailable in the policies page is that CAST AI has detected an already existing Evictor installation. If you want CAST AI to manage the Evictor instead, then you need to remove the current installation first. After you remove the evictor, follow installation steps above.","title":"Evictor policy is not allowed to be turned on"},{"location":"guides/evictor/#how-to-check-the-logs","text":"To check Evictor logs, run the following command: kubectl logs -l app.kubernetes.io/name = castai-evictor -n castai-agent","title":"How to check the logs"},{"location":"guides/external-clusters/","text":"External cluster troubleshooting \u00b6 This guide is intended for users who are experiencing issues while connecting their EKS, GCP, or AKS clusters to CAST AI. Once the cluster is connected, you can check the Status field in the Clusters overview screen to understand if cluster is operating as expected. Further sections will cover the most common issues and how to resolve them. Your cluster does not appear in the Connect Cluster screen \u00b6 If a cluster does not appear in the Connect your cluster screen after you've run the connection script, perform following steps. Check agent container logs: kubectl logs -n castai-agent -l app.kubernetes.io/name = castai-agent -c agent You might get output similar to this: time=\"2021-05-06T14:24:03Z\" level=fatal msg=\"agent failed: registering cluster: getting cluster name: describing instance_id=i-026b5fadab5b69d67: UnauthorizedOperation: You are not authorized to perform this operation.\\n\\tstatus code: 403, request id: 2165c357-b4a6-4f30-9266-a51f4aaa7ce7\" or time=\"2021-05-06T14:24:03Z\" level=fatal msg=agent failed: getting provider: configuring aws client: NoCredentialProviders: no valid providers in chain\" These errors indicate that the CAST AI Agent failed to connect to the AWS API either because the nodes and/or workloads running in your cluster have custom constrained IAM permissions or the IAM roles are removed entirely. However, the CAST AI Agent requires read-only access to the AWS EC2 API to correctly identify some properties of your EKS cluster. Access to the AWS EC2 Metadata endpoint is optional, but the variables discovered from the endpoint must then be provided. The CAST AI Agent uses the official AWS SDK, so all variables to customize your authentication mentioned in its documentation are supported. Provide cluster metadata by adding these environment variables to the CAST AI Agent deployment: - name: EKS_ACCOUNT_ID value: \"000000000000\" # your aws account id - name: EKS_REGION value: \"eu-central-1\" # your eks cluster region - name: EKS_CLUSTER_NAME value: \"staging-example\" # your eks cluster name Spot nodes are displayed as On-demand in your cluster's Available Savings page \u00b6 The CAST AI agent requires read-only permissions, so the default AmazonEC2ReadOnlyAccess is enough. Provide AWS API access by adding these variables to the CAST AI Agent secret: AWS_ACCESS_KEY_ID = xxxxxxxxxxxxxxxxxxxx AWS_SECRET_ACCESS_KEY = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Here is an example of a CAST AI Agent deployment and secret with all the mentioned environment variables added: # Source: castai-agent/templates/deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : castai-agent namespace : castai-agent labels : app.kubernetes.io/name : castai-agent app.kubernetes.io/instance : castai-agent app.kubernetes.io/version : \"v0.23.0\" app.kubernetes.io/managed-by : castai spec : replicas : 1 selector : matchLabels : app.kubernetes.io/name : castai-agent app.kubernetes.io/instance : castai-agent template : metadata : labels : app.kubernetes.io/name : castai-agent app.kubernetes.io/instance : castai-agent spec : priorityClassName : system-cluster-critical serviceAccountName : castai-agent affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : \"kubernetes.io/os\" operator : In values : [ \"linux\" ] - matchExpressions : - key : \"beta.kubernetes.io/os\" operator : In values : [ \"linux\" ] containers : - name : agent image : \"us-docker.pkg.dev/castai-hub/library/agent:v0.24.0\" imagePullPolicy : IfNotPresent env : - name : API_URL value : \"api.cast.ai\" - name : PPROF_PORT value : \"6060\" - name : PROVIDER value : \"eks\" # Provide values discovered via AWS EC2 Metadata endpoint: - name : EKS_ACCOUNT_ID value : \"000000000000\" - name : EKS_REGION value : \"eu-central-1\" - name : EKS_CLUSTER_NAME value : \"castai-example\" envFrom : - secretRef : name : castai-agent resources : requests : cpu : 100m limits : cpu : 1000m - name : autoscaler image : k8s.gcr.io/cpvpa-amd64:v0.8.3 command : - /cpvpa - --target=deployment/castai-agent - --namespace=castai-agent - --poll-period-seconds=300 - --config-file=/etc/config/castai-agent-autoscaler volumeMounts : - mountPath : /etc/config name : autoscaler-config volumes : - name : autoscaler-config configMap : name : castai-agent-autoscaler # Source: castai-agent/templates/secret.yaml apiVersion : v1 kind : Secret metadata : name : castai-agent namespace : castai-agent labels : app.kubernetes.io/instance : castai-agent app.kubernetes.io/managed-by : castai app.kubernetes.io/name : castai-agent app.kubernetes.io/version : \"v0.23.0\" data : # Keep API_KEY unchanged. API_KEY : \"xxxxxxxxxxxxxxxxxxxx\" # Provide an AWS Access Key to enable read-only AWS EC2 API access: AWS_ACCESS_KEY_ID : \"xxxxxxxxxxxxxxxxxxxx\" AWS_SECRET_ACCESS_KEY : \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" Alternatively, if you are using IAM roles for service accounts instead of providing AWS credentials you can annotate castai-agent service account with your IAM role. kubectl annotate serviceaccount -n castai-agent castai-agent eks.amazonaws.com/role-arn = \"arn:aws:iam::111122223333:role/iam-role-name\" TLS handshake timeout issue \u00b6 In some edge cases due to specific cluster network setup agent might fail with the following message in the agent container logs: time=\"2021-11-13T05:19:54Z\" level=fatal msg=\"agent failed: registering cluster: getting namespace \\\"kube-system\\\": Get \\\"https://100.10.1.0:443/api/v1/namespaces/kube-system\\\": net/http: TLS handshake timeout\" provider=eks version=v0.22.1 To resolve this issue delete castai-agent pod. The deployment will recreate the pod and issue will be resolved. Refused connection to control plane \u00b6 When enabling cluster optimization for the first time, the user runs the pre-generated script to grant required permissions to CAST AI as shown below. Error message No access to Kubernetes API server, please check your firewall settings indicates that a firewall prevents communication between the control plane and CAST AI. To solve this issue, allow access to CAST AI IP 35.221.40.21 and then enable optimization again. Disconnected or Not responding cluster \u00b6 If cluster has a Not responding status, most likely the CAST AI agent deployment is missing. Press Reconnect and follow the instructions provided. The Not responding state is temporary and if not fixed, the cluster will enter into the Disconnected state. A disconnected cluster can be reconnected or deleted from the console as shown. The delete action only removes the cluster from the CAST AI console, leaving it running in the cloud service provider. Upgrading the agent \u00b6 To check which agent version is running on your cluster, run the following command: kubectl describe pod castai-agent -n castai-agent | grep castai-hub/library/agent:v You can cross-check our Github repository for the number of the latest version available. In order to upgrade the CAST AI agent version, please perform the following steps: Go to Connect cluster Select the correct cloud service provider Run the provided script In case of an error when upgrading the agent: i.e. MatchExpressions:[]v1.LabelSelectorRequirement(nil)}: field is immutable run the following command: kubectl delete deployment -n castai-agent castai-agent and repeat the step 3. The latest version of CAST AI agent is now deployed in your cluster. Deleted agent \u00b6 In case CAST AI agent deployment got deleted from the cluster, you can re-install the agent by re-running the script from Connect cluster screen. Please ensure you have chosen the correct cloud service provider. Tip If you are still encountering any issues, ping us with logs output at: https://castai-community.slack.com/ Cluster-controller is receiving forbidden access \u00b6 In some scenarios, during multiple onboardings, failing updates or other issues, cluster token that is used by cluster-controller, can get invalidated, and become forbidden from accessing CAST AI API, thus failing to operate the cluster. To renew it, the following Helm commands should be ran. helm repo update helm upgrade -i cluster-controller castai-helm/castai-cluster-controller -n castai-agent \\ --set castai.apiKey = $CASTAI_API_TOKEN \\ --set castai.clusterID = 46d0d683-da70-4f69-b970-8d6509001c10 AKS Fail to pull images from Azure Container Registry to Azure Kubernetes Service cluster \u00b6 In case the cluster is already attached to an ACR, after onboarding on CAST AI, the Service Principal created to manage the cluster might not have the correct permissions to pull images from the private ACRs, resulting in failed to pull and unpack image, failed to fetch oauth token: unexpected status: 401 Unauthorized when new nodes are created. Microsoft has a detailed documentation on how to troubleshoot and fix the issue Fail to pull images from Azure Container Registry to Azure Kubernetes Service cluster . In most cases, Solution 1: Ensure AcrPull role assignment is created for identity is enough to resolve it.","title":"External cluster troubleshooting"},{"location":"guides/external-clusters/#external-cluster-troubleshooting","text":"This guide is intended for users who are experiencing issues while connecting their EKS, GCP, or AKS clusters to CAST AI. Once the cluster is connected, you can check the Status field in the Clusters overview screen to understand if cluster is operating as expected. Further sections will cover the most common issues and how to resolve them.","title":"External cluster troubleshooting"},{"location":"guides/external-clusters/#your-cluster-does-not-appear-in-the-connect-cluster-screen","text":"If a cluster does not appear in the Connect your cluster screen after you've run the connection script, perform following steps. Check agent container logs: kubectl logs -n castai-agent -l app.kubernetes.io/name = castai-agent -c agent You might get output similar to this: time=\"2021-05-06T14:24:03Z\" level=fatal msg=\"agent failed: registering cluster: getting cluster name: describing instance_id=i-026b5fadab5b69d67: UnauthorizedOperation: You are not authorized to perform this operation.\\n\\tstatus code: 403, request id: 2165c357-b4a6-4f30-9266-a51f4aaa7ce7\" or time=\"2021-05-06T14:24:03Z\" level=fatal msg=agent failed: getting provider: configuring aws client: NoCredentialProviders: no valid providers in chain\" These errors indicate that the CAST AI Agent failed to connect to the AWS API either because the nodes and/or workloads running in your cluster have custom constrained IAM permissions or the IAM roles are removed entirely. However, the CAST AI Agent requires read-only access to the AWS EC2 API to correctly identify some properties of your EKS cluster. Access to the AWS EC2 Metadata endpoint is optional, but the variables discovered from the endpoint must then be provided. The CAST AI Agent uses the official AWS SDK, so all variables to customize your authentication mentioned in its documentation are supported. Provide cluster metadata by adding these environment variables to the CAST AI Agent deployment: - name: EKS_ACCOUNT_ID value: \"000000000000\" # your aws account id - name: EKS_REGION value: \"eu-central-1\" # your eks cluster region - name: EKS_CLUSTER_NAME value: \"staging-example\" # your eks cluster name","title":"Your cluster does not appear in the Connect Cluster screen"},{"location":"guides/external-clusters/#spot-nodes-are-displayed-as-on-demand-in-your-clusters-available-savings-page","text":"The CAST AI agent requires read-only permissions, so the default AmazonEC2ReadOnlyAccess is enough. Provide AWS API access by adding these variables to the CAST AI Agent secret: AWS_ACCESS_KEY_ID = xxxxxxxxxxxxxxxxxxxx AWS_SECRET_ACCESS_KEY = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx Here is an example of a CAST AI Agent deployment and secret with all the mentioned environment variables added: # Source: castai-agent/templates/deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : castai-agent namespace : castai-agent labels : app.kubernetes.io/name : castai-agent app.kubernetes.io/instance : castai-agent app.kubernetes.io/version : \"v0.23.0\" app.kubernetes.io/managed-by : castai spec : replicas : 1 selector : matchLabels : app.kubernetes.io/name : castai-agent app.kubernetes.io/instance : castai-agent template : metadata : labels : app.kubernetes.io/name : castai-agent app.kubernetes.io/instance : castai-agent spec : priorityClassName : system-cluster-critical serviceAccountName : castai-agent affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : \"kubernetes.io/os\" operator : In values : [ \"linux\" ] - matchExpressions : - key : \"beta.kubernetes.io/os\" operator : In values : [ \"linux\" ] containers : - name : agent image : \"us-docker.pkg.dev/castai-hub/library/agent:v0.24.0\" imagePullPolicy : IfNotPresent env : - name : API_URL value : \"api.cast.ai\" - name : PPROF_PORT value : \"6060\" - name : PROVIDER value : \"eks\" # Provide values discovered via AWS EC2 Metadata endpoint: - name : EKS_ACCOUNT_ID value : \"000000000000\" - name : EKS_REGION value : \"eu-central-1\" - name : EKS_CLUSTER_NAME value : \"castai-example\" envFrom : - secretRef : name : castai-agent resources : requests : cpu : 100m limits : cpu : 1000m - name : autoscaler image : k8s.gcr.io/cpvpa-amd64:v0.8.3 command : - /cpvpa - --target=deployment/castai-agent - --namespace=castai-agent - --poll-period-seconds=300 - --config-file=/etc/config/castai-agent-autoscaler volumeMounts : - mountPath : /etc/config name : autoscaler-config volumes : - name : autoscaler-config configMap : name : castai-agent-autoscaler # Source: castai-agent/templates/secret.yaml apiVersion : v1 kind : Secret metadata : name : castai-agent namespace : castai-agent labels : app.kubernetes.io/instance : castai-agent app.kubernetes.io/managed-by : castai app.kubernetes.io/name : castai-agent app.kubernetes.io/version : \"v0.23.0\" data : # Keep API_KEY unchanged. API_KEY : \"xxxxxxxxxxxxxxxxxxxx\" # Provide an AWS Access Key to enable read-only AWS EC2 API access: AWS_ACCESS_KEY_ID : \"xxxxxxxxxxxxxxxxxxxx\" AWS_SECRET_ACCESS_KEY : \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" Alternatively, if you are using IAM roles for service accounts instead of providing AWS credentials you can annotate castai-agent service account with your IAM role. kubectl annotate serviceaccount -n castai-agent castai-agent eks.amazonaws.com/role-arn = \"arn:aws:iam::111122223333:role/iam-role-name\"","title":"Spot nodes are displayed as On-demand in your cluster's Available Savings page"},{"location":"guides/external-clusters/#tls-handshake-timeout-issue","text":"In some edge cases due to specific cluster network setup agent might fail with the following message in the agent container logs: time=\"2021-11-13T05:19:54Z\" level=fatal msg=\"agent failed: registering cluster: getting namespace \\\"kube-system\\\": Get \\\"https://100.10.1.0:443/api/v1/namespaces/kube-system\\\": net/http: TLS handshake timeout\" provider=eks version=v0.22.1 To resolve this issue delete castai-agent pod. The deployment will recreate the pod and issue will be resolved.","title":"TLS handshake timeout issue"},{"location":"guides/external-clusters/#refused-connection-to-control-plane","text":"When enabling cluster optimization for the first time, the user runs the pre-generated script to grant required permissions to CAST AI as shown below. Error message No access to Kubernetes API server, please check your firewall settings indicates that a firewall prevents communication between the control plane and CAST AI. To solve this issue, allow access to CAST AI IP 35.221.40.21 and then enable optimization again.","title":"Refused connection to control plane"},{"location":"guides/external-clusters/#disconnected-or-not-responding-cluster","text":"If cluster has a Not responding status, most likely the CAST AI agent deployment is missing. Press Reconnect and follow the instructions provided. The Not responding state is temporary and if not fixed, the cluster will enter into the Disconnected state. A disconnected cluster can be reconnected or deleted from the console as shown. The delete action only removes the cluster from the CAST AI console, leaving it running in the cloud service provider.","title":"Disconnected or Not responding cluster"},{"location":"guides/external-clusters/#upgrading-the-agent","text":"To check which agent version is running on your cluster, run the following command: kubectl describe pod castai-agent -n castai-agent | grep castai-hub/library/agent:v You can cross-check our Github repository for the number of the latest version available. In order to upgrade the CAST AI agent version, please perform the following steps: Go to Connect cluster Select the correct cloud service provider Run the provided script In case of an error when upgrading the agent: i.e. MatchExpressions:[]v1.LabelSelectorRequirement(nil)}: field is immutable run the following command: kubectl delete deployment -n castai-agent castai-agent and repeat the step 3. The latest version of CAST AI agent is now deployed in your cluster.","title":"Upgrading the agent"},{"location":"guides/external-clusters/#deleted-agent","text":"In case CAST AI agent deployment got deleted from the cluster, you can re-install the agent by re-running the script from Connect cluster screen. Please ensure you have chosen the correct cloud service provider. Tip If you are still encountering any issues, ping us with logs output at: https://castai-community.slack.com/","title":"Deleted agent"},{"location":"guides/external-clusters/#cluster-controller-is-receiving-forbidden-access","text":"In some scenarios, during multiple onboardings, failing updates or other issues, cluster token that is used by cluster-controller, can get invalidated, and become forbidden from accessing CAST AI API, thus failing to operate the cluster. To renew it, the following Helm commands should be ran. helm repo update helm upgrade -i cluster-controller castai-helm/castai-cluster-controller -n castai-agent \\ --set castai.apiKey = $CASTAI_API_TOKEN \\ --set castai.clusterID = 46d0d683-da70-4f69-b970-8d6509001c10","title":"Cluster-controller is receiving forbidden access"},{"location":"guides/external-clusters/#aks-fail-to-pull-images-from-azure-container-registry-to-azure-kubernetes-service-cluster","text":"In case the cluster is already attached to an ACR, after onboarding on CAST AI, the Service Principal created to manage the cluster might not have the correct permissions to pull images from the private ACRs, resulting in failed to pull and unpack image, failed to fetch oauth token: unexpected status: 401 Unauthorized when new nodes are created. Microsoft has a detailed documentation on how to troubleshoot and fix the issue Fail to pull images from Azure Container Registry to Azure Kubernetes Service cluster . In most cases, Solution 1: Ensure AcrPull role assignment is created for identity is enough to resolve it.","title":"AKS Fail to pull images from Azure Container Registry to Azure Kubernetes Service cluster"},{"location":"guides/gpu/","text":"GPU instances autoscaling \u00b6 The CAST AI autoscaler supports running your workloads on GPU-optimized instances. This guide will help you configure and run it in 5 minutes. Supported providers \u00b6 Provider GPUs supported AWS EKS NVIDIA (AMD coming soon) GCP GKE coming soon EKS KOPS coming soon Azure AKS coming soon Configuration \u00b6 GPU Driver \u00b6 A GPU-specific driver should be installed on the cluster to run GPU workloads. CAST AI verifies and ensures that the required driver is available on the cluster before provisioning the required nodes. How to install the NVIDIA driver \u00b6 Onboard a cluster to CAST AI by providing additional variable INSTALL_NVIDIA_DEVICE_PLUGIN=true Install it from NVIDIA repository: helm repo add nvdp https://nvidia.github.io/k8s-device-plugin helm repo update noglob helm upgrade -i nvdp nvdp/nvidia-device-plugin -n castai-agent \\ --set-string nodeSelector. \"nvidia\\.com/gpu\" = true \\ --set \\ tolerations [ 0 ] .key = CriticalAddonsOnly,tolerations [ 0 ] .operator = Exists, \\ tolerations [ 1 ] .effect = NoSchedule,tolerations [ 1 ] .key = \"nvidia\\.com/gpu\" ,tolerations [ 1 ] .operator = Exists, \\ tolerations [ 2 ] .key = \"scheduling\\.cast\\.ai/spot\" ,tolerations [ 2 ] .operator = Exists, \\ tolerations [ 3 ] .key = \"scheduling\\.cast\\.ai/scoped-autoscaler\" ,tolerations [ 3 ] .operator = Exists, \\ tolerations [ 4 ] .key = \"scheduling\\.cast\\.ai/node-template\" ,tolerations [ 4 ] .operator = Exists Use your own plugin. CAST AI does a plugin compatibility check with a new node before provisioning it, so CAST AI should detect a plugin in order to perform the check. Plugin will be detected by CAST AI if one of these conditions are honored: plugin daemon set name pattern is *nvidia-device-plugin* plugin daemon set has label nvidia-device-plugin: \"true\" Workload configuration \u00b6 To request a node that has an attached GPU, workload should: define (at least) GPU limits in the workload resources: resources : requests : cpu : 1 memory : 1Gi nvidia.com/gpu : 1 limits : memory : 1Gi nvidia.com/gpu : 1 add a toleration for GPU node ( toleration is required because CAST AI adds a taint on GPU nodes so that these nodes could be used only by workloads that truly require GPUs ): spec : tolerations : - key : \"nvidia.com/gpu\" operator : Exists","title":"GPU instances autoscaling"},{"location":"guides/gpu/#gpu-instances-autoscaling","text":"The CAST AI autoscaler supports running your workloads on GPU-optimized instances. This guide will help you configure and run it in 5 minutes.","title":"GPU instances autoscaling"},{"location":"guides/gpu/#supported-providers","text":"Provider GPUs supported AWS EKS NVIDIA (AMD coming soon) GCP GKE coming soon EKS KOPS coming soon Azure AKS coming soon","title":"Supported providers"},{"location":"guides/gpu/#configuration","text":"","title":"Configuration"},{"location":"guides/gpu/#gpu-driver","text":"A GPU-specific driver should be installed on the cluster to run GPU workloads. CAST AI verifies and ensures that the required driver is available on the cluster before provisioning the required nodes.","title":"GPU Driver"},{"location":"guides/gpu/#how-to-install-the-nvidia-driver","text":"Onboard a cluster to CAST AI by providing additional variable INSTALL_NVIDIA_DEVICE_PLUGIN=true Install it from NVIDIA repository: helm repo add nvdp https://nvidia.github.io/k8s-device-plugin helm repo update noglob helm upgrade -i nvdp nvdp/nvidia-device-plugin -n castai-agent \\ --set-string nodeSelector. \"nvidia\\.com/gpu\" = true \\ --set \\ tolerations [ 0 ] .key = CriticalAddonsOnly,tolerations [ 0 ] .operator = Exists, \\ tolerations [ 1 ] .effect = NoSchedule,tolerations [ 1 ] .key = \"nvidia\\.com/gpu\" ,tolerations [ 1 ] .operator = Exists, \\ tolerations [ 2 ] .key = \"scheduling\\.cast\\.ai/spot\" ,tolerations [ 2 ] .operator = Exists, \\ tolerations [ 3 ] .key = \"scheduling\\.cast\\.ai/scoped-autoscaler\" ,tolerations [ 3 ] .operator = Exists, \\ tolerations [ 4 ] .key = \"scheduling\\.cast\\.ai/node-template\" ,tolerations [ 4 ] .operator = Exists Use your own plugin. CAST AI does a plugin compatibility check with a new node before provisioning it, so CAST AI should detect a plugin in order to perform the check. Plugin will be detected by CAST AI if one of these conditions are honored: plugin daemon set name pattern is *nvidia-device-plugin* plugin daemon set has label nvidia-device-plugin: \"true\"","title":"How to install the NVIDIA driver"},{"location":"guides/gpu/#workload-configuration","text":"To request a node that has an attached GPU, workload should: define (at least) GPU limits in the workload resources: resources : requests : cpu : 1 memory : 1Gi nvidia.com/gpu : 1 limits : memory : 1Gi nvidia.com/gpu : 1 add a toleration for GPU node ( toleration is required because CAST AI adds a taint on GPU nodes so that these nodes could be used only by workloads that truly require GPUs ): spec : tolerations : - key : \"nvidia.com/gpu\" operator : Exists","title":"Workload configuration"},{"location":"guides/hpa/","text":"Horizontal Pod Autoscaler \u00b6 Scaling an application \u00b6 You can scale an application in two ways: Vertically: by adding more resources (RAM/CPU/Disk IOPS) to the same instance, Horizontally: by adding more instances (replicas) of the same application. The problem with vertical scaling is that either the required hardware (RAM, CPU, Disk IOPS) in a single machine costs too much, or the cloud provider cannot provision a machine with enough resources. We use replica sets in Kubernetes to achieve horizontal scaling. The Horizontal Pod Autoscaler allows automating the process of maintaining the replica count proportionally to the application load. Horizontal scaling strategy \u00b6 The horizontal scaling strategy involves adding (or removing) the additional replicas of the same application. The problem here lies in the fact that most application load patterns can have spikes that are not predictable. This renders manual scaling nearly impossible. Luckily, we can automate this process. The HPA & KEDA \u00b6 Kubernetes has the Horizontal Pod Autoscaler (HPA) functionality. It can scale up (add more replicas) or down (remove idling replicas) based on some metrics. However, HPA does not have the metrics' source by default. CAST AI offers you a solution with the KEDA addon. How does it work \u00b6 KEDA consists of two components: operator - watches k8s for ScaledObject resources and configures HPA accordingly metrics-apiserver - a bridge between Kubernetes and various scaling sources (including Prometheus) These components configure Kubernetes HPA and set up the custom metric sources. This enables us to autoscale almost any workload: Deployment , ReplicaSet , ReplicationController , or StatefulSet . KEDA supports autoscaling Jobs as well. Examples \u00b6 Autoscale Based on CPU and/or Memory usage \u00b6 Let's create a Deployment and a Service that we will Autoscale : apiVersion : apps/v1 kind : Deployment metadata : name : sample-app labels : app : sample-app spec : # Note that we omit the replica count so # when we redeploy, we wouldn't override # replica count set by the autoscaler #replicas: 1 selector : matchLabels : app : sample-app template : metadata : labels : app : sample-app spec : containers : - image : luxas/autoscale-demo:v0.1.2 name : sample-app ports : - containerPort : 8080 --- apiVersion : v1 kind : Service metadata : name : sample-app labels : app : sample-app spec : ports : - port : 8080 name : http targetPort : 8080 protocol : TCP selector : app : sample-app Note : We do not specify the ReplicaCount ourselves Now set up a CPU-based Autoscaler apiVersion : keda.sh/v1alpha1 kind : ScaledObject metadata : name : sample-app spec : scaleTargetRef : name : sample-app minReplicaCount : 1 # Optional. Default: 0 maxReplicaCount : 10 # Optional. Default: 100 triggers : # Either of the triggers can be omitted. - type : cpu metadata : # Possible values: `Value`, `Utilization`, or `AverageValue`. # More info at: https://keda.sh/docs/2.0/scalers/cpu/#trigger-specification type : \"Value\" value : \"30\" - type : memory metadata : # Possible values: `Value`, `Utilization`, or `AverageValue`. # More info at: https://keda.sh/docs/2.0/scalers/memory/ type : \"Value\" value : \"512\" Now our Deployment autoscaling will be triggered either by CPU or Memory usage. We could use any other trigger, or remove either of those if we want (i.e. to autoscale only on the CPU basis and remove the Memory trigger, or vice-versa). Autoscale based on the Prometheus metric \u00b6 It is possible to autoscale based on the result of an arbitrary Prometheus query. CAST AI k8s clusters come with Prometheus deployed out-of-the-box. Let's deploy the sample application again and instruct Prometheus to scrape metrics: apiVersion : apps/v1 kind : Deployment metadata : name : sample-app labels : app : sample-app spec : selector : matchLabels : app : sample-app template : metadata : labels : app : sample-app annotations : # These annotations the main difference! prometheus.io/path : \"/metrics\" prometheus.io/port : \"8080\" prometheus.io/scrape : \"true\" spec : containers : - image : luxas/autoscale-demo:v0.1.2 name : sample-app ports : - containerPort : 8080 --- apiVersion : v1 kind : Service metadata : name : sample-app labels : app : sample-app spec : ports : - port : 8080 name : http targetPort : 8080 protocol : TCP selector : app : sample-app Now let's deploy the Autoscaler. apiVersion : keda.sh/v1alpha1 kind : ScaledObject metadata : name : sample-app spec : scaleTargetRef : name : sample-app minReplicaCount : 1 # Optional. Default: 0 maxReplicaCount : 10 # Optional. Default: 100 triggers : - type : prometheus metadata : serverAddress : http://prom.castai:9090 metricName : http_requests_total_sample_app threshold : '1' # Note: query must return a vector/scalar single element response query : sum(rate(http_requests_total{app=\"sample-app\"}[2m])) Now let's generate some load and observe that the replica count is increased: # Deploy busybox image kubectl run -it --rm load-generator --image = busybox /bin/sh # Hit ENTER for command prompt # trigger infinite requests to the php-apache server while true ; do wget -q -O- http://sample-app:8080/metrics ; done # in order to cancel, hold CTRL+C # in order to quit, initiate CTRL+D sequence Troubleshooting \u00b6 Verify that KEDA is scheduled and running (the suffixes might be different): $ kubectl get pods -n keda NAME READY STATUS RESTARTS AGE keda-metrics-apiserver-59679c9f96-5lfr5 1 /1 Running 0 74m keda-operator-66744fc69d-7njdd 1 /1 Running 0 74m Describe ScaledObject for clues. In this case, scaledObjectRef points to nonexistent object: $ kubectl describe scaledobjects.keda.sh sample-app Name: sample-app Namespace: default Labels: scaledObjectName = sample-app Annotations: API Version: keda.sh/v1alpha1 Kind: ScaledObject Metadata: Creation Timestamp: 2020 -11-10T10:12:38Z Finalizers: finalizer.keda.sh Generation: 1 Managed Fields: <... snip ...> Resource Version: 394466 Self Link: /apis/keda.sh/v1alpha1/namespaces/default/scaledobjects/sample-app UID: 9394d57a-ae66-4e80-baf4-8d6bb7fd36f9 Spec: Advanced: Horizontal Pod Autoscaler Config: Behavior: Scale Down: Policies: Period Seconds: 15 Type: Percent Value: 100 Stabilization Window Seconds: 300 Restore To Original Replica Count: true Cooldown Period: 300 Max Replica Count: 10 Min Replica Count: 1 Polling Interval: 30 Scale Target Ref: API Version: apps/v1 Kind: Deployment Name: sample-app Triggers: Metadata: Metric Name: http_requests_total Query: sum ( rate ( http_requests_total { app = \"sample-app\" }[ 2m ])) Server Address: http://prom.castai:9090 Threshold: 1 Type: prometheus Status: Conditions: Message: ScaledObject doesn 't have correct scaleTargetRef specification Reason: ScaledObjectCheckFailed Status: False <--------- This means that this check didn' t pass Type: Ready Message: ScaledObject check failed Reason: UnkownState Status: Unknown Type: Active Events: <none> Inspect KEDA operator logs: kubectl logs -n keda $( kubectl get pods -n keda -o name | grep operator )","title":"Horizontal Pod Autoscaler"},{"location":"guides/hpa/#horizontal-pod-autoscaler","text":"","title":"Horizontal Pod Autoscaler"},{"location":"guides/hpa/#scaling-an-application","text":"You can scale an application in two ways: Vertically: by adding more resources (RAM/CPU/Disk IOPS) to the same instance, Horizontally: by adding more instances (replicas) of the same application. The problem with vertical scaling is that either the required hardware (RAM, CPU, Disk IOPS) in a single machine costs too much, or the cloud provider cannot provision a machine with enough resources. We use replica sets in Kubernetes to achieve horizontal scaling. The Horizontal Pod Autoscaler allows automating the process of maintaining the replica count proportionally to the application load.","title":"Scaling an application"},{"location":"guides/hpa/#horizontal-scaling-strategy","text":"The horizontal scaling strategy involves adding (or removing) the additional replicas of the same application. The problem here lies in the fact that most application load patterns can have spikes that are not predictable. This renders manual scaling nearly impossible. Luckily, we can automate this process.","title":"Horizontal scaling strategy"},{"location":"guides/hpa/#the-hpa-keda","text":"Kubernetes has the Horizontal Pod Autoscaler (HPA) functionality. It can scale up (add more replicas) or down (remove idling replicas) based on some metrics. However, HPA does not have the metrics' source by default. CAST AI offers you a solution with the KEDA addon.","title":"The HPA &amp; KEDA"},{"location":"guides/hpa/#how-does-it-work","text":"KEDA consists of two components: operator - watches k8s for ScaledObject resources and configures HPA accordingly metrics-apiserver - a bridge between Kubernetes and various scaling sources (including Prometheus) These components configure Kubernetes HPA and set up the custom metric sources. This enables us to autoscale almost any workload: Deployment , ReplicaSet , ReplicationController , or StatefulSet . KEDA supports autoscaling Jobs as well.","title":"How does it work"},{"location":"guides/hpa/#examples","text":"","title":"Examples"},{"location":"guides/hpa/#autoscale-based-on-cpu-andor-memory-usage","text":"Let's create a Deployment and a Service that we will Autoscale : apiVersion : apps/v1 kind : Deployment metadata : name : sample-app labels : app : sample-app spec : # Note that we omit the replica count so # when we redeploy, we wouldn't override # replica count set by the autoscaler #replicas: 1 selector : matchLabels : app : sample-app template : metadata : labels : app : sample-app spec : containers : - image : luxas/autoscale-demo:v0.1.2 name : sample-app ports : - containerPort : 8080 --- apiVersion : v1 kind : Service metadata : name : sample-app labels : app : sample-app spec : ports : - port : 8080 name : http targetPort : 8080 protocol : TCP selector : app : sample-app Note : We do not specify the ReplicaCount ourselves Now set up a CPU-based Autoscaler apiVersion : keda.sh/v1alpha1 kind : ScaledObject metadata : name : sample-app spec : scaleTargetRef : name : sample-app minReplicaCount : 1 # Optional. Default: 0 maxReplicaCount : 10 # Optional. Default: 100 triggers : # Either of the triggers can be omitted. - type : cpu metadata : # Possible values: `Value`, `Utilization`, or `AverageValue`. # More info at: https://keda.sh/docs/2.0/scalers/cpu/#trigger-specification type : \"Value\" value : \"30\" - type : memory metadata : # Possible values: `Value`, `Utilization`, or `AverageValue`. # More info at: https://keda.sh/docs/2.0/scalers/memory/ type : \"Value\" value : \"512\" Now our Deployment autoscaling will be triggered either by CPU or Memory usage. We could use any other trigger, or remove either of those if we want (i.e. to autoscale only on the CPU basis and remove the Memory trigger, or vice-versa).","title":"Autoscale Based on CPU and/or Memory usage"},{"location":"guides/hpa/#autoscale-based-on-the-prometheus-metric","text":"It is possible to autoscale based on the result of an arbitrary Prometheus query. CAST AI k8s clusters come with Prometheus deployed out-of-the-box. Let's deploy the sample application again and instruct Prometheus to scrape metrics: apiVersion : apps/v1 kind : Deployment metadata : name : sample-app labels : app : sample-app spec : selector : matchLabels : app : sample-app template : metadata : labels : app : sample-app annotations : # These annotations the main difference! prometheus.io/path : \"/metrics\" prometheus.io/port : \"8080\" prometheus.io/scrape : \"true\" spec : containers : - image : luxas/autoscale-demo:v0.1.2 name : sample-app ports : - containerPort : 8080 --- apiVersion : v1 kind : Service metadata : name : sample-app labels : app : sample-app spec : ports : - port : 8080 name : http targetPort : 8080 protocol : TCP selector : app : sample-app Now let's deploy the Autoscaler. apiVersion : keda.sh/v1alpha1 kind : ScaledObject metadata : name : sample-app spec : scaleTargetRef : name : sample-app minReplicaCount : 1 # Optional. Default: 0 maxReplicaCount : 10 # Optional. Default: 100 triggers : - type : prometheus metadata : serverAddress : http://prom.castai:9090 metricName : http_requests_total_sample_app threshold : '1' # Note: query must return a vector/scalar single element response query : sum(rate(http_requests_total{app=\"sample-app\"}[2m])) Now let's generate some load and observe that the replica count is increased: # Deploy busybox image kubectl run -it --rm load-generator --image = busybox /bin/sh # Hit ENTER for command prompt # trigger infinite requests to the php-apache server while true ; do wget -q -O- http://sample-app:8080/metrics ; done # in order to cancel, hold CTRL+C # in order to quit, initiate CTRL+D sequence","title":"Autoscale based on the Prometheus metric"},{"location":"guides/hpa/#troubleshooting","text":"Verify that KEDA is scheduled and running (the suffixes might be different): $ kubectl get pods -n keda NAME READY STATUS RESTARTS AGE keda-metrics-apiserver-59679c9f96-5lfr5 1 /1 Running 0 74m keda-operator-66744fc69d-7njdd 1 /1 Running 0 74m Describe ScaledObject for clues. In this case, scaledObjectRef points to nonexistent object: $ kubectl describe scaledobjects.keda.sh sample-app Name: sample-app Namespace: default Labels: scaledObjectName = sample-app Annotations: API Version: keda.sh/v1alpha1 Kind: ScaledObject Metadata: Creation Timestamp: 2020 -11-10T10:12:38Z Finalizers: finalizer.keda.sh Generation: 1 Managed Fields: <... snip ...> Resource Version: 394466 Self Link: /apis/keda.sh/v1alpha1/namespaces/default/scaledobjects/sample-app UID: 9394d57a-ae66-4e80-baf4-8d6bb7fd36f9 Spec: Advanced: Horizontal Pod Autoscaler Config: Behavior: Scale Down: Policies: Period Seconds: 15 Type: Percent Value: 100 Stabilization Window Seconds: 300 Restore To Original Replica Count: true Cooldown Period: 300 Max Replica Count: 10 Min Replica Count: 1 Polling Interval: 30 Scale Target Ref: API Version: apps/v1 Kind: Deployment Name: sample-app Triggers: Metadata: Metric Name: http_requests_total Query: sum ( rate ( http_requests_total { app = \"sample-app\" }[ 2m ])) Server Address: http://prom.castai:9090 Threshold: 1 Type: prometheus Status: Conditions: Message: ScaledObject doesn 't have correct scaleTargetRef specification Reason: ScaledObjectCheckFailed Status: False <--------- This means that this check didn' t pass Type: Ready Message: ScaledObject check failed Reason: UnkownState Status: Unknown Type: Active Events: <none> Inspect KEDA operator logs: kubectl logs -n keda $( kubectl get pods -n keda -o name | grep operator )","title":"Troubleshooting"},{"location":"guides/kubernetes-permissions/","text":"Kubernetes' Service Accounts and permissions used by CAST AI components \u00b6 CAST AI components running on customers' clusters use predefined Service Accounts and relevant permissions to be able to perform certain functions (like for example sending data about cluster state, etc.). This section contains detailed description of all required service accounts and permissions granted to CAST AI components. Kubernetes Service Accounts used by CAST AI components \u00b6 Each CAST AI component installed into customer's cluster uses a dedicated Service Account. Such setup allows fine-grained permissions tuning for each component: \u00bb kubectl get serviceAccounts -n castai-agent NAME SECRETS AGE castai-agent 1 46h castai-cluster-controller 1 4h20m castai-evictor 1 4h20m castai-spot-handler 1 4h20m default 1 46h CAST AI Kubernetes Agent permissions (Phase 1) \u00b6 CAST AI Kubernetes Agent must be able to collect cluster operational details (snapshots) and provide them to the central platform to estimate whether there is an optimisation opportunity. Thus, it must be granted with cluster wide permissions: API Group Resources Verbs core pods, nodes, replicationcontrollers, persistentvolumeclaims, persistentvolumes, services get, list, watch core namespaces get apps deployments, replicasets, daemonsets, statefulsets get, list, watch storage.k8s.io storageclasses, csinodes get, list, watch batch jobs get, list, watch CAST AI Kubernetes Agent's resource consumption vastly depends on the cluster size. The agent requires possibility to adjust resource limits proportionally to the size of the cluster. For that purpose Cluster Proportional Vertical Autoscaler patches CAST AI Kubernetes Agent's deployment with re-estimated limits, which requires following permission: API Group Resources Verbs Description apps deployments patch Used only to patch castai-agent deployment CAST AI Cluster Controller (Phase 2) \u00b6 CAST AI Cluster Controller component is installed when a connected cluster is promoted to Phase 2, which enables cost savings by managing customer's cluster: \u00bb kubectl get deployments -n castai-agent NAME READY UP-TO-DATE AVAILABLE AGE castai-agent 1 /1 1 1 43h castai-cluster-controller 2 /2 2 2 64m castai-evictor 0 /0 0 0 64m Cluster wide permissions used by Cluster Controller \u00b6 Cluster Controller operates mostly on cluster level as it performs operations required to optimize customer clusters' costs: API Group Resources Verbs Description core namespace get core pods, nodes get, list core nodes patch, update Used for node draining and patching core pods, nodes delete core pods/eviction create certificates.k8s.io certificatesigningrequests get, list, delete, create Used for creating a new certificate when adding a node to the cluster certificates.k8s.io certificatesigningrequests/approval patch, update Used for creating a new certificate when adding a node to the cluster certificates.k8s.io signers approve Applicable only for kubelet core events list, create, patch rbac.authorization.k8s.io roles, clusterroles, clusterrolebindings get, patch, update, delete, escalate Applicable for all CAST AI Components core namespace delete Applicable only for CAST AI Kubernetes Agent Namespace wide (castai-agent) permissions used by Cluster Controller \u00b6 One of the main task of Cluster Controller is to performs CAST AI components upgrades. Cluster Controller is granted with all permissions in castai-agent namespace which is required for the current and future updates. Additionally, Cluster Controller is granted with two cluster wide permissions to be able to manage RBAC of CAST AI components and possibility to delete CAST AI namespace (see above). CAST AI Evictor (Phase 2) \u00b6 When a cluster is onboarded with CAST AI for cost optimisation (Phase 2), there are more components installed (not just Cluster Controller). One other CAST AI components is Evictor - its responsibility is to minimize amount of nodes used by the cluster. Cluster wide permissions used by Evictor \u00b6 When installed Evictor manipulates non CAST AI pods, so it requires a set to cluster wide permissions: API Group Resources Verbs Description core events create, patch core nodes get, list, watch, patch, update Used to find a suitable node for eviction core pods get, list, watch, patch, update, create, delete List pods to find a suitable node for eviction and delete a stuck pod from a node apps replicasets get Used to find out whether it's safe to evict a pod (it belongs to RS and has replicas) core pods/eviction create Used for pod eviction coordination.k8s.io leases * Used for leader election when there may be a single instance active","title":"Kubernetes permissions"},{"location":"guides/kubernetes-permissions/#kubernetes-service-accounts-and-permissions-used-by-cast-ai-components","text":"CAST AI components running on customers' clusters use predefined Service Accounts and relevant permissions to be able to perform certain functions (like for example sending data about cluster state, etc.). This section contains detailed description of all required service accounts and permissions granted to CAST AI components.","title":"Kubernetes' Service Accounts and permissions used by CAST AI components"},{"location":"guides/kubernetes-permissions/#kubernetes-service-accounts-used-by-cast-ai-components","text":"Each CAST AI component installed into customer's cluster uses a dedicated Service Account. Such setup allows fine-grained permissions tuning for each component: \u00bb kubectl get serviceAccounts -n castai-agent NAME SECRETS AGE castai-agent 1 46h castai-cluster-controller 1 4h20m castai-evictor 1 4h20m castai-spot-handler 1 4h20m default 1 46h","title":"Kubernetes Service Accounts used by CAST AI components"},{"location":"guides/kubernetes-permissions/#cast-ai-kubernetes-agent-permissions-phase-1","text":"CAST AI Kubernetes Agent must be able to collect cluster operational details (snapshots) and provide them to the central platform to estimate whether there is an optimisation opportunity. Thus, it must be granted with cluster wide permissions: API Group Resources Verbs core pods, nodes, replicationcontrollers, persistentvolumeclaims, persistentvolumes, services get, list, watch core namespaces get apps deployments, replicasets, daemonsets, statefulsets get, list, watch storage.k8s.io storageclasses, csinodes get, list, watch batch jobs get, list, watch CAST AI Kubernetes Agent's resource consumption vastly depends on the cluster size. The agent requires possibility to adjust resource limits proportionally to the size of the cluster. For that purpose Cluster Proportional Vertical Autoscaler patches CAST AI Kubernetes Agent's deployment with re-estimated limits, which requires following permission: API Group Resources Verbs Description apps deployments patch Used only to patch castai-agent deployment","title":"CAST AI Kubernetes Agent permissions (Phase 1)"},{"location":"guides/kubernetes-permissions/#cast-ai-cluster-controller-phase-2","text":"CAST AI Cluster Controller component is installed when a connected cluster is promoted to Phase 2, which enables cost savings by managing customer's cluster: \u00bb kubectl get deployments -n castai-agent NAME READY UP-TO-DATE AVAILABLE AGE castai-agent 1 /1 1 1 43h castai-cluster-controller 2 /2 2 2 64m castai-evictor 0 /0 0 0 64m","title":"CAST AI Cluster Controller (Phase 2)"},{"location":"guides/kubernetes-permissions/#cluster-wide-permissions-used-by-cluster-controller","text":"Cluster Controller operates mostly on cluster level as it performs operations required to optimize customer clusters' costs: API Group Resources Verbs Description core namespace get core pods, nodes get, list core nodes patch, update Used for node draining and patching core pods, nodes delete core pods/eviction create certificates.k8s.io certificatesigningrequests get, list, delete, create Used for creating a new certificate when adding a node to the cluster certificates.k8s.io certificatesigningrequests/approval patch, update Used for creating a new certificate when adding a node to the cluster certificates.k8s.io signers approve Applicable only for kubelet core events list, create, patch rbac.authorization.k8s.io roles, clusterroles, clusterrolebindings get, patch, update, delete, escalate Applicable for all CAST AI Components core namespace delete Applicable only for CAST AI Kubernetes Agent","title":"Cluster wide permissions used by Cluster Controller"},{"location":"guides/kubernetes-permissions/#namespace-wide-castai-agent-permissions-used-by-cluster-controller","text":"One of the main task of Cluster Controller is to performs CAST AI components upgrades. Cluster Controller is granted with all permissions in castai-agent namespace which is required for the current and future updates. Additionally, Cluster Controller is granted with two cluster wide permissions to be able to manage RBAC of CAST AI components and possibility to delete CAST AI namespace (see above).","title":"Namespace wide (castai-agent) permissions used by Cluster Controller"},{"location":"guides/kubernetes-permissions/#cast-ai-evictor-phase-2","text":"When a cluster is onboarded with CAST AI for cost optimisation (Phase 2), there are more components installed (not just Cluster Controller). One other CAST AI components is Evictor - its responsibility is to minimize amount of nodes used by the cluster.","title":"CAST AI Evictor (Phase 2)"},{"location":"guides/kubernetes-permissions/#cluster-wide-permissions-used-by-evictor","text":"When installed Evictor manipulates non CAST AI pods, so it requires a set to cluster wide permissions: API Group Resources Verbs Description core events create, patch core nodes get, list, watch, patch, update Used to find a suitable node for eviction core pods get, list, watch, patch, update, create, delete List pods to find a suitable node for eviction and delete a stuck pod from a node apps replicasets get Used to find out whether it's safe to evict a pod (it belongs to RS and has replicas) core pods/eviction create Used for pod eviction coordination.k8s.io leases * Used for leader election when there may be a single instance active","title":"Cluster wide permissions used by Evictor"},{"location":"guides/kvisor-security/","text":"CAST AI kvisor \u00b6 Kvisor is resposible for images vulnerability scanning, Kubernetes YAML manifests linting and CIS security recommendations. It's open source and can be found on github . Install kvisor \u00b6 Before installing kvisor you must connect your cluster. Please see guide for cluster connection. Install using Console UI \u00b6 Follow guide and connect your cluster with enabled security. Install with Terraform \u00b6 CAST AI terraform modules for gke, eks and aks supports install_security_agent=true variable. See eks module example. Install with Helm \u00b6 Add CAST AI helm charts repository. helm repo add castai-helm https://castai.github.io/helm-charts helm repo update You can list all available components and versions. helm search repo castai-helm Expected example output NAME CHART VERSION APP VERSION DESCRIPTION castai-helm/castai-agent 0.18.0 v0.23.0 CAST AI agent deployment chart. castai-helm/castai-cluster-controller 0.17.0 v0.14.0 CAST AI cluster controller deployment chart. castai-helm/castai-evictor 0.10.0 0.5.1 Cluster utilization defragmentation tool castai-helm/castai-spot-handler 0.3.0 v0.3.0 CAST AI spot handler daemonset chart. castai-helm/castai-kvisor 0.16.9 v0.20.3 CAST AI security agent deployment chart. Now let's install it. helm upgrade --install castai-kvisor castai-helm/castai-kvisor -n castai-agent \\ --set castai.apiKey = <your-api-token> \\ --set castai.clusterID = <your-cluster-id> --set structuredConfig.provider = <aks | eks | gke> For structuredConfig.provider you should pass your kubernetes provider or leave empty if it's none of aks , eks or gke . You can create api key via CAST AI console UI. You can find your cluster ID in CAST AI console UI. Upgrade kvisor \u00b6 Upgrade to latest version. helm repo update helm upgrade castai-kvisor castai-helm/castai-kvisor -n castai-agent --reuse-values Configuring features \u00b6 You can change any of the supported config values described in kvisor helm chart To increase concurrent images scan count: helm upgrade castai-kvisor castai-helm/castai-kvisor -n castai-agent \\ --reuse-values --set structuredConfig.imageScan.maxConcurrentScans = 6 To disable images can: helm upgrade castai-kvisor castai-helm/castai-kvisor -n castai-agent \\ --reuse-values --set structuredConfig.imageScan.enabled = false To disable kube bench jobs: helm upgrade castai-kvisor castai-helm/castai-kvisor -n castai-agent \\ --reuse-values --set structuredConfig.kubeBench.enabled = false To disable kubernetes YAML manifests linters: helm upgrade castai-kvisor castai-helm/castai-kvisor -n castai-agent \\ --reuse-values --set structuredConfig.linter.enabled = false To check all applied configurations: helm get values castai-kvisor -n castai-agent Troubleshooting \u00b6 Check kvisor logs kubectl logs -l app.kubernetes.io/name = castai-kvisor -n castai-agent","title":"Kvisor security"},{"location":"guides/kvisor-security/#cast-ai-kvisor","text":"Kvisor is resposible for images vulnerability scanning, Kubernetes YAML manifests linting and CIS security recommendations. It's open source and can be found on github .","title":"CAST AI kvisor"},{"location":"guides/kvisor-security/#install-kvisor","text":"Before installing kvisor you must connect your cluster. Please see guide for cluster connection.","title":"Install kvisor"},{"location":"guides/kvisor-security/#install-using-console-ui","text":"Follow guide and connect your cluster with enabled security.","title":"Install using Console UI"},{"location":"guides/kvisor-security/#install-with-terraform","text":"CAST AI terraform modules for gke, eks and aks supports install_security_agent=true variable. See eks module example.","title":"Install with Terraform"},{"location":"guides/kvisor-security/#install-with-helm","text":"Add CAST AI helm charts repository. helm repo add castai-helm https://castai.github.io/helm-charts helm repo update You can list all available components and versions. helm search repo castai-helm Expected example output NAME CHART VERSION APP VERSION DESCRIPTION castai-helm/castai-agent 0.18.0 v0.23.0 CAST AI agent deployment chart. castai-helm/castai-cluster-controller 0.17.0 v0.14.0 CAST AI cluster controller deployment chart. castai-helm/castai-evictor 0.10.0 0.5.1 Cluster utilization defragmentation tool castai-helm/castai-spot-handler 0.3.0 v0.3.0 CAST AI spot handler daemonset chart. castai-helm/castai-kvisor 0.16.9 v0.20.3 CAST AI security agent deployment chart. Now let's install it. helm upgrade --install castai-kvisor castai-helm/castai-kvisor -n castai-agent \\ --set castai.apiKey = <your-api-token> \\ --set castai.clusterID = <your-cluster-id> --set structuredConfig.provider = <aks | eks | gke> For structuredConfig.provider you should pass your kubernetes provider or leave empty if it's none of aks , eks or gke . You can create api key via CAST AI console UI. You can find your cluster ID in CAST AI console UI.","title":"Install with Helm"},{"location":"guides/kvisor-security/#upgrade-kvisor","text":"Upgrade to latest version. helm repo update helm upgrade castai-kvisor castai-helm/castai-kvisor -n castai-agent --reuse-values","title":"Upgrade kvisor"},{"location":"guides/kvisor-security/#configuring-features","text":"You can change any of the supported config values described in kvisor helm chart To increase concurrent images scan count: helm upgrade castai-kvisor castai-helm/castai-kvisor -n castai-agent \\ --reuse-values --set structuredConfig.imageScan.maxConcurrentScans = 6 To disable images can: helm upgrade castai-kvisor castai-helm/castai-kvisor -n castai-agent \\ --reuse-values --set structuredConfig.imageScan.enabled = false To disable kube bench jobs: helm upgrade castai-kvisor castai-helm/castai-kvisor -n castai-agent \\ --reuse-values --set structuredConfig.kubeBench.enabled = false To disable kubernetes YAML manifests linters: helm upgrade castai-kvisor castai-helm/castai-kvisor -n castai-agent \\ --reuse-values --set structuredConfig.linter.enabled = false To check all applied configurations: helm get values castai-kvisor -n castai-agent","title":"Configuring features"},{"location":"guides/kvisor-security/#troubleshooting","text":"Check kvisor logs kubectl logs -l app.kubernetes.io/name = castai-kvisor -n castai-agent","title":"Troubleshooting"},{"location":"guides/metrics/","text":"Cluster metrics integration \u00b6 CAST AI delivers detailed metrics on your cluster utilization so that you can better understand your cloud infrastructure and ultimately reduce its cost. All metrics are scrapable, so you can scrape the data using Prometheus API and visualize it in Grafana or another tool of your choice. As a result, you can draw on the cluster utilization and cost stats and include them effortlessly in your team\u2019s wider cloud monitoring and reporting efforts. This guide outlines the metrics available in CAST AI and describes the process of exporting them to Prometheus and Grafana step by step. How to visualize CAST AI metrics in Prometheus and Grafana \u00b6 Why use CAST AI with Prometheus and Grafana \u00b6 The combination of Prometheus and Grafana has become a common choice for DevOps and CloudOps teams, and this is for a reason. The first provides a powerful querying language and gathers rich metrics, while the latter transforms these into meaningful visualizations. Both Prometheus and Grafana are compatible with most data source types. How to connect CAST AI with Prometheus and Grafana \u00b6 1. Create your CAST AI API key Enter your cluster in the CAST AI platform, click the API tab in the top menu, and generate a one-time token. You\u2019ll need to specify your key name and choose between a read-only or full access. Then, copy and paste it into the respective place in the above code and execute. You can also use this key to access CAST AI API in tools like Swagger UI. 2. Call the CAST AI API Open your Prometheus scraper config in your favorite tool and add scraping for CAST AI metrics: scrape_configs : 2. job_name : 'castai_cluster_metrics' scrape_interval : 15s scheme : https static_configs : - targets : [ 'api.cast.ai' ] metrics_path : '/v1/metrics/prom' authorization : type : 'Token' credentials : '{apiKey}' To access this data endpoint, you\u2019ll need to swap {apiKey} for the token created in step 1. 3. Specify your data source in Grafana Open Grafana, head to the Configuration tab, and click on Data Sources. When you select the Add data source option, you\u2019ll see a list of all supported data sources. From here, choose Prometheus and insert all required details, including HTTP, Auth, and more. After you specify your data source, you can go to Explore, select your data source by name, and start typing the metric name for autocompleting. 4. Create a dashboard in Grafana Click on the Dashboards tab in Grafana\u2019s main menu and select the Browse option. That\u2019s where you\u2019ll see the button to start a new dashboard. Give it a meaningful name and set the main options. For more information you can refer to Grafana\u2019s documentation , you can also check this list of best practices for creating dashboards . 5. Add and format your metrics Now it\u2019s time to start populating your dashboard with data. Add a new panel and scroll down to its bottom to ensure that the data source is set to Prometheus. Then, start typing the name of the required metric in the metric browser box, and it will appear on the screen. Common choices of metrics include the requested vs. provisioned CPUs and memory, and the monthly cost of your cluster. You can also expand the metrics presented in your dashboard by importing data in JSON files. Use the panel on the right to specify your stat title, legend, visualization styles, and other values to help you ensure the report makes the most sense to your team. You can then expand your dashboard with additional features, including annotations and alerts . Example Grafana dashboard \u00b6 Here\u2019s an example dashboard displaying CAST AI data. You can get the code here . CAST AI metrics \u00b6 Note: Label cast_node_type is deprecated, so instead of it please use castai_node_lifecycle . Name Type Description Action castai_autoscaler_agent_snapshots_received_total Counter The CAST AI Autoscaler agent snapshots received total. Check if the Agent is running in the cluster. castai_autoscaler_agent_snapshots_processed_total Counter The CAST AI Autoscaler agent snapshots processed total. Contact CAST AI support. castai_cluster_total_cost_hourly Gauge Cluster total hourly cost. castai_cluster_compute_cost_hourly Gauge Cluster compute cost. Has a lifecycle dimensions which can be summed up to a total cost: [on_demand, spot_fallback, spot] . castai_cluster_compute_cost_per_cpu_hourly Gauge Normalized cost per CPU. Has a lifecycle dimension, similar to castai_cluster_compute_cost_hourly . castai_cluster_allocatable_cpu_cores Gauge Cluster allocatable CPU cores. castai_cluster_allocatable_memory_bytes Gauge Cluster allocatable memory. castai_cluster_provisioned_cpu_cores Gauge Cluster provisioned CPU cores. castai_cluster_provisioned_memory_bytes Gauge Cluster provisioner memory. castai_cluster_requests_cpu_cores Gauge Cluster requested CPU cores. castai_cluster_requests_memory_bytes Gauge Cluster requested memory. castai_cluster_node_count Gauge Cluster nodes count. castai_cluster_pods_count Gauge Cluster pods count. castai_cluster_unschedulable_pods_count Gauge Cluster unschedulable pods count. castai_evictor_node_target_count Gauge CAST AI Evictor targeted nodes count. castai_evictor_pod_target_count Gauge CAST AI Evictor targeted pods count. Query examples \u00b6 Cost per cluster: sum(castai_cluster_total_cost_hourly{}) by (castai_cluster) Compute cost of spot instances of a specific cluster: castai_cluster_compute_cost_hourly{castai_cluster=\"$cluster\", lifecycle=\"spot\"} Received snapshots count: sum(increase(castai_autoscaler_agent_snapshots_received_total{castai_cluster=\"$cluster\"}[5m])) Alert on missing snapshots: absent_over_time(castai_autoscaler_agent_snapshots_received_total{castai_cluster=\"$cluster\"}[5m]) Get castai_node_lifecycle( on_demand , spot , spot_fallback ) of running nodes in cluster: sum(castai_cluster_node_count{castai_cluster=\"$cluster\"}) by (castai_node_lifecycle) Get CPU cores provisioned for spot_fallback nodes: castai_cluster_provisioned_cpu_cores(castai_node_lifecycle=\"spot_fallback\") Note : Replace $cluster with existing castai_cluster label value.","title":"Cluster metrics"},{"location":"guides/metrics/#cluster-metrics-integration","text":"CAST AI delivers detailed metrics on your cluster utilization so that you can better understand your cloud infrastructure and ultimately reduce its cost. All metrics are scrapable, so you can scrape the data using Prometheus API and visualize it in Grafana or another tool of your choice. As a result, you can draw on the cluster utilization and cost stats and include them effortlessly in your team\u2019s wider cloud monitoring and reporting efforts. This guide outlines the metrics available in CAST AI and describes the process of exporting them to Prometheus and Grafana step by step.","title":"Cluster metrics integration"},{"location":"guides/metrics/#how-to-visualize-cast-ai-metrics-in-prometheus-and-grafana","text":"","title":"How to visualize CAST AI metrics in Prometheus and Grafana"},{"location":"guides/metrics/#why-use-cast-ai-with-prometheus-and-grafana","text":"The combination of Prometheus and Grafana has become a common choice for DevOps and CloudOps teams, and this is for a reason. The first provides a powerful querying language and gathers rich metrics, while the latter transforms these into meaningful visualizations. Both Prometheus and Grafana are compatible with most data source types.","title":"Why use CAST AI with Prometheus and Grafana"},{"location":"guides/metrics/#how-to-connect-cast-ai-with-prometheus-and-grafana","text":"1. Create your CAST AI API key Enter your cluster in the CAST AI platform, click the API tab in the top menu, and generate a one-time token. You\u2019ll need to specify your key name and choose between a read-only or full access. Then, copy and paste it into the respective place in the above code and execute. You can also use this key to access CAST AI API in tools like Swagger UI. 2. Call the CAST AI API Open your Prometheus scraper config in your favorite tool and add scraping for CAST AI metrics: scrape_configs : 2. job_name : 'castai_cluster_metrics' scrape_interval : 15s scheme : https static_configs : - targets : [ 'api.cast.ai' ] metrics_path : '/v1/metrics/prom' authorization : type : 'Token' credentials : '{apiKey}' To access this data endpoint, you\u2019ll need to swap {apiKey} for the token created in step 1. 3. Specify your data source in Grafana Open Grafana, head to the Configuration tab, and click on Data Sources. When you select the Add data source option, you\u2019ll see a list of all supported data sources. From here, choose Prometheus and insert all required details, including HTTP, Auth, and more. After you specify your data source, you can go to Explore, select your data source by name, and start typing the metric name for autocompleting. 4. Create a dashboard in Grafana Click on the Dashboards tab in Grafana\u2019s main menu and select the Browse option. That\u2019s where you\u2019ll see the button to start a new dashboard. Give it a meaningful name and set the main options. For more information you can refer to Grafana\u2019s documentation , you can also check this list of best practices for creating dashboards . 5. Add and format your metrics Now it\u2019s time to start populating your dashboard with data. Add a new panel and scroll down to its bottom to ensure that the data source is set to Prometheus. Then, start typing the name of the required metric in the metric browser box, and it will appear on the screen. Common choices of metrics include the requested vs. provisioned CPUs and memory, and the monthly cost of your cluster. You can also expand the metrics presented in your dashboard by importing data in JSON files. Use the panel on the right to specify your stat title, legend, visualization styles, and other values to help you ensure the report makes the most sense to your team. You can then expand your dashboard with additional features, including annotations and alerts .","title":"How to connect CAST AI with Prometheus and Grafana"},{"location":"guides/metrics/#example-grafana-dashboard","text":"Here\u2019s an example dashboard displaying CAST AI data. You can get the code here .","title":"Example Grafana dashboard"},{"location":"guides/metrics/#cast-ai-metrics","text":"Note: Label cast_node_type is deprecated, so instead of it please use castai_node_lifecycle . Name Type Description Action castai_autoscaler_agent_snapshots_received_total Counter The CAST AI Autoscaler agent snapshots received total. Check if the Agent is running in the cluster. castai_autoscaler_agent_snapshots_processed_total Counter The CAST AI Autoscaler agent snapshots processed total. Contact CAST AI support. castai_cluster_total_cost_hourly Gauge Cluster total hourly cost. castai_cluster_compute_cost_hourly Gauge Cluster compute cost. Has a lifecycle dimensions which can be summed up to a total cost: [on_demand, spot_fallback, spot] . castai_cluster_compute_cost_per_cpu_hourly Gauge Normalized cost per CPU. Has a lifecycle dimension, similar to castai_cluster_compute_cost_hourly . castai_cluster_allocatable_cpu_cores Gauge Cluster allocatable CPU cores. castai_cluster_allocatable_memory_bytes Gauge Cluster allocatable memory. castai_cluster_provisioned_cpu_cores Gauge Cluster provisioned CPU cores. castai_cluster_provisioned_memory_bytes Gauge Cluster provisioner memory. castai_cluster_requests_cpu_cores Gauge Cluster requested CPU cores. castai_cluster_requests_memory_bytes Gauge Cluster requested memory. castai_cluster_node_count Gauge Cluster nodes count. castai_cluster_pods_count Gauge Cluster pods count. castai_cluster_unschedulable_pods_count Gauge Cluster unschedulable pods count. castai_evictor_node_target_count Gauge CAST AI Evictor targeted nodes count. castai_evictor_pod_target_count Gauge CAST AI Evictor targeted pods count.","title":"CAST AI metrics"},{"location":"guides/metrics/#query-examples","text":"Cost per cluster: sum(castai_cluster_total_cost_hourly{}) by (castai_cluster) Compute cost of spot instances of a specific cluster: castai_cluster_compute_cost_hourly{castai_cluster=\"$cluster\", lifecycle=\"spot\"} Received snapshots count: sum(increase(castai_autoscaler_agent_snapshots_received_total{castai_cluster=\"$cluster\"}[5m])) Alert on missing snapshots: absent_over_time(castai_autoscaler_agent_snapshots_received_total{castai_cluster=\"$cluster\"}[5m]) Get castai_node_lifecycle( on_demand , spot , spot_fallback ) of running nodes in cluster: sum(castai_cluster_node_count{castai_cluster=\"$cluster\"}) by (castai_node_lifecycle) Get CPU cores provisioned for spot_fallback nodes: castai_cluster_provisioned_cpu_cores(castai_node_lifecycle=\"spot_fallback\") Note : Replace $cluster with existing castai_cluster label value.","title":"Query examples"},{"location":"guides/mutating-admision-webhook/","text":"Mutating Admission Webhook \u00b6 Modifying workload manifests to achieve the desired savings takes time and effort. The CAST AI Mutating Admission Webhook slightly modifies workload manifests on-the-fly. It's a quick and effortless way to achieve savings without modifying every workload. Whenever there's a request to schedule a pod, the CAST AI Mutating Admission Webhook (in short, mutating webhook) will mutate workload manifest - for example, adding spot toleration to influence the desired pod placement by the Kubernetes Scheduler. CAST AI Mutating Admission Webhook presets: Spot-only Spot-only except kube-system Partial Spot Custom [Coming soon] Intelligent placement on Rebalancing Running pods will not be affected The Webhook only mutates pods during scheduling. Over time, all pods should eventually be re-scheduled and, in turn, mutated. The application owners will release a new version of workload that will trigger all the replicas to be rescheduled, Evictor, or Rebalancing will remove older nodes, putting pods for rescheduling, etc. If you'd like to initiate mutation for the whole namespace immediately, run this command which will recreate all pods: kubectl -n { NAMESPACE } rollout restart deploy Spot-only \u00b6 Preset allSpot . The Spot-only mutating webhook will mark all workloads in your cluster as suitable for spot instances, causing the autoscaler to prefer spot instances when scaling the cluster up. As this will make cluster more cost-efficient, choosing this mode is recommended for Development and Staging environments, batch job processing clusters, etc. The CAST AI autoscaler will create spot instances only if the pod has \"Spot toleration,\" see Spot/Preemptible Instances . The Mutating Webhook will add the Spot toleration and the Spot node selector to all the workloads being scheduled. Install Spot-only \u00b6 To run all pods (including kube-system ) on spot instances, use: helm repo add castai-helm https://castai.github.io/helm-charts helm upgrade -i --create-namespace -n castai-pod-node-lifecycle castai-pod-node-lifecycle \\ castai-helm/castai-pod-node-lifecycle \\ --set staticConfig.preset = allSpot Spot-only except kube-system \u00b6 Preset allSpotExceptKubeSystem . This mode works the same as the Spot-only mode but it forces all pods in the kube-system namespace to be placed on on-demand nodes. This mode is recommended for clusters where the high-availability aspect of the control-plane is vitally important while other pods can tolerate spot interruptions. Install Spot-only except kube-system \u00b6 To run all pods excluding kube-system on spot instances, use: helm repo add castai-helm https://castai.github.io/helm-charts helm upgrade -i --create-namespace -n castai-pod-node-lifecycle castai-pod-node-lifecycle \\ castai-helm/castai-pod-node-lifecycle \\ --set staticConfig.preset = allSpotExceptKubeSystem Partial Spot \u00b6 Preset partialSpot . When 100% of pods on spot instances is not a desirable scenario, you can use a ratio like 60% on stable on-demand instances and remaining 40% of pods in same ReplicaSet (Deployment / StatefulSet) running on spot instances. This conservative configuration ensures that there are enough pods on stable compute for the base load, but still allows achieving significant savings for pods above the base load by putting them on spot instances. This setup is recommended for all types of environment, from Production to Development. Install Partial Spot \u00b6 For running 40% workload pods on spot instances and keep remaining pods of same ReplicaSet on on-demand instances, use: helm repo add castai-helm https://castai.github.io/helm-charts helm upgrade -i --create-namespace -n castai-pod-node-lifecycle castai-pod-node-lifecycle \\ castai-helm/castai-pod-node-lifecycle \\ --set staticConfig.preset = partialSpot To set a custom ratio for partial Spot, replace 70 with [1-99] as percentage value: helm repo add castai-helm https://castai.github.io/helm-charts helm upgrade -i --create-namespace -n castai-pod-node-lifecycle castai-pod-node-lifecycle \\ castai-helm/castai-pod-node-lifecycle \\ --set staticConfig.defaultToSpot = false --set staticConfig.spotPercentageOfReplicaSet = 70 Custom \u00b6 No preset. This mode can be adjusted to match the needs and requirements of your cluster. Instead of choosing a specific preset, you configure the behavior yourself. Key Type Default Description staticConfig.defaultToSpot boolean true Should the webhook add spot tolerations and node selectors to all pods which don't match other rules? staticConfig.spotPercentageOfReplicaSet int 0 The percentage of pods (per ReplicaSet) which should be put on Spot instances. Acceptable values [1-100] . 0 means the feature is turned off. defaultToSpot and spotPercentageOfReplicaSet are mutually exclusive settings as they are cluster wide staticConfig.ignorePods list of PodAffinityTerm [] Terms describing the label selectors for pods which should be ignored by the webhook. staticConfig.forcePodsToSpot list of PodAffinityTerm [] Terms describing the label selectors for pods which should be put on Spot instances. staticConfig.forcePodsToOnDemand list of PodAffinityTerm [] Terms describing the label selectors for pods which should be put on Spot instances. Schema description of the PodAffinityTerm object can be found in the official kubernetes-api documentation . The property topologyKey is ignored and the property namespaceSelector is not yet supported. Install Custom \u00b6 Here is an example of a values.yaml with custom rules defined: staticConfig : defaultToSpot : false spotPercentageOfReplicaSet : 30 ignorePods : - labelSelector : matchLabels : app.kubernetes.io/name : ignored-pod forcePodsToSpot : - labelSelector : matchExpressions : - key : app.kubernetes.io/name operator : In values : - spot-pod-1 - spot-pod-2 forcePodsToOnDemand : - namespaces : - kube-system - default To install the webhook with these custom rules, execute this command: helm repo add castai-helm https://castai.github.io/helm-charts helm upgrade -i --create-namespace -n castai-pod-node-lifecycle castai-pod-node-lifecycle \\ castai-helm/castai-pod-node-lifecycle \\ --values values.yaml Workload level override \u00b6 Mutating Webhook is a cluster level configuration, but one can have exceptions that could be enforced per Deployment or StatefulSet. Annotation Name Value Location Effect scheduling.cast.ai/lifecycle \"on-demand\" Deployment or StatefulSet All Pods will be scheduled on on-demand instances scheduling.cast.ai/lifecycle \"spot\" Deployment or StatefulSet All Pods will be scheduled on spot instances scheduling.cast.ai/spot-percentage \"65\" [1-99] Deployment or StatefulSet Override Partial Spot configuration, schedule up to 65% on spot and remaining (at least 35%) on on-demand kubectl patch deployment resilient-app -p '{\"spec\": {\"template\":{\"metadata\":{\"annotations\":{\"scheduling.cast.ai/lifecycle\":\"spot\"}}}}}' kubectl patch deployment sensitive-app -p '{\"spec\": {\"template\":{\"metadata\":{\"annotations\":{\"scheduling.cast.ai/lifecycle\":\"on-demand\"}}}}}' kubectl patch deployment conservative-app -p '{\"spec\": {\"template\":{\"metadata\":{\"annotations\":{\"scheduling.cast.ai/spot-percentage\":\"50\"}}}}}' Annotation added to Pod is NOT permanent and will not impact Mutation Webhook behaviour. To set permanent override on workload, one needs to modify Pods Template on the controller (for example Deployment). Operation will re-create all Deployment Pods. Troubleshooting \u00b6 The mutating webhook will ignore these type of pods: Bare pods without ReplicaSet Controller Pods in \"castai-pod-node-lifecycle\" namespace Pods with TopologySpreadConstraints with TopologyKey=Lifecycle DaemonSets will get Spot Toleration by default, ensuring DaemonSet Pods could run on spot and on-demand nodes The CAST AI Mutating webhook pods write logs to stdOut. If cluster has Deployments with 1000+ replicas set higher Memory Requests and Limits, by appending these parameters to Helm command --set resources.requests.memory = 1G --set resources.limits.memory = 1G","title":"Mutating Admission Webhook"},{"location":"guides/mutating-admision-webhook/#mutating-admission-webhook","text":"Modifying workload manifests to achieve the desired savings takes time and effort. The CAST AI Mutating Admission Webhook slightly modifies workload manifests on-the-fly. It's a quick and effortless way to achieve savings without modifying every workload. Whenever there's a request to schedule a pod, the CAST AI Mutating Admission Webhook (in short, mutating webhook) will mutate workload manifest - for example, adding spot toleration to influence the desired pod placement by the Kubernetes Scheduler. CAST AI Mutating Admission Webhook presets: Spot-only Spot-only except kube-system Partial Spot Custom [Coming soon] Intelligent placement on Rebalancing Running pods will not be affected The Webhook only mutates pods during scheduling. Over time, all pods should eventually be re-scheduled and, in turn, mutated. The application owners will release a new version of workload that will trigger all the replicas to be rescheduled, Evictor, or Rebalancing will remove older nodes, putting pods for rescheduling, etc. If you'd like to initiate mutation for the whole namespace immediately, run this command which will recreate all pods: kubectl -n { NAMESPACE } rollout restart deploy","title":"Mutating Admission Webhook"},{"location":"guides/mutating-admision-webhook/#spot-only","text":"Preset allSpot . The Spot-only mutating webhook will mark all workloads in your cluster as suitable for spot instances, causing the autoscaler to prefer spot instances when scaling the cluster up. As this will make cluster more cost-efficient, choosing this mode is recommended for Development and Staging environments, batch job processing clusters, etc. The CAST AI autoscaler will create spot instances only if the pod has \"Spot toleration,\" see Spot/Preemptible Instances . The Mutating Webhook will add the Spot toleration and the Spot node selector to all the workloads being scheduled.","title":"Spot-only"},{"location":"guides/mutating-admision-webhook/#install-spot-only","text":"To run all pods (including kube-system ) on spot instances, use: helm repo add castai-helm https://castai.github.io/helm-charts helm upgrade -i --create-namespace -n castai-pod-node-lifecycle castai-pod-node-lifecycle \\ castai-helm/castai-pod-node-lifecycle \\ --set staticConfig.preset = allSpot","title":"Install Spot-only"},{"location":"guides/mutating-admision-webhook/#spot-only-except-kube-system","text":"Preset allSpotExceptKubeSystem . This mode works the same as the Spot-only mode but it forces all pods in the kube-system namespace to be placed on on-demand nodes. This mode is recommended for clusters where the high-availability aspect of the control-plane is vitally important while other pods can tolerate spot interruptions.","title":"Spot-only except kube-system"},{"location":"guides/mutating-admision-webhook/#install-spot-only-except-kube-system","text":"To run all pods excluding kube-system on spot instances, use: helm repo add castai-helm https://castai.github.io/helm-charts helm upgrade -i --create-namespace -n castai-pod-node-lifecycle castai-pod-node-lifecycle \\ castai-helm/castai-pod-node-lifecycle \\ --set staticConfig.preset = allSpotExceptKubeSystem","title":"Install Spot-only except kube-system"},{"location":"guides/mutating-admision-webhook/#partial-spot","text":"Preset partialSpot . When 100% of pods on spot instances is not a desirable scenario, you can use a ratio like 60% on stable on-demand instances and remaining 40% of pods in same ReplicaSet (Deployment / StatefulSet) running on spot instances. This conservative configuration ensures that there are enough pods on stable compute for the base load, but still allows achieving significant savings for pods above the base load by putting them on spot instances. This setup is recommended for all types of environment, from Production to Development.","title":"Partial Spot"},{"location":"guides/mutating-admision-webhook/#install-partial-spot","text":"For running 40% workload pods on spot instances and keep remaining pods of same ReplicaSet on on-demand instances, use: helm repo add castai-helm https://castai.github.io/helm-charts helm upgrade -i --create-namespace -n castai-pod-node-lifecycle castai-pod-node-lifecycle \\ castai-helm/castai-pod-node-lifecycle \\ --set staticConfig.preset = partialSpot To set a custom ratio for partial Spot, replace 70 with [1-99] as percentage value: helm repo add castai-helm https://castai.github.io/helm-charts helm upgrade -i --create-namespace -n castai-pod-node-lifecycle castai-pod-node-lifecycle \\ castai-helm/castai-pod-node-lifecycle \\ --set staticConfig.defaultToSpot = false --set staticConfig.spotPercentageOfReplicaSet = 70","title":"Install Partial Spot"},{"location":"guides/mutating-admision-webhook/#custom","text":"No preset. This mode can be adjusted to match the needs and requirements of your cluster. Instead of choosing a specific preset, you configure the behavior yourself. Key Type Default Description staticConfig.defaultToSpot boolean true Should the webhook add spot tolerations and node selectors to all pods which don't match other rules? staticConfig.spotPercentageOfReplicaSet int 0 The percentage of pods (per ReplicaSet) which should be put on Spot instances. Acceptable values [1-100] . 0 means the feature is turned off. defaultToSpot and spotPercentageOfReplicaSet are mutually exclusive settings as they are cluster wide staticConfig.ignorePods list of PodAffinityTerm [] Terms describing the label selectors for pods which should be ignored by the webhook. staticConfig.forcePodsToSpot list of PodAffinityTerm [] Terms describing the label selectors for pods which should be put on Spot instances. staticConfig.forcePodsToOnDemand list of PodAffinityTerm [] Terms describing the label selectors for pods which should be put on Spot instances. Schema description of the PodAffinityTerm object can be found in the official kubernetes-api documentation . The property topologyKey is ignored and the property namespaceSelector is not yet supported.","title":"Custom"},{"location":"guides/mutating-admision-webhook/#install-custom","text":"Here is an example of a values.yaml with custom rules defined: staticConfig : defaultToSpot : false spotPercentageOfReplicaSet : 30 ignorePods : - labelSelector : matchLabels : app.kubernetes.io/name : ignored-pod forcePodsToSpot : - labelSelector : matchExpressions : - key : app.kubernetes.io/name operator : In values : - spot-pod-1 - spot-pod-2 forcePodsToOnDemand : - namespaces : - kube-system - default To install the webhook with these custom rules, execute this command: helm repo add castai-helm https://castai.github.io/helm-charts helm upgrade -i --create-namespace -n castai-pod-node-lifecycle castai-pod-node-lifecycle \\ castai-helm/castai-pod-node-lifecycle \\ --values values.yaml","title":"Install Custom"},{"location":"guides/mutating-admision-webhook/#workload-level-override","text":"Mutating Webhook is a cluster level configuration, but one can have exceptions that could be enforced per Deployment or StatefulSet. Annotation Name Value Location Effect scheduling.cast.ai/lifecycle \"on-demand\" Deployment or StatefulSet All Pods will be scheduled on on-demand instances scheduling.cast.ai/lifecycle \"spot\" Deployment or StatefulSet All Pods will be scheduled on spot instances scheduling.cast.ai/spot-percentage \"65\" [1-99] Deployment or StatefulSet Override Partial Spot configuration, schedule up to 65% on spot and remaining (at least 35%) on on-demand kubectl patch deployment resilient-app -p '{\"spec\": {\"template\":{\"metadata\":{\"annotations\":{\"scheduling.cast.ai/lifecycle\":\"spot\"}}}}}' kubectl patch deployment sensitive-app -p '{\"spec\": {\"template\":{\"metadata\":{\"annotations\":{\"scheduling.cast.ai/lifecycle\":\"on-demand\"}}}}}' kubectl patch deployment conservative-app -p '{\"spec\": {\"template\":{\"metadata\":{\"annotations\":{\"scheduling.cast.ai/spot-percentage\":\"50\"}}}}}' Annotation added to Pod is NOT permanent and will not impact Mutation Webhook behaviour. To set permanent override on workload, one needs to modify Pods Template on the controller (for example Deployment). Operation will re-create all Deployment Pods.","title":"Workload level override"},{"location":"guides/mutating-admision-webhook/#troubleshooting","text":"The mutating webhook will ignore these type of pods: Bare pods without ReplicaSet Controller Pods in \"castai-pod-node-lifecycle\" namespace Pods with TopologySpreadConstraints with TopologyKey=Lifecycle DaemonSets will get Spot Toleration by default, ensuring DaemonSet Pods could run on spot and on-demand nodes The CAST AI Mutating webhook pods write logs to stdOut. If cluster has Deployments with 1000+ replicas set higher Memory Requests and Limits, by appending these parameters to Helm command --set resources.requests.memory = 1G --set resources.limits.memory = 1G","title":"Troubleshooting"},{"location":"guides/node-configuration/","text":"Node Configuration \u00b6 Feature availability \u00b6 EKS KOPS GKE AKS + - - - The CAST AI provisioner allows you to provide node configuration parameters that will be applied to CAST AI provisioned nodes. Node configuration on its own does not influence workload placement. The list of supported configuration parameters: Configuration parameters Description Root volume ratio CPU to storage (GiB) ratio Subnets Subnet IDs for CAST AI provisioned nodes Instance tags Tags for CAST AI provisioned nodes Image version Image ID or name to be used when building CAST AI provisioned node SSH key Base64 encoded public key or AWS key ID Security groups (EKS) Security group IDs for CAST AI provisioned nodes Instance profile ARN (EKS) Instance profile ARN for CAST AI provisioned nodes Dns-cluster-ip (EKS) Override the IP address to be used for DNS queries within the cluster By default values are either inferred from the cluster (subnets, security groups...) or a generic value is applied. Some configuration options are cloud provider specific, see table below: Shared configuration options \u00b6 Configuration Default value Root volume ratio 1 CPU : 5 GiB Image version Latest available for kubernetes release* SSH key \"\" Subnets All subnets pointing to NAT/Internet Gateways inside cluster VPC Instance tags [] * List of available images for EKS in aws docs EKS configuration options \u00b6 Configuration Default value Security groups Tagged and CAST AI SG Instance profile ARN cast-<cluster-name>-eks-<cluster-id> (only last 8 digits of cluster ID) Dns-cluster-ip \"\" Create node configuration \u00b6 A default node configuration is created during phase 2 cluster onboarding. You can choose to modify this configuration or create a new one. If you choose to add new node configuration that will be applied to all newly provisioned nodes, you will have to mark it as the default node configuration. Node configurations are versioned and when the CAST AI provisioner adds new node, the latest version of node configuration is applied. Over time CAST AI provisioned nodes trend to the latest available node configuration. Create node configuration in CAST AI Console \u00b6 In the cluster view a new tab \"Node configuration\" has been created. Here you can view and manage node configurations. Use the button \"Add configuration\" Name your configuration and fill in your values Click \"Save\" Click \"...\" and \"Set as default\" Create node configuration with CAST AI Terraform provider \u00b6 Use the resource castai_node_configuration from CAST AI terraform provider . Reference example: resource \"castai_node_configuration\" \"test\" { name = local.name cluster_id = castai_eks_cluster.test.id disk_cpu_ratio = 35 subnets = aws_subnet.test[*].id tags = { env = \"development\" } eks { instance_profile_arn = aws_iam_instance_profile.test.arn dns_cluster_ip = \"10.100.0.10\" security_groups = [aws_security_group.test.id] } } Create node configuration with CAST AI API \u00b6 For API operations consult the generated documentation . Delete node configuration \u00b6 To delete a node configuration, the following has to be true: the configuration is not linked to a node template if the configuration is marked as \"default\", it must not be the latest version Delete node configuration in CAST AI Console \u00b6 In the node configuration view, click \"...\" of the configuration you wish to delete and then \"Delete configuration\". Node view \u00b6 In the \"Nodes\" tab, you can view and filter nodes based on applied node configuration:","title":"Node Configuration"},{"location":"guides/node-configuration/#node-configuration","text":"","title":"Node Configuration"},{"location":"guides/node-configuration/#feature-availability","text":"EKS KOPS GKE AKS + - - - The CAST AI provisioner allows you to provide node configuration parameters that will be applied to CAST AI provisioned nodes. Node configuration on its own does not influence workload placement. The list of supported configuration parameters: Configuration parameters Description Root volume ratio CPU to storage (GiB) ratio Subnets Subnet IDs for CAST AI provisioned nodes Instance tags Tags for CAST AI provisioned nodes Image version Image ID or name to be used when building CAST AI provisioned node SSH key Base64 encoded public key or AWS key ID Security groups (EKS) Security group IDs for CAST AI provisioned nodes Instance profile ARN (EKS) Instance profile ARN for CAST AI provisioned nodes Dns-cluster-ip (EKS) Override the IP address to be used for DNS queries within the cluster By default values are either inferred from the cluster (subnets, security groups...) or a generic value is applied. Some configuration options are cloud provider specific, see table below:","title":"Feature availability"},{"location":"guides/node-configuration/#shared-configuration-options","text":"Configuration Default value Root volume ratio 1 CPU : 5 GiB Image version Latest available for kubernetes release* SSH key \"\" Subnets All subnets pointing to NAT/Internet Gateways inside cluster VPC Instance tags [] * List of available images for EKS in aws docs","title":"Shared configuration options"},{"location":"guides/node-configuration/#eks-configuration-options","text":"Configuration Default value Security groups Tagged and CAST AI SG Instance profile ARN cast-<cluster-name>-eks-<cluster-id> (only last 8 digits of cluster ID) Dns-cluster-ip \"\"","title":"EKS configuration options"},{"location":"guides/node-configuration/#create-node-configuration","text":"A default node configuration is created during phase 2 cluster onboarding. You can choose to modify this configuration or create a new one. If you choose to add new node configuration that will be applied to all newly provisioned nodes, you will have to mark it as the default node configuration. Node configurations are versioned and when the CAST AI provisioner adds new node, the latest version of node configuration is applied. Over time CAST AI provisioned nodes trend to the latest available node configuration.","title":"Create node configuration"},{"location":"guides/node-configuration/#create-node-configuration-in-cast-ai-console","text":"In the cluster view a new tab \"Node configuration\" has been created. Here you can view and manage node configurations. Use the button \"Add configuration\" Name your configuration and fill in your values Click \"Save\" Click \"...\" and \"Set as default\"","title":"Create node configuration in CAST AI Console"},{"location":"guides/node-configuration/#create-node-configuration-with-cast-ai-terraform-provider","text":"Use the resource castai_node_configuration from CAST AI terraform provider . Reference example: resource \"castai_node_configuration\" \"test\" { name = local.name cluster_id = castai_eks_cluster.test.id disk_cpu_ratio = 35 subnets = aws_subnet.test[*].id tags = { env = \"development\" } eks { instance_profile_arn = aws_iam_instance_profile.test.arn dns_cluster_ip = \"10.100.0.10\" security_groups = [aws_security_group.test.id] } }","title":"Create node configuration with CAST AI Terraform provider"},{"location":"guides/node-configuration/#create-node-configuration-with-cast-ai-api","text":"For API operations consult the generated documentation .","title":"Create node configuration with CAST AI API"},{"location":"guides/node-configuration/#delete-node-configuration","text":"To delete a node configuration, the following has to be true: the configuration is not linked to a node template if the configuration is marked as \"default\", it must not be the latest version","title":"Delete node configuration"},{"location":"guides/node-configuration/#delete-node-configuration-in-cast-ai-console","text":"In the node configuration view, click \"...\" of the configuration you wish to delete and then \"Delete configuration\".","title":"Delete node configuration in CAST AI Console"},{"location":"guides/node-configuration/#node-view","text":"In the \"Nodes\" tab, you can view and filter nodes based on applied node configuration:","title":"Node view"},{"location":"guides/node-kernel-tuning/","text":"Tuning node kernel parameters \u00b6 Kubernetes DaemonSet schedules pod on in each, and we can use it to configure linux kernel parameters. Using the example below you could configure linux kernel using sysctl depending on your requirements. apiVersion : apps/v1 kind : DaemonSet metadata : name : node-kernel-tuning spec : selector : matchLabels : app : node-kernel-tuning template : metadata : labels : app : node-kernel-tuning spec : hostNetwork : yes initContainers : - name : init image : alpine:3.14 command : - /bin/sh - -xc - | sysctl net.ipv4.tcp_keepalive_time=7200 sysctl fs.inotify.max_user_watches=524288 securityContext : privileged : true containers : - name : sleep image : alpine:3.14 command : - /bin/sh - -c - | while true; do sleep 60s; done resources : requests : cpu : 10m memory : 10Mi limits : cpu : 10m memory : 10Mi Using DaemonSet is not ideal as it still requires keep dummy pod running on each node. There is an open issue to support running one time job on each node.","title":"Node kernel tuning"},{"location":"guides/node-kernel-tuning/#tuning-node-kernel-parameters","text":"Kubernetes DaemonSet schedules pod on in each, and we can use it to configure linux kernel parameters. Using the example below you could configure linux kernel using sysctl depending on your requirements. apiVersion : apps/v1 kind : DaemonSet metadata : name : node-kernel-tuning spec : selector : matchLabels : app : node-kernel-tuning template : metadata : labels : app : node-kernel-tuning spec : hostNetwork : yes initContainers : - name : init image : alpine:3.14 command : - /bin/sh - -xc - | sysctl net.ipv4.tcp_keepalive_time=7200 sysctl fs.inotify.max_user_watches=524288 securityContext : privileged : true containers : - name : sleep image : alpine:3.14 command : - /bin/sh - -c - | while true; do sleep 60s; done resources : requests : cpu : 10m memory : 10Mi limits : cpu : 10m memory : 10Mi Using DaemonSet is not ideal as it still requires keep dummy pod running on each node. There is an open issue to support running one time job on each node.","title":"Tuning node kernel parameters"},{"location":"guides/pod-placement/","text":"Configure pod placement by topology \u00b6 This guide will show how to place pods in particular node, zone, region, cloud, etc., using labels and advanced Kubernetes scheduling features. Kubernetes supports this by using: Node selector Node affinity and anti-affinity Topology spread constraints All of these methods require special labels to be present on each Kubernetes node. External clusters connected to CAST AI \u00b6 CAST AI supports the following labels: Label Type Description Example(s) kubernetes.io/arch and beta.kubernetes.io/arch well-known Node CPU architecture amd64 node.kubernetes.io/instance-type and beta.kubernetes.io/instance-type well-known Node type (cloud-specific) t3a.large , e2-standard-4 kubernetes.io/os and beta.kubernetes.io/os well-known Node Operating System linux kubernetes.io/hostname well-known Node Hostname ip-192-168-32-94.eu-central-1.compute.internal , testcluster-31qd-gcp-3ead topology.kubernetes.io/region and failure-domain.beta.kubernetes.io/region well-known Node region in the CSP eu-central-1 , europe-central1 topology.kubernetes.io/zone and failure-domain.beta.kubernetes.io/zone well-known Node zone of the region in the CSP eu-central-1a , europe-central1-a provisioner.cast.ai/managed-by CAST AI specific CAST AI managed node cast.ai provisioner.cast.ai/node-id CAST AI specific CAST AI node ID 816d634e-9fd5-4eed-b13d-9319933c9ef0 scheduling.cast.ai/spot CAST AI specific Node lifecycle type - spot true scheduling.cast.ai/spot-backup CAST AI specific A fallback for spot instance true topology.cast.ai/subnet-id CAST AI specific Node subnet ID subnet-006a6d1f18fc5d390 scheduling.cast.ai/storage-optimized CAST AI specific Local SSD attached node true scheduling.cast.ai/compute-optimized CAST AI specific A compute optimized instance true Highly-available pod scheduling \u00b6 Pods can be scheduled in a highly-available fashion by using the topology spread constraints feature. CAST AI supports these fault-domains, i.e. topology keys: topology.kubernetes.io/zone - enables your pods to be spread between availability zones, taking advantage of cloud redundancy. CAST AI will treat whenUnstatisfiable property equally both in DoNotSchedule and ScheduleAnyway modes. Thus, ScheduleAnyway gets the best treatment, getting the chance to spread pods across multiple availability zones. Meanwhile, Kubernetes scheduler is still free to use any free space for pods with ScheduleAnyway . You can use this less-restrictive topology spread option as a hint for CAST AI autoscaler that spreading across zones is preferred. The deployment described below will be spread and scheduled on all availability zones supported by your cluster: apiVersion : apps/v1 kind : Deployment metadata : labels : app : az-topology-spread name : az-topology-spread spec : replicas : 30 selector : matchLabels : app : az-topology-spread template : metadata : labels : app : az-topology-spread spec : topologySpreadConstraints : - maxSkew : 1 topologyKey : topology.kubernetes.io/zone whenUnsatisfiable : DoNotSchedule labelSelector : matchExpressions : - key : app operator : In values : - az-topology-spread containers : - image : nginx name : nginx Scheduling on nodes with locally attached SSD \u00b6 Storage optimized nodes have local SSDs backed by nvme drivers which provide higher throughput and lower latency than standard disks. It's an ideal choice for workloads that require efficient local storage. Pods can request a storage optimized node by defining a node selector (or a required node affinity) and toleration for label scheduling.cast.ai/storage-optimized . Furthermore, pods can control the amount of available storage by specifying ephemeral storage resource requests. If resource requests aren't specified, CAST AI will still provision a storage optimized node but the available storage amount will be the lowest possible based on the cloud offerings. Currently supported clouds for storage optimized nodes: AWS GCP The pod described below will be scheduled on a node with locally attached SSD disks. apiVersion : v1 kind : Pod metadata : name : demopod spec : nodeSelector : scheduling.cast.ai/storage-optimized : \"true\" tolerations : - key : scheduling.cast.ai/storage-optimized operator : Exists containers : - name : app image : nginx resources : requests : ephemeral-storage : \"2Gi\" volumeMounts : - name : ephemeral mountPath : \"/tmp\" volumes : - name : ephemeral emptyDir : {} Scheduling on compute optimized nodes \u00b6 Compute optimized instances are ideal for compute-bound applications that benefit from high-performance processors. They offer the highest consistent performance per core to support real-time application performance. The pod described below will be scheduled on a compute optimized instance. apiVersion : v1 kind : Pod metadata : name : demopod spec : nodeSelector : scheduling.cast.ai/compute-optimized : \"true\" containers : - name : app image : nginx CAST AI multi cloud Kubernetes clusters \u00b6 CAST AI multi cloud Kubernetes cluster nodes are already equipped with the following labels: Label Type Description Example(s) node.kubernetes.io/instance-type well-known Node type (cloud-specific) t3a.large, e2-standard-4 kubernetes.io/arch well-known Node CPU architecture amd64 kubernetes.io/hostname well-known Node Hostname ip-10-10-2-81, testcluster-31qd-gcp-3ead kubernetes.io/os well-known Node Operating System linux topology.kubernetes.io/region well-known Node region in the CSP eu-central-1 topology.kubernetes.io/zone well-known Node zone of the region in the CSP eu-central-1a topology.cast.ai/csp CAST AI specific Node Cloud Service Provider aws, gcp, azure How to a isolate specific workloads \u00b6 It's a best practice to set workload requests and limits identical and distribute various workloads among all the nodes in the cluster so that Law of Averages would provide best performance and availability. Having said that, there might be some edge case to isolate volatile workloads to their nodes and not mix them with other workloads in the same clusters. In such scenario we will use affinity.podAntiAffinity : affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : - topologyKey : kubernetes.io/hostname labelSelector : matchExpressions : - key : <any POD label> operator : DoesNotExist Pod example: apiVersion : apps/v1 kind : Deployment metadata : name : worklow-jobs labels : app : workflows spec : replicas : 2 selector : matchLabels : app : workflows template : metadata : labels : app : workflows no-requests-workflows : \"true\" spec : affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : - topologyKey : kubernetes.io/hostname labelSelector : matchExpressions : - key : no-requests-workflows operator : DoesNotExist containers : - name : nginx image : nginx:1.14.2 ports : - containerPort : 80 resources : requests : cpu : 300m","title":"Pod placement"},{"location":"guides/pod-placement/#configure-pod-placement-by-topology","text":"This guide will show how to place pods in particular node, zone, region, cloud, etc., using labels and advanced Kubernetes scheduling features. Kubernetes supports this by using: Node selector Node affinity and anti-affinity Topology spread constraints All of these methods require special labels to be present on each Kubernetes node.","title":"Configure pod placement by topology"},{"location":"guides/pod-placement/#external-clusters-connected-to-cast-ai","text":"CAST AI supports the following labels: Label Type Description Example(s) kubernetes.io/arch and beta.kubernetes.io/arch well-known Node CPU architecture amd64 node.kubernetes.io/instance-type and beta.kubernetes.io/instance-type well-known Node type (cloud-specific) t3a.large , e2-standard-4 kubernetes.io/os and beta.kubernetes.io/os well-known Node Operating System linux kubernetes.io/hostname well-known Node Hostname ip-192-168-32-94.eu-central-1.compute.internal , testcluster-31qd-gcp-3ead topology.kubernetes.io/region and failure-domain.beta.kubernetes.io/region well-known Node region in the CSP eu-central-1 , europe-central1 topology.kubernetes.io/zone and failure-domain.beta.kubernetes.io/zone well-known Node zone of the region in the CSP eu-central-1a , europe-central1-a provisioner.cast.ai/managed-by CAST AI specific CAST AI managed node cast.ai provisioner.cast.ai/node-id CAST AI specific CAST AI node ID 816d634e-9fd5-4eed-b13d-9319933c9ef0 scheduling.cast.ai/spot CAST AI specific Node lifecycle type - spot true scheduling.cast.ai/spot-backup CAST AI specific A fallback for spot instance true topology.cast.ai/subnet-id CAST AI specific Node subnet ID subnet-006a6d1f18fc5d390 scheduling.cast.ai/storage-optimized CAST AI specific Local SSD attached node true scheduling.cast.ai/compute-optimized CAST AI specific A compute optimized instance true","title":"External clusters connected to CAST AI"},{"location":"guides/pod-placement/#highly-available-pod-scheduling","text":"Pods can be scheduled in a highly-available fashion by using the topology spread constraints feature. CAST AI supports these fault-domains, i.e. topology keys: topology.kubernetes.io/zone - enables your pods to be spread between availability zones, taking advantage of cloud redundancy. CAST AI will treat whenUnstatisfiable property equally both in DoNotSchedule and ScheduleAnyway modes. Thus, ScheduleAnyway gets the best treatment, getting the chance to spread pods across multiple availability zones. Meanwhile, Kubernetes scheduler is still free to use any free space for pods with ScheduleAnyway . You can use this less-restrictive topology spread option as a hint for CAST AI autoscaler that spreading across zones is preferred. The deployment described below will be spread and scheduled on all availability zones supported by your cluster: apiVersion : apps/v1 kind : Deployment metadata : labels : app : az-topology-spread name : az-topology-spread spec : replicas : 30 selector : matchLabels : app : az-topology-spread template : metadata : labels : app : az-topology-spread spec : topologySpreadConstraints : - maxSkew : 1 topologyKey : topology.kubernetes.io/zone whenUnsatisfiable : DoNotSchedule labelSelector : matchExpressions : - key : app operator : In values : - az-topology-spread containers : - image : nginx name : nginx","title":"Highly-available pod scheduling"},{"location":"guides/pod-placement/#scheduling-on-nodes-with-locally-attached-ssd","text":"Storage optimized nodes have local SSDs backed by nvme drivers which provide higher throughput and lower latency than standard disks. It's an ideal choice for workloads that require efficient local storage. Pods can request a storage optimized node by defining a node selector (or a required node affinity) and toleration for label scheduling.cast.ai/storage-optimized . Furthermore, pods can control the amount of available storage by specifying ephemeral storage resource requests. If resource requests aren't specified, CAST AI will still provision a storage optimized node but the available storage amount will be the lowest possible based on the cloud offerings. Currently supported clouds for storage optimized nodes: AWS GCP The pod described below will be scheduled on a node with locally attached SSD disks. apiVersion : v1 kind : Pod metadata : name : demopod spec : nodeSelector : scheduling.cast.ai/storage-optimized : \"true\" tolerations : - key : scheduling.cast.ai/storage-optimized operator : Exists containers : - name : app image : nginx resources : requests : ephemeral-storage : \"2Gi\" volumeMounts : - name : ephemeral mountPath : \"/tmp\" volumes : - name : ephemeral emptyDir : {}","title":"Scheduling on nodes with locally attached SSD"},{"location":"guides/pod-placement/#scheduling-on-compute-optimized-nodes","text":"Compute optimized instances are ideal for compute-bound applications that benefit from high-performance processors. They offer the highest consistent performance per core to support real-time application performance. The pod described below will be scheduled on a compute optimized instance. apiVersion : v1 kind : Pod metadata : name : demopod spec : nodeSelector : scheduling.cast.ai/compute-optimized : \"true\" containers : - name : app image : nginx","title":"Scheduling on compute optimized nodes"},{"location":"guides/pod-placement/#cast-ai-multi-cloud-kubernetes-clusters","text":"CAST AI multi cloud Kubernetes cluster nodes are already equipped with the following labels: Label Type Description Example(s) node.kubernetes.io/instance-type well-known Node type (cloud-specific) t3a.large, e2-standard-4 kubernetes.io/arch well-known Node CPU architecture amd64 kubernetes.io/hostname well-known Node Hostname ip-10-10-2-81, testcluster-31qd-gcp-3ead kubernetes.io/os well-known Node Operating System linux topology.kubernetes.io/region well-known Node region in the CSP eu-central-1 topology.kubernetes.io/zone well-known Node zone of the region in the CSP eu-central-1a topology.cast.ai/csp CAST AI specific Node Cloud Service Provider aws, gcp, azure","title":"CAST AI multi cloud Kubernetes clusters"},{"location":"guides/pod-placement/#how-to-a-isolate-specific-workloads","text":"It's a best practice to set workload requests and limits identical and distribute various workloads among all the nodes in the cluster so that Law of Averages would provide best performance and availability. Having said that, there might be some edge case to isolate volatile workloads to their nodes and not mix them with other workloads in the same clusters. In such scenario we will use affinity.podAntiAffinity : affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : - topologyKey : kubernetes.io/hostname labelSelector : matchExpressions : - key : <any POD label> operator : DoesNotExist Pod example: apiVersion : apps/v1 kind : Deployment metadata : name : worklow-jobs labels : app : workflows spec : replicas : 2 selector : matchLabels : app : workflows template : metadata : labels : app : workflows no-requests-workflows : \"true\" spec : affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : - topologyKey : kubernetes.io/hostname labelSelector : matchExpressions : - key : no-requests-workflows operator : DoesNotExist containers : - name : nginx image : nginx:1.14.2 ports : - containerPort : 80 resources : requests : cpu : 300m","title":"How to a isolate specific workloads"},{"location":"guides/spot/","text":"Spot/Preemptible Instances \u00b6 The CAST AI autoscaler supports running your workloads on Spot/Preemptible instances. This guide will help you configure and run it in 5 minutes. Available configurations \u00b6 Tolerations \u00b6 When to use: Spot instances are optional When a pod is marked only with tolerations, the Kubernetes scheduler could place such a pod/pods on regular nodes as well. tolerations : - key : scheduling.cast.ai/spot operator : Exists Node Selectors \u00b6 When to use: only use Spot instances If you want to make sure that a pod is scheduled on Spot instances only, add nodeSelector as well as per the example below. The autoscaler will then ensure that only a Spot instance is picked whenever your pod requires additional workload in the cluster. tolerations : - key : scheduling.cast.ai/spot operator : Exists nodeSelector : scheduling.cast.ai/spot : \"true\" Node Affinity \u00b6 When to use: Spot instances are preferred - if not available, fall back to on-demand nodes When a Spot instance is interrupted, and on-demand nodes in the cluster have available capacity, pods that previously ran on the Spot instance will be scheduled on the available on-demand nodes if the following affinity rule is applied: spec : affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : scheduling.cast.ai/spot operator : Exists If you want to move pods back to Spot instances, use the Rebalancer feature. Spot Reliability \u00b6 When to use: you want to minimize workload interruptions The Autoscaler can identify which instance types are less likely to be interrupted. You can set a default reliability value cluster-wide in spot instance policy . If you want to control that per-workload, e.g. leave most cost-efficient value globally and only choose more stable instances for specific pods, define this in the deployment configuration by setting scheduling.cast.ai/spot-reliability label on the pod. Here's an example of how it's done for the typical deployment: spec : template : metadata : labels : scheduling.cast.ai/spot-reliability : 10 Reliability is measured by \"what is the percentage of reclaimed instances during trailing month for this instance type\". This tag specifies an upper limit - all instances below specified reliability value will be considered. The value is a percentage (range is 1-100), and the meaningful values are: 5 : most reliable category; by using this value you'll restrict the Autoscaler to use only the narrowest set of Spot instance types; 10 - 15 : reasonable value range to compromise between reliability and price; 25 and above: typically most instances fall into this category. For AWS, have a look at Spot instance advisor to get an idea which instances correspond to which reliability category. Spot/Preemptible Instances fallback \u00b6 CAST AI supports the fallback of Spot/Preemptible instances to on-demand nodes in case there is no Spot/Preemptible instance availability. Our Autoscaler will temporarily add an on-demand node for your Spot-only workloads to run on. Once inventory of Spot/Preemptible instances is again available, on-demand nodes used for the fallback will be replaced with actual Spot/Preemptible instances. Fallback on-demand instances will be labeled with scheduling.cast.ai/spot-fallback:\"true\" label. To enable this feature turn on spot fallback policy using API: PUT /v1/kubernetes/clusters/{clusterId}/policies { ... \"spotInstances\" : { \"clouds\" : [ \"aws\" ], \"enabled\" : true , \"maxReclaimRate\" : 0 , \"spotBackups\" : { \"enabled\" : true , // t his parame ter will e na ble spo t fall back feature \"spotBackupRestoreRateSeconds\" : 1800 // co nf igure how o ften CAST AI should tr y t o swi t ch back t o spo t /preemp t ible i nstan ces } } ... } Step-by-step deployment on Spot Instance \u00b6 In this step-by-step guide, we demonstrate how to use Spot Instances with your CAST AI clusters. To do that, we will use an example NGINX deployment configured to run only on Spot/Preemptible instances. 1. Enable relevant policies \u00b6 To start using spot instances go to Autoscaler menu in the UI and enable the following policies: Spot/Preemptible instances policy This policy allows the Autoscaler to use Spot instances. Unschedulable pods policy This policy requests an additional workload to be scheduled based on your deployment requirements (i.e. run on Spot instances). 2. Example deployment \u00b6 Save the following yaml file, and name it: nginx.yaml : apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment labels : app : nginx spec : replicas : 1 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : nodeSelector : scheduling.cast.ai/spot : \"true\" tolerations : - key : scheduling.cast.ai/spot operator : Exists containers : - name : nginx image : nginx:1.14.2 ports : - containerPort : 80 resources : requests : cpu : '2' limits : cpu : '3' 2.1. Apply the example deployment \u00b6 With kubeconfig set in your current shell session, you can execute the following (or use other means of applying deployment files): kubectl apply -f ngninx.yaml 2.2. Wait several minutes \u00b6 Once the deployment is created, it will take up to several minutes for the Autoscaler to pick up the information about your pending deployment and schedule the relevant workloads in order to satisfy the deployment needs, such as: This deployment tolerates Spot instances This deployment must run only on Spot instances 3. Spot Instance added \u00b6 You can see your newly added Spot instance in the cluster node list. 3.1. AWS instance list \u00b6 Just to double-check, go to the AWS console and check that the added node has the Lifecycle: spot indicator.","title":"Spot/Preemptible Instances"},{"location":"guides/spot/#spotpreemptible-instances","text":"The CAST AI autoscaler supports running your workloads on Spot/Preemptible instances. This guide will help you configure and run it in 5 minutes.","title":"Spot/Preemptible Instances"},{"location":"guides/spot/#available-configurations","text":"","title":"Available configurations"},{"location":"guides/spot/#tolerations","text":"When to use: Spot instances are optional When a pod is marked only with tolerations, the Kubernetes scheduler could place such a pod/pods on regular nodes as well. tolerations : - key : scheduling.cast.ai/spot operator : Exists","title":"Tolerations"},{"location":"guides/spot/#node-selectors","text":"When to use: only use Spot instances If you want to make sure that a pod is scheduled on Spot instances only, add nodeSelector as well as per the example below. The autoscaler will then ensure that only a Spot instance is picked whenever your pod requires additional workload in the cluster. tolerations : - key : scheduling.cast.ai/spot operator : Exists nodeSelector : scheduling.cast.ai/spot : \"true\"","title":"Node Selectors"},{"location":"guides/spot/#node-affinity","text":"When to use: Spot instances are preferred - if not available, fall back to on-demand nodes When a Spot instance is interrupted, and on-demand nodes in the cluster have available capacity, pods that previously ran on the Spot instance will be scheduled on the available on-demand nodes if the following affinity rule is applied: spec : affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : scheduling.cast.ai/spot operator : Exists If you want to move pods back to Spot instances, use the Rebalancer feature.","title":"Node Affinity"},{"location":"guides/spot/#spot-reliability","text":"When to use: you want to minimize workload interruptions The Autoscaler can identify which instance types are less likely to be interrupted. You can set a default reliability value cluster-wide in spot instance policy . If you want to control that per-workload, e.g. leave most cost-efficient value globally and only choose more stable instances for specific pods, define this in the deployment configuration by setting scheduling.cast.ai/spot-reliability label on the pod. Here's an example of how it's done for the typical deployment: spec : template : metadata : labels : scheduling.cast.ai/spot-reliability : 10 Reliability is measured by \"what is the percentage of reclaimed instances during trailing month for this instance type\". This tag specifies an upper limit - all instances below specified reliability value will be considered. The value is a percentage (range is 1-100), and the meaningful values are: 5 : most reliable category; by using this value you'll restrict the Autoscaler to use only the narrowest set of Spot instance types; 10 - 15 : reasonable value range to compromise between reliability and price; 25 and above: typically most instances fall into this category. For AWS, have a look at Spot instance advisor to get an idea which instances correspond to which reliability category.","title":"Spot Reliability"},{"location":"guides/spot/#spotpreemptible-instances-fallback","text":"CAST AI supports the fallback of Spot/Preemptible instances to on-demand nodes in case there is no Spot/Preemptible instance availability. Our Autoscaler will temporarily add an on-demand node for your Spot-only workloads to run on. Once inventory of Spot/Preemptible instances is again available, on-demand nodes used for the fallback will be replaced with actual Spot/Preemptible instances. Fallback on-demand instances will be labeled with scheduling.cast.ai/spot-fallback:\"true\" label. To enable this feature turn on spot fallback policy using API: PUT /v1/kubernetes/clusters/{clusterId}/policies { ... \"spotInstances\" : { \"clouds\" : [ \"aws\" ], \"enabled\" : true , \"maxReclaimRate\" : 0 , \"spotBackups\" : { \"enabled\" : true , // t his parame ter will e na ble spo t fall back feature \"spotBackupRestoreRateSeconds\" : 1800 // co nf igure how o ften CAST AI should tr y t o swi t ch back t o spo t /preemp t ible i nstan ces } } ... }","title":"Spot/Preemptible Instances fallback"},{"location":"guides/spot/#step-by-step-deployment-on-spot-instance","text":"In this step-by-step guide, we demonstrate how to use Spot Instances with your CAST AI clusters. To do that, we will use an example NGINX deployment configured to run only on Spot/Preemptible instances.","title":"Step-by-step deployment on Spot Instance"},{"location":"guides/spot/#1-enable-relevant-policies","text":"To start using spot instances go to Autoscaler menu in the UI and enable the following policies: Spot/Preemptible instances policy This policy allows the Autoscaler to use Spot instances. Unschedulable pods policy This policy requests an additional workload to be scheduled based on your deployment requirements (i.e. run on Spot instances).","title":"1. Enable relevant policies"},{"location":"guides/spot/#2-example-deployment","text":"Save the following yaml file, and name it: nginx.yaml : apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment labels : app : nginx spec : replicas : 1 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : nodeSelector : scheduling.cast.ai/spot : \"true\" tolerations : - key : scheduling.cast.ai/spot operator : Exists containers : - name : nginx image : nginx:1.14.2 ports : - containerPort : 80 resources : requests : cpu : '2' limits : cpu : '3'","title":"2. Example deployment"},{"location":"guides/spot/#21-apply-the-example-deployment","text":"With kubeconfig set in your current shell session, you can execute the following (or use other means of applying deployment files): kubectl apply -f ngninx.yaml","title":"2.1. Apply the example deployment"},{"location":"guides/spot/#22-wait-several-minutes","text":"Once the deployment is created, it will take up to several minutes for the Autoscaler to pick up the information about your pending deployment and schedule the relevant workloads in order to satisfy the deployment needs, such as: This deployment tolerates Spot instances This deployment must run only on Spot instances","title":"2.2. Wait several minutes"},{"location":"guides/spot/#3-spot-instance-added","text":"You can see your newly added Spot instance in the cluster node list.","title":"3. Spot Instance added"},{"location":"guides/spot/#31-aws-instance-list","text":"Just to double-check, go to the AWS console and check that the added node has the Lifecycle: spot indicator.","title":"3.1. AWS instance list"},{"location":"guides/start-saving-quickly/","text":"Start saving on your connected EKS cluster immediately \u00b6 So, you liked the results of the Savings Estimator after connecting your existing EKS cluster to CAST AI, but don't want to wait until these savings reach you in a slow and risk-free ongoing process. You can speed this up, here's how. Onboard credentials (EKS) \u00b6 First, you need to onboard the credentials so IAM user for CAST AI gets created and cluster can be optimized using CAST AI policies. Enable Autoscaler \u00b6 Enable Node deletion policy - this policy will remove nodes without pods (ignores DaemonSets). Enable Unscheduled Pod policy - it will make sure that you always have the capacity in the cluster to run pods. The Unscheduled Pod policy will provision a new node when required, taking no more than 2-3 minutes. Enable and adjust Node constraints % sub-policy of Unscheduled pods policy. for migration purposes - each node adds overhead through DaemonSets, also more smaller nodes means that more pods won't find their destination on the same node (added latency). So ideally, one should have nodes that are as large as possible, but 5-6 nodes minimum (for below 200 CPUs cluster) for good SLA and adequate capacity distribution for the lifecycle process (upgrades, patching). Take the number from Available Savings - this is the total amount of nodes you should have in the optimized state. In the Policies tab, it should look like this: \"Slow and safe\" or \"maximize savings now\" \u00b6 Evictor is our recommended way - it will constantly look for inefficiencies. But reducing costs in a safe manner takes time. If you want to maximize your savings as quickly as possible and you have a maintenance window, you can do it in CAST AI. Install Evictor \u00b6 Evictor will compact your pods into fewer nodes, creating empty nodes that will be removed by the Node deletion policy. Install and configure Evictor using this guide then complete remaining steps listed below. Stir the pod with manual migration \u00b6 You will have to get rid of your existing nodes and let CAST AI create an optimized state right away. This might cause some downtime depending on your workload configuration. For example, pick 50% of your nodes in one availability zone (AZ) or 20% of nodes if your connected cluster is in a single AZ. kubectl get nodes -Lfailure-domain.beta.kubernetes.io/zone --selector=eks.amazonaws.com/nodegroup-image The percentage is arbitrary - it depends on your risk appetite and how much time you want to spend on this. Taint (cordon) the selected nodes, so no new pods are placed on these nodes. We like Lens Kubernetes IDE, but you can use kubectl as well: kubectl cordon nodeName1 kubectl cordon nodeName2 And now drain these nodes: kubectl drain nodeName1 --ignore-daemonsets --delete-local-data kubectl drain nodeName2 --ignore-daemonsets --delete-local-data Some nodes will not drain because of the Disruption Budget violation (downtime). These cases should be fixed since they are going to cause pain in the future (or at least noted to be addressed at a more convenient time). If you want to progress anyway and accept downtime, cancel the drain command and retry draining with the additional --force flag. You should see that the drained nodes disappear (empty Node deletion policy) and, in few moments, new nodes in the same availability zone appear (Unscheduled Pod policy). Check the remaining nodes. You will see that list is shorter because the command below selects only nodes in the AWS autoscaling group (ASG) and new nodes don't use ASG. kubectl get nodes -Lfailure-domain.beta.kubernetes.io/zone --selector=eks.amazonaws.com/nodegroup-image Select next batch -> cordon -> drain -> write down problematic pod that don't migrate easily -> rinse and repeat until the list is empty. Utilize Spot instances \u00b6 In the Available savings window, you can find a list of deployments that could use Spot instances. I have a recommendation service running with 10 replicas. I could separate this workload into two deployments: Reduce the current replica count to a bare minimum (in my case, 2 replicas), Create a copy of deployment with \"-spot\" appending name, add toleration, and set to 8 replicas - or better, configure to use KEDA, see HPA documentation ... tolerations : - key : scheduling.cast.ai/spot operator : Exists ... You're all done \u00b6 Share the Available savings window screenshot with your CFO/manager - there's nothing left to save. Install Evictor if you haven't already done that.","title":"Start saving quickly"},{"location":"guides/start-saving-quickly/#start-saving-on-your-connected-eks-cluster-immediately","text":"So, you liked the results of the Savings Estimator after connecting your existing EKS cluster to CAST AI, but don't want to wait until these savings reach you in a slow and risk-free ongoing process. You can speed this up, here's how.","title":"Start saving on your connected EKS cluster immediately"},{"location":"guides/start-saving-quickly/#onboard-credentials-eks","text":"First, you need to onboard the credentials so IAM user for CAST AI gets created and cluster can be optimized using CAST AI policies.","title":"Onboard credentials (EKS)"},{"location":"guides/start-saving-quickly/#enable-autoscaler","text":"Enable Node deletion policy - this policy will remove nodes without pods (ignores DaemonSets). Enable Unscheduled Pod policy - it will make sure that you always have the capacity in the cluster to run pods. The Unscheduled Pod policy will provision a new node when required, taking no more than 2-3 minutes. Enable and adjust Node constraints % sub-policy of Unscheduled pods policy. for migration purposes - each node adds overhead through DaemonSets, also more smaller nodes means that more pods won't find their destination on the same node (added latency). So ideally, one should have nodes that are as large as possible, but 5-6 nodes minimum (for below 200 CPUs cluster) for good SLA and adequate capacity distribution for the lifecycle process (upgrades, patching). Take the number from Available Savings - this is the total amount of nodes you should have in the optimized state. In the Policies tab, it should look like this:","title":"Enable Autoscaler"},{"location":"guides/start-saving-quickly/#slow-and-safe-or-maximize-savings-now","text":"Evictor is our recommended way - it will constantly look for inefficiencies. But reducing costs in a safe manner takes time. If you want to maximize your savings as quickly as possible and you have a maintenance window, you can do it in CAST AI.","title":"\"Slow and safe\" or \"maximize savings now\""},{"location":"guides/start-saving-quickly/#install-evictor","text":"Evictor will compact your pods into fewer nodes, creating empty nodes that will be removed by the Node deletion policy. Install and configure Evictor using this guide then complete remaining steps listed below.","title":"Install Evictor"},{"location":"guides/start-saving-quickly/#stir-the-pod-with-manual-migration","text":"You will have to get rid of your existing nodes and let CAST AI create an optimized state right away. This might cause some downtime depending on your workload configuration. For example, pick 50% of your nodes in one availability zone (AZ) or 20% of nodes if your connected cluster is in a single AZ. kubectl get nodes -Lfailure-domain.beta.kubernetes.io/zone --selector=eks.amazonaws.com/nodegroup-image The percentage is arbitrary - it depends on your risk appetite and how much time you want to spend on this. Taint (cordon) the selected nodes, so no new pods are placed on these nodes. We like Lens Kubernetes IDE, but you can use kubectl as well: kubectl cordon nodeName1 kubectl cordon nodeName2 And now drain these nodes: kubectl drain nodeName1 --ignore-daemonsets --delete-local-data kubectl drain nodeName2 --ignore-daemonsets --delete-local-data Some nodes will not drain because of the Disruption Budget violation (downtime). These cases should be fixed since they are going to cause pain in the future (or at least noted to be addressed at a more convenient time). If you want to progress anyway and accept downtime, cancel the drain command and retry draining with the additional --force flag. You should see that the drained nodes disappear (empty Node deletion policy) and, in few moments, new nodes in the same availability zone appear (Unscheduled Pod policy). Check the remaining nodes. You will see that list is shorter because the command below selects only nodes in the AWS autoscaling group (ASG) and new nodes don't use ASG. kubectl get nodes -Lfailure-domain.beta.kubernetes.io/zone --selector=eks.amazonaws.com/nodegroup-image Select next batch -> cordon -> drain -> write down problematic pod that don't migrate easily -> rinse and repeat until the list is empty.","title":"Stir the pod with manual migration"},{"location":"guides/start-saving-quickly/#utilize-spot-instances","text":"In the Available savings window, you can find a list of deployments that could use Spot instances. I have a recommendation service running with 10 replicas. I could separate this workload into two deployments: Reduce the current replica count to a bare minimum (in my case, 2 replicas), Create a copy of deployment with \"-spot\" appending name, add toleration, and set to 8 replicas - or better, configure to use KEDA, see HPA documentation ... tolerations : - key : scheduling.cast.ai/spot operator : Exists ...","title":"Utilize Spot instances"},{"location":"guides/start-saving-quickly/#youre-all-done","text":"Share the Available savings window screenshot with your CFO/manager - there's nothing left to save. Install Evictor if you haven't already done that.","title":"You're all done"},{"location":"guides/volumes/","text":"Dynamic volume provisioning \u00b6 Dynamic volume provisioning allows storage volumes to be created on-demand. Without dynamic provisioning, cluster administrators have to create new storage volumes manually (using cloud or storage providers) and the corresponding PersistentVolume objects for the storage to be available in Kubernetes. Dynamic volume provisioning is enabled by default on the CAST AI cluster. Overview \u00b6 Each CAST AI cluster is pre-configured with the default StorageClass that handles volume requests. \u00bb kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE cast-block-storage ( default ) storage.csi.cast.ai Delete WaitForFirstConsumer true 2m18s The binding mode WaitForFirstConsumer will delay the binding and provisioning of a PersistentVolume until a Pod using the PVC is created. Meaning, the volume will be created and attached to the Node on which a Pod using the PVC will be run. In the case of a Pod replicated across multiple clouds, volumes will be distributed across clouds as well. This will limit Pod scheduling only to the nodes of the same cloud since to reschedule a Pod to a different cloud service, the volume must be replicated to that cloud. This limitation will be removed by the cross-cloud volume replication feature which is not available at the moment. Deleting a cluster will delete all the volumes that were provisioned dynamically. Using dynamic volumes \u00b6 Creating persitent volume claim (PVC) \u00b6 Users can request dynamically provisioned storage by simply creating PersistentVolumeClaim and a Pod that will use it. apiVersion : v1 kind : PersistentVolumeClaim metadata : name : example-claim spec : accessModes : - ReadWriteOnce resources : requests : storage : 50Gi Pod example: apiVersion : v1 kind : Pod metadata : name : app spec : containers : - name : app image : centos command : [ \"/bin/sh\" ] args : [ \"-c\" , \"while true; do echo $(date -u) >> /data/out.txt; sleep 5; done\" ] volumeMounts : - name : persistent-storage mountPath : /data volumes : - name : persistent-storage persistentVolumeClaim : claimName : example-claim This claim results in a Persistent Disk being automatically provisioned. When the claim is deleted, the volume is deleted as well. Volume claim templates \u00b6 Additionally, having StatefulSet user can define volumeClaimTemplates to provision volumes without creating PVC beforehand. apiVersion : v1 kind : Service metadata : name : nginx labels : app : nginx spec : ports : - port : 80 name : web clusterIP : None selector : app : nginx --- apiVersion : apps/v1 kind : StatefulSet metadata : name : web spec : selector : matchLabels : app : nginx serviceName : \"nginx\" replicas : 3 template : metadata : labels : app : nginx spec : containers : - name : nginx image : k8s.gcr.io/nginx-slim:0.8 ports : - containerPort : 80 name : web volumeMounts : - name : www mountPath : /usr/share/nginx/html volumeClaimTemplates : - metadata : name : www spec : accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 1Gi This will result in dynamic PVC for each StatefulSet pod. \u00bb kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE www-web-0 Bound pvc-3b550fd0-4b79-449d-b1cf-f51264f975fc 1Gi RWO cast-block-storage 3m11s www-web-1 Bound pvc-0c7470a8-cc59-49d4-b2ca-5d3db45c1b60 1Gi RWO cast-block-storage 2m41s www-web-2 Bound pvc-70c341cc-fa1c-471a-882a-e46225e1824f 1Gi RWO cast-block-storage 2m18s Deleting a StatefulSet will delete all provisioned volumes. Resizing PVC \u00b6 Any PVC created using cast-block-storage StorageClass can be edited to request more space. Kubernetes will interpret a change to the storage field as a request for more space. This will trigger automatic volume resizing. \u00bb kubectl edit pvc www-web-0 Change storage field as shown below: # www-web-0... spec : accessModes : - ReadWriteOnce resources : requests : storage : 10Gi # new storage size storageClassName : cast-block-storage # www-web-0... After storage is resized successfully, we can observe new PVC capacity: k get pvc www-web-0 NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE www-web-0 Bound pvc-edd59e56-cb22-41b6-a075-ab8820f222b8 10Gi RWO cast-block-storage 4m57s","title":"Dynamic volume provisioning"},{"location":"guides/volumes/#dynamic-volume-provisioning","text":"Dynamic volume provisioning allows storage volumes to be created on-demand. Without dynamic provisioning, cluster administrators have to create new storage volumes manually (using cloud or storage providers) and the corresponding PersistentVolume objects for the storage to be available in Kubernetes. Dynamic volume provisioning is enabled by default on the CAST AI cluster.","title":"Dynamic volume provisioning"},{"location":"guides/volumes/#overview","text":"Each CAST AI cluster is pre-configured with the default StorageClass that handles volume requests. \u00bb kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE cast-block-storage ( default ) storage.csi.cast.ai Delete WaitForFirstConsumer true 2m18s The binding mode WaitForFirstConsumer will delay the binding and provisioning of a PersistentVolume until a Pod using the PVC is created. Meaning, the volume will be created and attached to the Node on which a Pod using the PVC will be run. In the case of a Pod replicated across multiple clouds, volumes will be distributed across clouds as well. This will limit Pod scheduling only to the nodes of the same cloud since to reschedule a Pod to a different cloud service, the volume must be replicated to that cloud. This limitation will be removed by the cross-cloud volume replication feature which is not available at the moment. Deleting a cluster will delete all the volumes that were provisioned dynamically.","title":"Overview"},{"location":"guides/volumes/#using-dynamic-volumes","text":"","title":"Using dynamic volumes"},{"location":"guides/volumes/#creating-persitent-volume-claim-pvc","text":"Users can request dynamically provisioned storage by simply creating PersistentVolumeClaim and a Pod that will use it. apiVersion : v1 kind : PersistentVolumeClaim metadata : name : example-claim spec : accessModes : - ReadWriteOnce resources : requests : storage : 50Gi Pod example: apiVersion : v1 kind : Pod metadata : name : app spec : containers : - name : app image : centos command : [ \"/bin/sh\" ] args : [ \"-c\" , \"while true; do echo $(date -u) >> /data/out.txt; sleep 5; done\" ] volumeMounts : - name : persistent-storage mountPath : /data volumes : - name : persistent-storage persistentVolumeClaim : claimName : example-claim This claim results in a Persistent Disk being automatically provisioned. When the claim is deleted, the volume is deleted as well.","title":"Creating persitent volume claim (PVC)"},{"location":"guides/volumes/#volume-claim-templates","text":"Additionally, having StatefulSet user can define volumeClaimTemplates to provision volumes without creating PVC beforehand. apiVersion : v1 kind : Service metadata : name : nginx labels : app : nginx spec : ports : - port : 80 name : web clusterIP : None selector : app : nginx --- apiVersion : apps/v1 kind : StatefulSet metadata : name : web spec : selector : matchLabels : app : nginx serviceName : \"nginx\" replicas : 3 template : metadata : labels : app : nginx spec : containers : - name : nginx image : k8s.gcr.io/nginx-slim:0.8 ports : - containerPort : 80 name : web volumeMounts : - name : www mountPath : /usr/share/nginx/html volumeClaimTemplates : - metadata : name : www spec : accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 1Gi This will result in dynamic PVC for each StatefulSet pod. \u00bb kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE www-web-0 Bound pvc-3b550fd0-4b79-449d-b1cf-f51264f975fc 1Gi RWO cast-block-storage 3m11s www-web-1 Bound pvc-0c7470a8-cc59-49d4-b2ca-5d3db45c1b60 1Gi RWO cast-block-storage 2m41s www-web-2 Bound pvc-70c341cc-fa1c-471a-882a-e46225e1824f 1Gi RWO cast-block-storage 2m18s Deleting a StatefulSet will delete all provisioned volumes.","title":"Volume claim templates"},{"location":"guides/volumes/#resizing-pvc","text":"Any PVC created using cast-block-storage StorageClass can be edited to request more space. Kubernetes will interpret a change to the storage field as a request for more space. This will trigger automatic volume resizing. \u00bb kubectl edit pvc www-web-0 Change storage field as shown below: # www-web-0... spec : accessModes : - ReadWriteOnce resources : requests : storage : 10Gi # new storage size storageClassName : cast-block-storage # www-web-0... After storage is resized successfully, we can observe new PVC capacity: k get pvc www-web-0 NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE www-web-0 Bound pvc-edd59e56-cb22-41b6-a075-ab8820f222b8 10Gi RWO cast-block-storage 4m57s","title":"Resizing PVC"},{"location":"guides/vpa/","text":"Right Sizing recommendations with VPA \u00b6 Recommendations example \u00b6 \u2190 On the left side you can see: The initial values that were set before Virtual Pod Autoscaler (VPA) took action. One of the pods is being terminated, due to auto mode changing the CPU/MEM values right away (causing potential downtime). \u2192 On the right side you can see: VPA recommendation couple minutes right after. VPA install guide \u00b6 In order for the instructions to work on macOS one needs to have latest version of OpenSSL from Homebrew: brew install openssl (make use of that OpenSSL version, rather than macOS native one). Run brew info openssl to see the instructions for setting this openssl version as your default one. Metrics-server up and running If you don't have metrics-server installed, run the following to install it: kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml Run kubectl top nodes to check if metrics server was installed successfully Vertical Pod Autoscaler installed Prerequisite: helm Install helm chart helm repo add fairwinds-stable https://charts.fairwinds.com/stable helm install vpa fairwinds-stable/vpa --namespace vpa --create-namespace Run helm chart tests to make sure VPA is successfully installed helm test vpa -n vpa Output similar to this should be visible: LAST DEPLOYED: Thu Apr 29 19:53:50 2021 NAMESPACE: vpa STATUS: deployed REVISION: 1 TEST SUITE: vpa-test Last Started: Thu Apr 29 20:04:13 2021 Last Completed: Thu Apr 29 20:04:13 2021 Phase: Succeeded TEST SUITE: vpa-test Last Started: Thu Apr 29 20:04:14 2021 Last Completed: Thu Apr 29 20:04:14 2021 Phase: Succeeded TEST SUITE: vpa-test Last Started: Thu Apr 29 20:04:14 2021 Last Completed: Thu Apr 29 20:04:14 2021 Phase: Succeeded TEST SUITE: vpa-checkpoint-crd-available Last Started: Thu Apr 29 20:04:14 2021 Last Completed: Thu Apr 29 20:04:16 2021 Phase: Succeeded TEST SUITE: vpa-crd-available Last Started: Thu Apr 29 20:04:16 2021 Last Completed: Thu Apr 29 20:04:19 2021 Phase: Succeeded TEST SUITE: vpa-test-create-vpa Last Started: Thu Apr 29 20:04:21 2021 Last Completed: Thu Apr 29 20:04:49 2021 Phase: Succeeded TEST SUITE: vpa-metrics-api-available Last Started: Thu Apr 29 20:04:19 2021 Last Completed: Thu Apr 29 20:04:21 2021 Phase: Succeeded NOTES: Congratulations on installing the Vertical Pod Autoscaler! Components Installed: - recommender - updater Create VPA for each deployment to get the recommendations See Configure VPA for your deployment below for examples. Each deployment that wants make use of VPA, needs to have VPA created for it. Wait a day for the VPA. Send us the output of: kubectl get vpa -A -o yaml > recommendations.txt Troubleshooting \u00b6 In case something goes wrong, detailed instructions can be found at our github . Make sure that on macOS the openssl from Homebrew is used Configure VPA for your deployment \u00b6 In order to see how it works, you can use the example bellow, which would create both example deployment & example VPA configuration for that deployment. Once the deployment yaml is applied using kubectl view recommendations with the following line: kubectl describe vpa/hamster-vpa Example output: Status: Conditions: Last Transition Time: 2021-04-27T06:13:54Z Status: True Type: RecommendationProvided Recommendation: Container Recommendations: Container Name: hamster Lower Bound: Cpu: 491m Memory: 262144k Target: Cpu: 587m Memory: 262144k Uncapped Target: Cpu: 587m Memory: 262144k Upper Bound: Cpu: 1 Memory: 262144k Events: <none> Example deployment with VPA: # This config creates a deployment with two pods, each requesting 100 millicores # and trying to utilize slightly above 500 millicores (repeatedly using CPU for # 0.5s and sleeping 0.5s). # It also creates a corresponding Vertical Pod Autoscaler that adjusts the # requests. # Note that the update mode is left unset, so it defaults to \"Auto\" mode. --- apiVersion: \"autoscaling.k8s.io/v1\" kind: VerticalPodAutoscaler metadata: name: hamster-vpa namespace: default spec: targetRef: apiVersion: \"apps/v1\" kind: Deployment name: hamster updatePolicy: # updateMode set to Off # - runs in recommendation mode # - does not mess with your pod request/limit configurations updateMode: \"Off\" resourcePolicy: containerPolicies: - containerName: '*' controlledResources: [\"cpu\", \"memory\"] --- apiVersion: apps/v1 kind: Deployment metadata: name: hamster namespace: default spec: selector: matchLabels: app: hamster replicas: 2 template: metadata: labels: app: hamster spec: securityContext: runAsNonRoot: true runAsUser: 65534 # nobody containers: - name: hamster image: k8s.gcr.io/ubuntu-slim:0.1 resources: requests: cpu: 100m memory: 50Mi command: [\"/bin/sh\"] args: - \"-c\" - \"while true; do timeout 0.5s yes >/dev/null; sleep 0.5s; done\" Available VPA modes \u00b6 \"Off\" : VPA does not automatically change resource requirements of the pods. The recommendations are calculated and can be inspected in the VPA object. \"Auto\" : VPA assigns resource requests on pod creation as well as updates them on existing pods using the preferred update mechanism. Currently this is equivalent to \"Recreate\" (see below). Once restart free (\"in-place\") update of pod requests is available, it may be used as the preferred update mechanism by the \"Auto\" mode. Warning \"Auto\" feature of VPA is experimental and may cause downtime for your applications. \"Initial\" : VPA only assigns resource requests on pod creation and never changes them later. Resources \u00b6 VPA definitive guide","title":"Right Sizing recommendations with VPA"},{"location":"guides/vpa/#right-sizing-recommendations-with-vpa","text":"","title":"Right Sizing recommendations with VPA"},{"location":"guides/vpa/#recommendations-example","text":"\u2190 On the left side you can see: The initial values that were set before Virtual Pod Autoscaler (VPA) took action. One of the pods is being terminated, due to auto mode changing the CPU/MEM values right away (causing potential downtime). \u2192 On the right side you can see: VPA recommendation couple minutes right after.","title":"Recommendations example"},{"location":"guides/vpa/#vpa-install-guide","text":"In order for the instructions to work on macOS one needs to have latest version of OpenSSL from Homebrew: brew install openssl (make use of that OpenSSL version, rather than macOS native one). Run brew info openssl to see the instructions for setting this openssl version as your default one. Metrics-server up and running If you don't have metrics-server installed, run the following to install it: kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml Run kubectl top nodes to check if metrics server was installed successfully Vertical Pod Autoscaler installed Prerequisite: helm Install helm chart helm repo add fairwinds-stable https://charts.fairwinds.com/stable helm install vpa fairwinds-stable/vpa --namespace vpa --create-namespace Run helm chart tests to make sure VPA is successfully installed helm test vpa -n vpa Output similar to this should be visible: LAST DEPLOYED: Thu Apr 29 19:53:50 2021 NAMESPACE: vpa STATUS: deployed REVISION: 1 TEST SUITE: vpa-test Last Started: Thu Apr 29 20:04:13 2021 Last Completed: Thu Apr 29 20:04:13 2021 Phase: Succeeded TEST SUITE: vpa-test Last Started: Thu Apr 29 20:04:14 2021 Last Completed: Thu Apr 29 20:04:14 2021 Phase: Succeeded TEST SUITE: vpa-test Last Started: Thu Apr 29 20:04:14 2021 Last Completed: Thu Apr 29 20:04:14 2021 Phase: Succeeded TEST SUITE: vpa-checkpoint-crd-available Last Started: Thu Apr 29 20:04:14 2021 Last Completed: Thu Apr 29 20:04:16 2021 Phase: Succeeded TEST SUITE: vpa-crd-available Last Started: Thu Apr 29 20:04:16 2021 Last Completed: Thu Apr 29 20:04:19 2021 Phase: Succeeded TEST SUITE: vpa-test-create-vpa Last Started: Thu Apr 29 20:04:21 2021 Last Completed: Thu Apr 29 20:04:49 2021 Phase: Succeeded TEST SUITE: vpa-metrics-api-available Last Started: Thu Apr 29 20:04:19 2021 Last Completed: Thu Apr 29 20:04:21 2021 Phase: Succeeded NOTES: Congratulations on installing the Vertical Pod Autoscaler! Components Installed: - recommender - updater Create VPA for each deployment to get the recommendations See Configure VPA for your deployment below for examples. Each deployment that wants make use of VPA, needs to have VPA created for it. Wait a day for the VPA. Send us the output of: kubectl get vpa -A -o yaml > recommendations.txt","title":"VPA install guide"},{"location":"guides/vpa/#troubleshooting","text":"In case something goes wrong, detailed instructions can be found at our github . Make sure that on macOS the openssl from Homebrew is used","title":"Troubleshooting"},{"location":"guides/vpa/#configure-vpa-for-your-deployment","text":"In order to see how it works, you can use the example bellow, which would create both example deployment & example VPA configuration for that deployment. Once the deployment yaml is applied using kubectl view recommendations with the following line: kubectl describe vpa/hamster-vpa Example output: Status: Conditions: Last Transition Time: 2021-04-27T06:13:54Z Status: True Type: RecommendationProvided Recommendation: Container Recommendations: Container Name: hamster Lower Bound: Cpu: 491m Memory: 262144k Target: Cpu: 587m Memory: 262144k Uncapped Target: Cpu: 587m Memory: 262144k Upper Bound: Cpu: 1 Memory: 262144k Events: <none> Example deployment with VPA: # This config creates a deployment with two pods, each requesting 100 millicores # and trying to utilize slightly above 500 millicores (repeatedly using CPU for # 0.5s and sleeping 0.5s). # It also creates a corresponding Vertical Pod Autoscaler that adjusts the # requests. # Note that the update mode is left unset, so it defaults to \"Auto\" mode. --- apiVersion: \"autoscaling.k8s.io/v1\" kind: VerticalPodAutoscaler metadata: name: hamster-vpa namespace: default spec: targetRef: apiVersion: \"apps/v1\" kind: Deployment name: hamster updatePolicy: # updateMode set to Off # - runs in recommendation mode # - does not mess with your pod request/limit configurations updateMode: \"Off\" resourcePolicy: containerPolicies: - containerName: '*' controlledResources: [\"cpu\", \"memory\"] --- apiVersion: apps/v1 kind: Deployment metadata: name: hamster namespace: default spec: selector: matchLabels: app: hamster replicas: 2 template: metadata: labels: app: hamster spec: securityContext: runAsNonRoot: true runAsUser: 65534 # nobody containers: - name: hamster image: k8s.gcr.io/ubuntu-slim:0.1 resources: requests: cpu: 100m memory: 50Mi command: [\"/bin/sh\"] args: - \"-c\" - \"while true; do timeout 0.5s yes >/dev/null; sleep 0.5s; done\"","title":"Configure VPA for your deployment"},{"location":"guides/vpa/#available-vpa-modes","text":"\"Off\" : VPA does not automatically change resource requirements of the pods. The recommendations are calculated and can be inspected in the VPA object. \"Auto\" : VPA assigns resource requests on pod creation as well as updates them on existing pods using the preferred update mechanism. Currently this is equivalent to \"Recreate\" (see below). Once restart free (\"in-place\") update of pod requests is available, it may be used as the preferred update mechanism by the \"Auto\" mode. Warning \"Auto\" feature of VPA is experimental and may cause downtime for your applications. \"Initial\" : VPA only assigns resource requests on pod creation and never changes them later.","title":"Available VPA modes"},{"location":"guides/vpa/#resources","text":"VPA definitive guide","title":"Resources"},{"location":"guides/webhooks/","text":"How to setup Notification Webhooks \u00b6 To send notifications from CAST AI Console to a external system, select the organization you want to configure the webhook. Click on the Notifications Icon -> View All Click on Webhooks Click on Add webhooks Create the Webhook Field Description Name The name of the Webhook configuration Callback Url The callback URL to send the requests to Severity Triggers The severity levels that will trigger that notification Template The template of the request that will be sent to the callback URL The Request Template should be a valid JSON . We provide a better overview on how to customize the payloads in the next section Request Template Configuration . Request Template Configuration \u00b6 We allow users to fully customize the request sent to external systems, in that way we can support almost any application out there. The Request Template is the payload sent within the webhook call, the following variables from notifications are avaialable: Variable Description Usage NotificationID The UUID of the notification, it is unique {{ .NotificationID }} OrganizationID Organization that owns the notification {{ .OrganizationID }} Severity Indicates the severity of the impact to the affected system. {{ .Severity }} Name Name of the notification {{ .Name }} Message A high-level, text summary message of the event. {{ .Message}} Details Free-form details from the event, can be parsed to JSON {{ toJSON .Details }} Timestamp When the Notification was created by CAST AI {{ toISO8601 .Timestamp }} Cluster Cluster information, might be empty if the notification ins't cluster specific {{ toJSON .Cluster }} Cluster.ID The unique identifier of the cluster on CAST AI {{ .Cluster.ID }} Cluster.Name Name of the cluster on CAST AI {{ .Cluster.Name }} Cluster.ProviderType Cloud provider of the cluster (eks, gke, aks, kops) {{ .Cluster.ProviderType }} Cluster.ProjectNamespaceID Cluster location where cloud provider organizes resources, eg.: GCP project ID, AWS account ID. {{ .Cluster.ProjectNamespaceID }} As you can see the variables are in go template style, and you can mix them anywhere you want in your Request Template. Example of Request Template Slack \u00b6 To send a notification on slack we need a simple JSON request with payload in the body, { \"text\" : \"CAST AI - {{ .Name }}\" , \"blocks\" : [ { \"type\" : \"section\" , \"text\" : { \"type\" : \"mrkdwn\" , \"text\" : \"{{ .Cluster.Name }}<br> {{ .Message}}\" } } ] } How to create the webhook url isn't in the scope of this how to, you can find information following the link https://api.slack.com/messaging/webhooks . Example of Request Template PageDuty \u00b6 Page duty accepts Alerts in the endpoint https://events.pagerduty.com/v2/enqueue . The content is a simple JSON request with the payload in the body. You can find bellow an example of request template with the available variables: { \"payload\" : { \"summary\" : \"{{ .Message }}\" , \"timestamp\" : \"{{ toISO8601 .Timestamp }}\" , \"severity\" : \"critical\" , \"source\" : \"CAST AI\" , \"component\" : \"{{ .Cluster.Name}}-{{ .Cluster.ProviderType}}-{{ .Cluster.ProjectNamespaceID }}\" , \"group\" : \"{{ .Name }}\" , \"class\" : \"kubernetes\" , \"custom_details\" : { \"details\" : \"{{ toJSON .Details }}\" } }, \"routing_key\" : \"--routing_key--\" , \"dedup_key\" : \"{{ .NotificationID }}\" , \"event_action\" : \"trigger\" , \"client\" : \"CAST AI\" , \"client_url\" : \"https://console.cast.ai/external-clusters/{{ .Cluster.ID}}?org={{ .OrganizationID }}\" , } Note that as dedup_key it was set the NotificationID, this field is unique in CAST AI, and will ensure you won't produce an alert with the same content more than once. How to create the routing_key isn't in the scope of this how to, you can find more information at https://developer.pagerduty.com/docs/ZG9jOjExMDI5NTgx-send-an-alert-event.","title":"How to setup Notification Webhooks"},{"location":"guides/webhooks/#how-to-setup-notification-webhooks","text":"To send notifications from CAST AI Console to a external system, select the organization you want to configure the webhook. Click on the Notifications Icon -> View All Click on Webhooks Click on Add webhooks Create the Webhook Field Description Name The name of the Webhook configuration Callback Url The callback URL to send the requests to Severity Triggers The severity levels that will trigger that notification Template The template of the request that will be sent to the callback URL The Request Template should be a valid JSON . We provide a better overview on how to customize the payloads in the next section Request Template Configuration .","title":"How to setup Notification Webhooks"},{"location":"guides/webhooks/#request-template-configuration","text":"We allow users to fully customize the request sent to external systems, in that way we can support almost any application out there. The Request Template is the payload sent within the webhook call, the following variables from notifications are avaialable: Variable Description Usage NotificationID The UUID of the notification, it is unique {{ .NotificationID }} OrganizationID Organization that owns the notification {{ .OrganizationID }} Severity Indicates the severity of the impact to the affected system. {{ .Severity }} Name Name of the notification {{ .Name }} Message A high-level, text summary message of the event. {{ .Message}} Details Free-form details from the event, can be parsed to JSON {{ toJSON .Details }} Timestamp When the Notification was created by CAST AI {{ toISO8601 .Timestamp }} Cluster Cluster information, might be empty if the notification ins't cluster specific {{ toJSON .Cluster }} Cluster.ID The unique identifier of the cluster on CAST AI {{ .Cluster.ID }} Cluster.Name Name of the cluster on CAST AI {{ .Cluster.Name }} Cluster.ProviderType Cloud provider of the cluster (eks, gke, aks, kops) {{ .Cluster.ProviderType }} Cluster.ProjectNamespaceID Cluster location where cloud provider organizes resources, eg.: GCP project ID, AWS account ID. {{ .Cluster.ProjectNamespaceID }} As you can see the variables are in go template style, and you can mix them anywhere you want in your Request Template.","title":"Request Template Configuration"},{"location":"guides/webhooks/#example-of-request-template-slack","text":"To send a notification on slack we need a simple JSON request with payload in the body, { \"text\" : \"CAST AI - {{ .Name }}\" , \"blocks\" : [ { \"type\" : \"section\" , \"text\" : { \"type\" : \"mrkdwn\" , \"text\" : \"{{ .Cluster.Name }}<br> {{ .Message}}\" } } ] } How to create the webhook url isn't in the scope of this how to, you can find information following the link https://api.slack.com/messaging/webhooks .","title":"Example of Request Template Slack"},{"location":"guides/webhooks/#example-of-request-template-pageduty","text":"Page duty accepts Alerts in the endpoint https://events.pagerduty.com/v2/enqueue . The content is a simple JSON request with the payload in the body. You can find bellow an example of request template with the available variables: { \"payload\" : { \"summary\" : \"{{ .Message }}\" , \"timestamp\" : \"{{ toISO8601 .Timestamp }}\" , \"severity\" : \"critical\" , \"source\" : \"CAST AI\" , \"component\" : \"{{ .Cluster.Name}}-{{ .Cluster.ProviderType}}-{{ .Cluster.ProjectNamespaceID }}\" , \"group\" : \"{{ .Name }}\" , \"class\" : \"kubernetes\" , \"custom_details\" : { \"details\" : \"{{ toJSON .Details }}\" } }, \"routing_key\" : \"--routing_key--\" , \"dedup_key\" : \"{{ .NotificationID }}\" , \"event_action\" : \"trigger\" , \"client\" : \"CAST AI\" , \"client_url\" : \"https://console.cast.ai/external-clusters/{{ .Cluster.ID}}?org={{ .OrganizationID }}\" , } Note that as dedup_key it was set the NotificationID, this field is unique in CAST AI, and will ensure you won't produce an alert with the same content more than once. How to create the routing_key isn't in the scope of this how to, you can find more information at https://developer.pagerduty.com/docs/ZG9jOjExMDI5NTgx-send-an-alert-event.","title":"Example of Request Template PageDuty"},{"location":"product-overview/hosted-components/","text":"CAST AI Components Hosted On Customers' Clusters \u00b6 Connecting existing cluster procedure installs several CAST AI components into a cluster. This is done in phases to provide different levels of experience: Phase 1 - is meant to provide visibility around connected clusters without possibility of tuning it; one can think of Phase 1 as about operating in read-only mode. Phase 2 - enables all the functionality of CAST AI platform mostly around clusters optimisation; CAST AI platform instructs clusters and Cloud Providers to re-arrange used resources to reach most optimal state. Phase 1 Component - CAST AI Kubernetes Agent \u00b6 CAST AI Agent is the first component installed when a new cluster is connected. The agent runs as a Pod in a CAST AI dedicated namespace: \u00bb kubectl get pods -n castai-agent NAME READY STATUS RESTARTS AGE castai-agent-5559cfb4b6-92rkm 2 /2 Running 0 21h There is are two containers running inside that Pod: CAST AI Kubernetes Agent is responsible for sending cluster state data (snapshots) to the main system Cluster Proportional Vertical Autoscaler is responsible for tuning allocated resource for this Pod (self-tuning) based on predefined formula Phase 2 Components - Cluster Controller, Evictor, Spot Handler, AKS Init Data \u00b6 CAST AI Cluster Controller, Evictor and Spot Handler (installed as DaemonSet, not as a regular Deployment) components are installed when a connected cluster is promoted to Phase 2, which enables cost savings by managing customer's cluster: \u00bb kubectl get deployments -n castai-agent NAME READY UP-TO-DATE AVAILABLE AGE castai-agent 1 /1 1 1 43h castai-cluster-controller 2 /2 2 2 64m castai-evictor 0 /0 0 0 64m DaemonSet installed by CAST AI \u00bb kubectl get daemonsets.apps -n castai-agent NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE castai-spot-handler 0 0 0 0 0 scheduling.cast.ai/spot = true 64m DaemonSet installed only for AKS \u00bb kubectl get daemonsets.apps -n castai-agent castai-aks-init-data 0 0 0 0 0 provisioner.cast.ai/aks-init-data = true 64m castai-spot-handler 0 0 0 0 0 scheduling.cast.ai/spot = true 64m Cluster Controller is responsible for executing actions it receives from the central platform (like for example accept a newly created node to the cluster, etc.) Evictor is responsible for removing pods from underutilised nodes to be able to decrease overall amount of cluster nodes Spot Handler is responsible for scheduled events monitoring (provided by Instance Metadata Service) and delivering them to the central platform aks-init-data is responsible for gathering and sending necessary data for AKS node creation. The data contains files from /var/lib/waagent and /var/lib/waagent/ovf-env.xml from the host. Whole process of reading data can be found here . The DaemonSet is a part of cluster-controller helm chart. Phase 2 Security Components - Kvisor \u00b6 Kvisor is resposible for images vulnerability scanning, Kubernetes YAML manifests linting and CIS security recommendations.","title":"Hosted Components"},{"location":"product-overview/hosted-components/#cast-ai-components-hosted-on-customers-clusters","text":"Connecting existing cluster procedure installs several CAST AI components into a cluster. This is done in phases to provide different levels of experience: Phase 1 - is meant to provide visibility around connected clusters without possibility of tuning it; one can think of Phase 1 as about operating in read-only mode. Phase 2 - enables all the functionality of CAST AI platform mostly around clusters optimisation; CAST AI platform instructs clusters and Cloud Providers to re-arrange used resources to reach most optimal state.","title":"CAST AI Components Hosted On Customers' Clusters"},{"location":"product-overview/hosted-components/#phase-1-component-cast-ai-kubernetes-agent","text":"CAST AI Agent is the first component installed when a new cluster is connected. The agent runs as a Pod in a CAST AI dedicated namespace: \u00bb kubectl get pods -n castai-agent NAME READY STATUS RESTARTS AGE castai-agent-5559cfb4b6-92rkm 2 /2 Running 0 21h There is are two containers running inside that Pod: CAST AI Kubernetes Agent is responsible for sending cluster state data (snapshots) to the main system Cluster Proportional Vertical Autoscaler is responsible for tuning allocated resource for this Pod (self-tuning) based on predefined formula","title":"Phase 1 Component - CAST AI Kubernetes Agent"},{"location":"product-overview/hosted-components/#phase-2-components-cluster-controller-evictor-spot-handler-aks-init-data","text":"CAST AI Cluster Controller, Evictor and Spot Handler (installed as DaemonSet, not as a regular Deployment) components are installed when a connected cluster is promoted to Phase 2, which enables cost savings by managing customer's cluster: \u00bb kubectl get deployments -n castai-agent NAME READY UP-TO-DATE AVAILABLE AGE castai-agent 1 /1 1 1 43h castai-cluster-controller 2 /2 2 2 64m castai-evictor 0 /0 0 0 64m DaemonSet installed by CAST AI \u00bb kubectl get daemonsets.apps -n castai-agent NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE castai-spot-handler 0 0 0 0 0 scheduling.cast.ai/spot = true 64m DaemonSet installed only for AKS \u00bb kubectl get daemonsets.apps -n castai-agent castai-aks-init-data 0 0 0 0 0 provisioner.cast.ai/aks-init-data = true 64m castai-spot-handler 0 0 0 0 0 scheduling.cast.ai/spot = true 64m Cluster Controller is responsible for executing actions it receives from the central platform (like for example accept a newly created node to the cluster, etc.) Evictor is responsible for removing pods from underutilised nodes to be able to decrease overall amount of cluster nodes Spot Handler is responsible for scheduled events monitoring (provided by Instance Metadata Service) and delivering them to the central platform aks-init-data is responsible for gathering and sending necessary data for AKS node creation. The data contains files from /var/lib/waagent and /var/lib/waagent/ovf-env.xml from the host. Whole process of reading data can be found here . The DaemonSet is a part of cluster-controller helm chart.","title":"Phase 2 Components - Cluster Controller, Evictor, Spot Handler, AKS Init Data"},{"location":"product-overview/hosted-components/#phase-2-security-components-kvisor","text":"Kvisor is resposible for images vulnerability scanning, Kubernetes YAML manifests linting and CIS security recommendations.","title":"Phase 2 Security Components - Kvisor"},{"location":"product-overview/security/","text":"Overview \u00b6 Monitor your Kubernetes cluster security and hygiene using CAST AI Security Report features. Features \u00b6 Best Practices checks \u00b6 CAST AI Security Report\u2019s Best Practices checks assess clusters against good security and DevOps practices based on CIS Kubernetes Benchmark, NSA, OWASP, and PCI recommendations for Kubernetes. We introduced a transparent issue scoring and prioritization system for these checks, so customers can easily prioritize the problems and spend their effort where needed. Image scanning \u00b6 Stay informed about vulnerabilities detected in operating system packages and libraries when running images in Kubernetes clusters monitored by the CAST AI. CAST AI assesses your private image for known vulnerabilities once the platform detects it in the cluster. Vulnerability information comes from various vulnerability databases and security advisories. Runtime vulnerability scan detects vulnerabilities that have bypassed the security scan in the deployment environment. How it works \u00b6 CAST AI Security Report leverages cluster state data collected by the CAST AI Kubernetes Agent , so all you need is to connect your Azure AKS , AWS EKS , kOps or GCP GKE cluster to the CAST AI cloud platform. Then CAST AI cloud platform analyses cluster state data and provide you with security insights via the CAST AI Console .","title":"Security Report"},{"location":"product-overview/security/#overview","text":"Monitor your Kubernetes cluster security and hygiene using CAST AI Security Report features.","title":"Overview"},{"location":"product-overview/security/#features","text":"","title":"Features"},{"location":"product-overview/security/#best-practices-checks","text":"CAST AI Security Report\u2019s Best Practices checks assess clusters against good security and DevOps practices based on CIS Kubernetes Benchmark, NSA, OWASP, and PCI recommendations for Kubernetes. We introduced a transparent issue scoring and prioritization system for these checks, so customers can easily prioritize the problems and spend their effort where needed.","title":"Best Practices checks"},{"location":"product-overview/security/#image-scanning","text":"Stay informed about vulnerabilities detected in operating system packages and libraries when running images in Kubernetes clusters monitored by the CAST AI. CAST AI assesses your private image for known vulnerabilities once the platform detects it in the cluster. Vulnerability information comes from various vulnerability databases and security advisories. Runtime vulnerability scan detects vulnerabilities that have bypassed the security scan in the deployment environment.","title":"Image scanning"},{"location":"product-overview/security/#how-it-works","text":"CAST AI Security Report leverages cluster state data collected by the CAST AI Kubernetes Agent , so all you need is to connect your Azure AKS , AWS EKS , kOps or GCP GKE cluster to the CAST AI cloud platform. Then CAST AI cloud platform analyses cluster state data and provide you with security insights via the CAST AI Console .","title":"How it works"},{"location":"product-overview/autoscaler/","text":"CAST AI autoscaling engine \u00b6 Introduction \u00b6 Autoscaler is a CAST AI tool built to scale Kubernetes clusters with cost efficiency as the main objective. It's goal is to dynamically adjust the count of nodes by adding new right-sized nodes and removing underutilized nodes when: There are pods which are unschedulable due to insufficient resources in the cluster. There are empty nodes which have not been utilized for a certain period of time. Autoscaler works by watching your cluster state with the help of the CAST AI Kubernetes Agent , which syncs cluster state changes to CAST AI. Autoscaler then calculates the required extra cluster capacity and detects empty nodes. The extra capacity is evaluated and converted into optimal node instance types. New optimal nodes are then provisioned while the underutilized empty nodes are removed . Concepts \u00b6 Unschedulable pods : Kubernetes pods that are in the Pending phase and their condition PodScheduled is False with reason Unschedulable . Pod constraints : Kubernetes pods can be declared with node selectors, node affinity and pod anti/affinity properties, which reduce the number of node types and topology, i.e. the hardware and zonal placement, that could satisfy the pod. Kubernetes scheduler : The Kubernetes scheduler is a control plane process which assigns pods to nodes. Empty node : A node that does not have any pods running on them. Exited, static or DaemonSet pods are ignored. Cluster state \u00b6 Autoscaler needs the cluster state to optimize the cluster. The CAST AI Kubernetes Agent is responsible for collecting all cluster state changes and sending them to the Autoscaler. It watches the Kubernetes resources with the Kubernetes informers package and syncs all changes to CAST AI in regular 15s intervals. List of resources watched by the Kubernetes Agent, required to correctly apply autoscaling decisions by the Autoscaler: Node Pod PersistentVolume PersistentVolumeClaim ReplicationController Service Deployment ReplicaSet DaemonSet StatefulSet StorageClass Job CSINode Cluster upscaling \u00b6 Autoscaler adds new nodes when it detects unschedulable pods. Pods become unschedulable when the Kubernetes scheduler fails to place them on any node. The Kubernetes scheduler has various node filters which are applied when deciding the scheduling outcome. If any of them fail, the pod is marked as unschedulable. For example: none of the nodes could fit the pod, or none of the nodes match the pod's constraints. Bin-packing \u00b6 Autoscaler calculates the required extra capacity based on the unschedulable pod count and their requested resources. Autoscaler uses bin-packing algorithms to find the most optimal pod groupings. Some pods cannot be placed on the same node because of their declared constraints. The goal of bin-packing is to find all the possible groupings. Bin-packing is considered a P versus NP problem , therefore, as the number of unschedulable pods increases, the Autoscaler will start adding various approximation strategies to keep the time of the algorithm consistent. High level diagram of a bin-packing algorithm: Instance type choosing \u00b6 Autoscaler converts the bin-packed pod groups into instance types. The goal when choosing an instance type is to find the cheapest one which matches all constraints. First, the Autoscaler filters the list of all instance types and chooses only those that are viable. Then it will sort them by their price and chooses the cheapest one. This always results in the most optimal node. Instance type pricing and availability is updated regularly and often by scraping cloud service provider APIs. So, the Autoscaler completely removes the need for humans to track the pricing and to analyze viable instance type offerings. If pods have no constraints defined, the Autoscaler is free to choose out of a vast instance type offering list. However, there are often cases where a pod needs to define constraints due to high-availability or hardware requirements. To find out more about how to take control visit the Pod placement section. Also, CAST AI has full support for spot instance scaling. Visit the Spot/Preemtible instances section to find how you can start using spot instances. Cluster downscaling \u00b6 Autoscaler removes nodes when it detects that nodes which were not utilized for a certain period of time. The goal of removing empty nodes is to save costs and reduce waste. Autoscaler only removes empty nodes. Nodes end up empty because all pods running on the node have been deleted, possibly due to ReplicaSet scale down or Job completion. Another CAST AI component which helps to bin-pack running pods in order to consolidate them into fewer nodes is the Evictor .","title":"CAST AI autoscaling engine"},{"location":"product-overview/autoscaler/#cast-ai-autoscaling-engine","text":"","title":"CAST AI autoscaling engine"},{"location":"product-overview/autoscaler/#introduction","text":"Autoscaler is a CAST AI tool built to scale Kubernetes clusters with cost efficiency as the main objective. It's goal is to dynamically adjust the count of nodes by adding new right-sized nodes and removing underutilized nodes when: There are pods which are unschedulable due to insufficient resources in the cluster. There are empty nodes which have not been utilized for a certain period of time. Autoscaler works by watching your cluster state with the help of the CAST AI Kubernetes Agent , which syncs cluster state changes to CAST AI. Autoscaler then calculates the required extra cluster capacity and detects empty nodes. The extra capacity is evaluated and converted into optimal node instance types. New optimal nodes are then provisioned while the underutilized empty nodes are removed .","title":"Introduction"},{"location":"product-overview/autoscaler/#concepts","text":"Unschedulable pods : Kubernetes pods that are in the Pending phase and their condition PodScheduled is False with reason Unschedulable . Pod constraints : Kubernetes pods can be declared with node selectors, node affinity and pod anti/affinity properties, which reduce the number of node types and topology, i.e. the hardware and zonal placement, that could satisfy the pod. Kubernetes scheduler : The Kubernetes scheduler is a control plane process which assigns pods to nodes. Empty node : A node that does not have any pods running on them. Exited, static or DaemonSet pods are ignored.","title":"Concepts"},{"location":"product-overview/autoscaler/#cluster-state","text":"Autoscaler needs the cluster state to optimize the cluster. The CAST AI Kubernetes Agent is responsible for collecting all cluster state changes and sending them to the Autoscaler. It watches the Kubernetes resources with the Kubernetes informers package and syncs all changes to CAST AI in regular 15s intervals. List of resources watched by the Kubernetes Agent, required to correctly apply autoscaling decisions by the Autoscaler: Node Pod PersistentVolume PersistentVolumeClaim ReplicationController Service Deployment ReplicaSet DaemonSet StatefulSet StorageClass Job CSINode","title":"Cluster state"},{"location":"product-overview/autoscaler/#cluster-upscaling","text":"Autoscaler adds new nodes when it detects unschedulable pods. Pods become unschedulable when the Kubernetes scheduler fails to place them on any node. The Kubernetes scheduler has various node filters which are applied when deciding the scheduling outcome. If any of them fail, the pod is marked as unschedulable. For example: none of the nodes could fit the pod, or none of the nodes match the pod's constraints.","title":"Cluster upscaling"},{"location":"product-overview/autoscaler/#bin-packing","text":"Autoscaler calculates the required extra capacity based on the unschedulable pod count and their requested resources. Autoscaler uses bin-packing algorithms to find the most optimal pod groupings. Some pods cannot be placed on the same node because of their declared constraints. The goal of bin-packing is to find all the possible groupings. Bin-packing is considered a P versus NP problem , therefore, as the number of unschedulable pods increases, the Autoscaler will start adding various approximation strategies to keep the time of the algorithm consistent. High level diagram of a bin-packing algorithm:","title":"Bin-packing"},{"location":"product-overview/autoscaler/#instance-type-choosing","text":"Autoscaler converts the bin-packed pod groups into instance types. The goal when choosing an instance type is to find the cheapest one which matches all constraints. First, the Autoscaler filters the list of all instance types and chooses only those that are viable. Then it will sort them by their price and chooses the cheapest one. This always results in the most optimal node. Instance type pricing and availability is updated regularly and often by scraping cloud service provider APIs. So, the Autoscaler completely removes the need for humans to track the pricing and to analyze viable instance type offerings. If pods have no constraints defined, the Autoscaler is free to choose out of a vast instance type offering list. However, there are often cases where a pod needs to define constraints due to high-availability or hardware requirements. To find out more about how to take control visit the Pod placement section. Also, CAST AI has full support for spot instance scaling. Visit the Spot/Preemtible instances section to find how you can start using spot instances.","title":"Instance type choosing"},{"location":"product-overview/autoscaler/#cluster-downscaling","text":"Autoscaler removes nodes when it detects that nodes which were not utilized for a certain period of time. The goal of removing empty nodes is to save costs and reduce waste. Autoscaler only removes empty nodes. Nodes end up empty because all pods running on the node have been deleted, possibly due to ReplicaSet scale down or Job completion. Another CAST AI component which helps to bin-pack running pods in order to consolidate them into fewer nodes is the Evictor .","title":"Cluster downscaling"},{"location":"product-overview/autoscaler/features/subnets/","text":"Subnets selection \u00b6 Feature availability \u00b6 EKS KOPS GKE AKS Random subnets selection (from available set) + + - + Subnets selection by usage* +** - - +** * Select the subnet with the greatest number of free IP addresses ** Available if AWS cloud CNI is used with specific settings or if Azure CNI is used for AKS clusters Available subnets detection \u00b6 Cluster subnets with subnet IP CIDR and availability zone are synced periodically with CAST AI. The autoscaler based on various rules decides from which subnets to choose when constructing in-memory nodes for autoscaling. The selection is influenced by: Pod node selector for topology.cast.ai/subnet-id label; Pod node affinity for topology.cast.ai/subnet-id label; Availability zone on in-memory node (will only consider the subnets in the same zone); Choose a zone based on constraints from other parts - e.g., Persistent Volume zone is affecting in-memory node zone; Choose the least allocated zone; Choose a random zone. If subnet calculation is supported and we detect that all the available subnets are full, the pod will get a pod event with a message there is no subnet with enough available IP addresses. EKS \u00b6 Subnets usage calculation is only available for EKS clusters that use AWS cloud CNI for networking. Subnets usage is calculated based on CNI settings and instance type networking capabilities( max ENI count on instance type and ipv4 count per ENI ). CNI settings used to calculate used IP addresses: Name Description Default WARM_ENI_TARGET How many free ENIs should be attached as reserve. 1 WARM_IP_TARGET How many free secondary IPs should be kept as reserve. MINIMUM_IP_TARGET Minimum IP count to be requested when adding node. MAX_ENI Additional caping on instance type max ENI count. CNI settings that disable subnets usage calculation: Name Supported values AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG None or false ENABLE_POD_ENI None or false ENABLE_PREFIX_DELEGATION None or false ENABLE_IPv6 None or false How does the calculation work \u00b6 The source of documentation is here . Each instance type in AWS has limits on how many ENIs can be attached and how many IPs each ENI can have , those numbers are synchronized with CAST AI periodically and used by this algorithm. Some key points: Each ENI uses 1 IP for itself and all other IPs are secondary and can be used for pods, so always (max IPs for pods in ENI = max IPs per ENI - 1). If we just attach ENI, 1 IP will always be used regardless of CNI settings. If WARM_IP_TARGET is specified WARM_ENI_TARGET is not used. If MAX_ENI < instances max ENI count, it works as an override for instance setting, otherwise instance setting is used. All the pods that have hostNetwork:true don't get secondary IP and host IP is used for communication (as an example AWS CNI and kube-proxy) PS. They are still counted as PODs and are capped by podCount constraint on node. Here is a detailed description of how WARM_ENI_TARGET, WARM_IP_TARGET, and MINIMUM_IP_TARGET work . Useful commands for investigations \u00b6 Command to get subnet IPs allocation - we consider that subnet is used only for this K8s cluster (some worker groups or security groups might use some IPs if they were created with this subnet and this could result in few IPs difference between calculation and actual allocation, bringing failed node creation instead of pod event in some edge cases), using same subnets for anything else than this cluster will make this feature work incorrectly. aws ec2 describe-network-interfaces --filters Name = subnet-id,Values = subnet_id > subnet_id.yaml Command to print all pods with IPs information, sorted by node. kubectl get pod -o = custom-columns = NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName,POD-IP:.status.podIP,HOST-IP:.status.hostIP --sort-by = .spec.nodeName --all-namespaces AKS \u00b6 Subnets usage calculation is only available for AKS clusters with Azure CNI enabled for networking. The network that contains the subnets to dynamically choose from should only be used in one cluster. Azure subnets are regional. Therefore, the allocation based on the least allocated zone cannot be performed while the allocation based on the least used subnet can. The calculation of subnet usage is done based on the fact that Azure reserves the first four and last IP address for a total of 5 IP addresses within each subnet. Additionally, whenever a node is created there is also a reservation of the number of IPs equal to the maximum number of pods supported by the node plus 1 for the node itself. The topology.cast.ai/subnet-id node selector as well as the node affinity should contain just the subnet names as values. For example, if there are two subnet IDs: /subscriptions/<subscription-id>/resourceGroups/<resource-group>/providers/Microsoft.Network/virtualNetworks/<virtual-network>/subnets/subnet-1 and /subscriptions/<subscription-id>/resourceGroups/<resource-group>/providers/Microsoft.Network/virtualNetworks/<virtual-network>/subnets/subnet-2 the nodeSelector should be as follows: spec : nodeSelector : topology.cast.ai/subnet-id : \"subnet-1\" while the nodeAffinity as below: spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : topology.cast.ai/subnet-id operator : In values : - subnet-1 - subnet-2","title":"Subnets selection"},{"location":"product-overview/autoscaler/features/subnets/#subnets-selection","text":"","title":"Subnets selection"},{"location":"product-overview/autoscaler/features/subnets/#feature-availability","text":"EKS KOPS GKE AKS Random subnets selection (from available set) + + - + Subnets selection by usage* +** - - +** * Select the subnet with the greatest number of free IP addresses ** Available if AWS cloud CNI is used with specific settings or if Azure CNI is used for AKS clusters","title":"Feature availability"},{"location":"product-overview/autoscaler/features/subnets/#available-subnets-detection","text":"Cluster subnets with subnet IP CIDR and availability zone are synced periodically with CAST AI. The autoscaler based on various rules decides from which subnets to choose when constructing in-memory nodes for autoscaling. The selection is influenced by: Pod node selector for topology.cast.ai/subnet-id label; Pod node affinity for topology.cast.ai/subnet-id label; Availability zone on in-memory node (will only consider the subnets in the same zone); Choose a zone based on constraints from other parts - e.g., Persistent Volume zone is affecting in-memory node zone; Choose the least allocated zone; Choose a random zone. If subnet calculation is supported and we detect that all the available subnets are full, the pod will get a pod event with a message there is no subnet with enough available IP addresses.","title":"Available subnets detection"},{"location":"product-overview/autoscaler/features/subnets/#eks","text":"Subnets usage calculation is only available for EKS clusters that use AWS cloud CNI for networking. Subnets usage is calculated based on CNI settings and instance type networking capabilities( max ENI count on instance type and ipv4 count per ENI ). CNI settings used to calculate used IP addresses: Name Description Default WARM_ENI_TARGET How many free ENIs should be attached as reserve. 1 WARM_IP_TARGET How many free secondary IPs should be kept as reserve. MINIMUM_IP_TARGET Minimum IP count to be requested when adding node. MAX_ENI Additional caping on instance type max ENI count. CNI settings that disable subnets usage calculation: Name Supported values AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG None or false ENABLE_POD_ENI None or false ENABLE_PREFIX_DELEGATION None or false ENABLE_IPv6 None or false","title":"EKS"},{"location":"product-overview/autoscaler/features/subnets/#how-does-the-calculation-work","text":"The source of documentation is here . Each instance type in AWS has limits on how many ENIs can be attached and how many IPs each ENI can have , those numbers are synchronized with CAST AI periodically and used by this algorithm. Some key points: Each ENI uses 1 IP for itself and all other IPs are secondary and can be used for pods, so always (max IPs for pods in ENI = max IPs per ENI - 1). If we just attach ENI, 1 IP will always be used regardless of CNI settings. If WARM_IP_TARGET is specified WARM_ENI_TARGET is not used. If MAX_ENI < instances max ENI count, it works as an override for instance setting, otherwise instance setting is used. All the pods that have hostNetwork:true don't get secondary IP and host IP is used for communication (as an example AWS CNI and kube-proxy) PS. They are still counted as PODs and are capped by podCount constraint on node. Here is a detailed description of how WARM_ENI_TARGET, WARM_IP_TARGET, and MINIMUM_IP_TARGET work .","title":"How does the calculation work"},{"location":"product-overview/autoscaler/features/subnets/#useful-commands-for-investigations","text":"Command to get subnet IPs allocation - we consider that subnet is used only for this K8s cluster (some worker groups or security groups might use some IPs if they were created with this subnet and this could result in few IPs difference between calculation and actual allocation, bringing failed node creation instead of pod event in some edge cases), using same subnets for anything else than this cluster will make this feature work incorrectly. aws ec2 describe-network-interfaces --filters Name = subnet-id,Values = subnet_id > subnet_id.yaml Command to print all pods with IPs information, sorted by node. kubectl get pod -o = custom-columns = NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName,POD-IP:.status.podIP,HOST-IP:.status.hostIP --sort-by = .spec.nodeName --all-namespaces","title":"Useful commands for investigations"},{"location":"product-overview/autoscaler/features/subnets/#aks","text":"Subnets usage calculation is only available for AKS clusters with Azure CNI enabled for networking. The network that contains the subnets to dynamically choose from should only be used in one cluster. Azure subnets are regional. Therefore, the allocation based on the least allocated zone cannot be performed while the allocation based on the least used subnet can. The calculation of subnet usage is done based on the fact that Azure reserves the first four and last IP address for a total of 5 IP addresses within each subnet. Additionally, whenever a node is created there is also a reservation of the number of IPs equal to the maximum number of pods supported by the node plus 1 for the node itself. The topology.cast.ai/subnet-id node selector as well as the node affinity should contain just the subnet names as values. For example, if there are two subnet IDs: /subscriptions/<subscription-id>/resourceGroups/<resource-group>/providers/Microsoft.Network/virtualNetworks/<virtual-network>/subnets/subnet-1 and /subscriptions/<subscription-id>/resourceGroups/<resource-group>/providers/Microsoft.Network/virtualNetworks/<virtual-network>/subnets/subnet-2 the nodeSelector should be as follows: spec : nodeSelector : topology.cast.ai/subnet-id : \"subnet-1\" while the nodeAffinity as below: spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : topology.cast.ai/subnet-id operator : In values : - subnet-1 - subnet-2","title":"AKS"},{"location":"product-overview/console/api/","text":"API Access Keys \u00b6 API documentation API authentication You can read more about our API here - API","title":"API Access Keys"},{"location":"product-overview/console/api/#api-access-keys","text":"API documentation API authentication You can read more about our API here - API","title":"API Access Keys"},{"location":"product-overview/console/audit-log/","text":"Audit Log \u00b6 Audit log of cluster management on a high level. Select a date range for the log. View operations made and who initiated them.","title":"Audit Log"},{"location":"product-overview/console/audit-log/#audit-log","text":"Audit log of cluster management on a high level. Select a date range for the log. View operations made and who initiated them.","title":"Audit Log"},{"location":"product-overview/console/autoscaler/","text":"Autoscaler \u00b6 CAST AI autoscaler automates the process of upscaling and downscaling the cluster. Autoscaling decisions are based on configured settings and policies. Ultimately autoscaler will help you optimize your cluster and reduce the cloud bill. Autoscaling modes \u00b6 Users who have connected an external cluster to CAST AI have two options how to use the autoscaler. Scoped autoscaler - is a mode of operations where CAST AI autoscaler co-exists with other autoscaler(s) on the same cluster and is managing only specific workloads. In order to restrict the scope of the autoscaler, workloads have to be modified as prescribed in this guide . Scoped autoscaler will not ensure that all workloads on the cluster have capacity to run and advanced features like Rebalancer are not available in this mode. Due to these limitations we recommend using CAST AI autoscaler as the only autoscaler on the cluster (i.e. in the full autscaling mode). Full autoscaler - this is our recommended mode of operations when CAST AI autoscaler is managing the whole cluster and all workloads. It is also the only way how autoscaler works on clusters created in CAST AI. Overview of autoscaler policies \u00b6 Bellow is the list of policies that control the autoscaler. 1. Cluster limits Cluster limits - policies that limit the cluster scale to the defined limits. This policy has the highest priority, and all the other policies cannot scale the cluster over the defined limits. CPU policy - This policy ensures that your cluster stays within the defined CPU minimum and maximum counts. Use this policy to create a guardrail against unexpected costs, in cases where traffic or workload requirements grow beyond budget expectations. 2. Node autoscaler Node autoscaler - policies to scale cluster based on the CPU or memory demand Spot/Preemtive Instances policy - This policy enables the CAST optimization engine to purchase Spot (AWS / Azure) or Preemptive (GCP) instances when pods are labeled by the user. CAST automatically handles instance interruptions and replaces instances when they are terminated by the CSP. Spot instances typically yield savings of 60-80% and are useful for stateless workloads such as microservices. CAST AI currently supports AWS Spot instances, with GCP and Azure rolling out shortly. Unscheduled pods policy - This policy automatically adds nodes to your cluster so that your pods have a place to run. Both CPU and Memory requirements are considered. You can use CAST specified labels to ensure that your pods run in a specific Cloud, or let the CAST AI optimization engine choose for you. Node Deletion Policy - This policy will automatically remove nodes from your cluster when they no longer have scheduled workloads. This allows your cluster to maintain a minimal footprint and reduce cloud costs. 3. Pod autoscaler (in CAST AI created clusters) Pod autoscaler - policies that scale the cluster pods based on demand (e.g. CPU or memory utilization). This policy is only available if cluster was created in CAST AI. Horizontal pod autoscaler (HPA) policy - This policy enables the Kubernetes Event Driven Autoscaler (KEDA) to automatically increase/decrease pod replica counts based on metrics. This enables cost savings by eliminating wasteful pods, and also ensures that your services are able to scale up to handle increased traffic and workload requirements. For more information see: Autoscaling policies Horizontal Pod autoscaler Spot/Preemptible Instances","title":"Autoscaler"},{"location":"product-overview/console/autoscaler/#autoscaler","text":"CAST AI autoscaler automates the process of upscaling and downscaling the cluster. Autoscaling decisions are based on configured settings and policies. Ultimately autoscaler will help you optimize your cluster and reduce the cloud bill.","title":"Autoscaler"},{"location":"product-overview/console/autoscaler/#autoscaling-modes","text":"Users who have connected an external cluster to CAST AI have two options how to use the autoscaler. Scoped autoscaler - is a mode of operations where CAST AI autoscaler co-exists with other autoscaler(s) on the same cluster and is managing only specific workloads. In order to restrict the scope of the autoscaler, workloads have to be modified as prescribed in this guide . Scoped autoscaler will not ensure that all workloads on the cluster have capacity to run and advanced features like Rebalancer are not available in this mode. Due to these limitations we recommend using CAST AI autoscaler as the only autoscaler on the cluster (i.e. in the full autscaling mode). Full autoscaler - this is our recommended mode of operations when CAST AI autoscaler is managing the whole cluster and all workloads. It is also the only way how autoscaler works on clusters created in CAST AI.","title":"Autoscaling modes"},{"location":"product-overview/console/autoscaler/#overview-of-autoscaler-policies","text":"Bellow is the list of policies that control the autoscaler. 1. Cluster limits Cluster limits - policies that limit the cluster scale to the defined limits. This policy has the highest priority, and all the other policies cannot scale the cluster over the defined limits. CPU policy - This policy ensures that your cluster stays within the defined CPU minimum and maximum counts. Use this policy to create a guardrail against unexpected costs, in cases where traffic or workload requirements grow beyond budget expectations. 2. Node autoscaler Node autoscaler - policies to scale cluster based on the CPU or memory demand Spot/Preemtive Instances policy - This policy enables the CAST optimization engine to purchase Spot (AWS / Azure) or Preemptive (GCP) instances when pods are labeled by the user. CAST automatically handles instance interruptions and replaces instances when they are terminated by the CSP. Spot instances typically yield savings of 60-80% and are useful for stateless workloads such as microservices. CAST AI currently supports AWS Spot instances, with GCP and Azure rolling out shortly. Unscheduled pods policy - This policy automatically adds nodes to your cluster so that your pods have a place to run. Both CPU and Memory requirements are considered. You can use CAST specified labels to ensure that your pods run in a specific Cloud, or let the CAST AI optimization engine choose for you. Node Deletion Policy - This policy will automatically remove nodes from your cluster when they no longer have scheduled workloads. This allows your cluster to maintain a minimal footprint and reduce cloud costs. 3. Pod autoscaler (in CAST AI created clusters) Pod autoscaler - policies that scale the cluster pods based on demand (e.g. CPU or memory utilization). This policy is only available if cluster was created in CAST AI. Horizontal pod autoscaler (HPA) policy - This policy enables the Kubernetes Event Driven Autoscaler (KEDA) to automatically increase/decrease pod replica counts based on metrics. This enables cost savings by eliminating wasteful pods, and also ensures that your services are able to scale up to handle increased traffic and workload requirements. For more information see: Autoscaling policies Horizontal Pod autoscaler Spot/Preemptible Instances","title":"Overview of autoscaler policies"},{"location":"product-overview/console/clusters/","text":"Clusters \u00b6 When you open any cluster from the /dashboard menu you will arrive at /clusters management. Here you will see more information about the selected cluster and will get access to the cluster management menu. Quickly navigate through active clusters. Information and log of the selected cluster. Management menu.","title":"Clusters"},{"location":"product-overview/console/clusters/#clusters","text":"When you open any cluster from the /dashboard menu you will arrive at /clusters management. Here you will see more information about the selected cluster and will get access to the cluster management menu. Quickly navigate through active clusters. Information and log of the selected cluster. Management menu.","title":"Clusters"},{"location":"product-overview/console/cost-report/","text":"Introduction \u00b6 The main purpose of the Cost report is to provide information about the cluster compute cost, cost allocation across workloads and namespaces, as well as its fluctuation over different time periods. The report is available as soon as the cluster is connected to CAST AI, and the underlying data is refreshed every 60 seconds. Cost reporting includes three separate sections: Cluster - compute cost report, including normalized compute costs and daily costs. Workloads - compute cost broken down by workload, with some additional information like controller type and namespace. Namespaces - compute cost broken down by namespace, including normalized CPU cost, on-demand, spot, and fallback CPU cost. Concepts \u00b6 Total cluster compute cost - the total monthly cost of compute resources provisioned on a cluster. Normalized cost per 1 CPU - the total cluster compute cost divided by the total number of CPUs provisioned on a cluster. Subtotals of this number will also be calculated for spot, on-demand, and fallback instances. Workloads - a workload is an application running on Kubernetes. Namespaces - a namespace provides a mechanism for isolating groups of resources within a single cluster. The names of resources need to be unique within a namespace but not across namespaces. Namespace-based scoping is applicable only for namespaced objects (e.g., Deployments, Services, etc.) and not for cluster-wide objects (e.g., StorageClass, Nodes, PersistentVolumes, etc.). Sections \u00b6 The CAST AI Cost report includes three sections with different levels of granularity - total cluster compute cost report, workload cost report, and namespace cost report. The information below is split by the report. Cluster (Total compute cost) \u00b6 Cluster cost report consists of compute spend , normalized cost per CPU and daily compute cost details views. CAST AI uses public cloud inventory data to calculate cluster compute costs. Custom pricing models are not reflected in our reports at this time. The report differs depending on the selected date filter, so the information below is split into two sections. Date filter: This month \u00b6 Current month spend: Sum of all complete days spend + sum of projected today's spend Monthly forecast: Takes into account all complete days and forecasts spending until the end of the month using 7-day moving average, e.g., tomorrow's average spend is the sum of today's spend + the last 6 days divided by 7. The day after tomorrow is calculated similarly, including tomorrow's projection. Average daily cost: The sum of all complete days' spend divided by the number of complete days. Average cost per CPU: Monthly - A monthly cluster compute cost forecast divided by an average number of provisioned CPUs in the period. Daily - An average daily cluster compute cost divided by an average provisioned CPUs in the period. Hourly - An average hourly cluster compute cost divided by an average provisioned CPUs in the period. Projected end of the day spend: Current day's total spend (complete hours) + average spent per hour multiplied by remaining time. This can also be broken down by on-demand, fallback, and spot spend. Date filter: Any past date (day or hour) \u00b6 Compute spend: Actual cluster compute spend in the selected period. Avg. monthly cost: Actual cluster compute spend extrapolated into full monthly spend. Avg. daily cost: Total cluster compute cost in the selected period divided by days in the selected period. Avg. cost per CPU: Monthly - An avg. monthly cluster compute cost divided by an average provisioned CPUs in the period. Daily - An avg. daily cluster compute cost divided by an average provisioned CPUs in the period. Hourly - An avg. hourly cluster compute cost divided by an average provisioned CPUs in the period. Total compute spend: Total cluster cost in the selected period broken down by on-demand, fallback, and spot. Other metrics \u00b6 Normalized cost per CPU is calculated as follows: Hourly: Average hourly CPU cost divided by an average hourly provisioned CPUs. Daily: Average hourly CPU cost divided by an average hourly provisioned CPUs and multiplied by 24. Monthly: Average hourly CPU cost divided by an average hourly provisioned CPUs and multiplied by 730. Different cost periods will yield different results, i.e. you can select to view monthly, daily, or hourly CPU cost rate. Daily compute cost details are calculated similarly (all metrics are calculated per hour): Normalized CPU: Total day's CPU cost divided by day's provisioned CPUs. On-demand: Total day's On-demand CPU cost divided by day's provisioned On-demand CPUs. Spot: Total day's Spot CPU cost divided by day's provisioned Spot CPUs. Fallback: Total day's Fallback CPU cost divided by day's provisioned Fallback CPUs. Workloads \u00b6 Workloads report uses the same underlying data as the cluster Cost report. This report allows you to view your cluster costs broken down by workload. Additional information includes controller type, namespace, and information about replicas, CPU requests, and costs. Changing the date filter does not impact logic of how costs are calculated (unlike in the total compute cost report). Replica: Average hourly replica count (rounded to integer). CPU: Average hourly CPU requests (rounded to 3 decimal numbers). $ per CPU: Average hourly cost divided by average hourly CPU requested count. Total cost: Sum of all pod costs per workload. Namespaces \u00b6 This report allows you to view your cluster costs broken down by the namespace. Daily compute cost details for all namespaces are calculated similarly (all metrics are calculated per hour): Normalized CPU per namespace: Total day's CPU cost divided by day's provisioned CPUs. On-demand: Total day's On-demand CPU cost divided by day's provisioned On-demand CPUs. Fallback: Total day's Fallback CPU cost divided by day's provisioned Fallback CPUs. Spot: Total day's Spot CPU cost divided by day's provisioned Spot CPUs. Total cost: Sum of all compute costs associated with the workloads in a namespace.","title":"Cost report"},{"location":"product-overview/console/cost-report/#introduction","text":"The main purpose of the Cost report is to provide information about the cluster compute cost, cost allocation across workloads and namespaces, as well as its fluctuation over different time periods. The report is available as soon as the cluster is connected to CAST AI, and the underlying data is refreshed every 60 seconds. Cost reporting includes three separate sections: Cluster - compute cost report, including normalized compute costs and daily costs. Workloads - compute cost broken down by workload, with some additional information like controller type and namespace. Namespaces - compute cost broken down by namespace, including normalized CPU cost, on-demand, spot, and fallback CPU cost.","title":"Introduction"},{"location":"product-overview/console/cost-report/#concepts","text":"Total cluster compute cost - the total monthly cost of compute resources provisioned on a cluster. Normalized cost per 1 CPU - the total cluster compute cost divided by the total number of CPUs provisioned on a cluster. Subtotals of this number will also be calculated for spot, on-demand, and fallback instances. Workloads - a workload is an application running on Kubernetes. Namespaces - a namespace provides a mechanism for isolating groups of resources within a single cluster. The names of resources need to be unique within a namespace but not across namespaces. Namespace-based scoping is applicable only for namespaced objects (e.g., Deployments, Services, etc.) and not for cluster-wide objects (e.g., StorageClass, Nodes, PersistentVolumes, etc.).","title":"Concepts"},{"location":"product-overview/console/cost-report/#sections","text":"The CAST AI Cost report includes three sections with different levels of granularity - total cluster compute cost report, workload cost report, and namespace cost report. The information below is split by the report.","title":"Sections"},{"location":"product-overview/console/cost-report/#cluster-total-compute-cost","text":"Cluster cost report consists of compute spend , normalized cost per CPU and daily compute cost details views. CAST AI uses public cloud inventory data to calculate cluster compute costs. Custom pricing models are not reflected in our reports at this time. The report differs depending on the selected date filter, so the information below is split into two sections.","title":"Cluster (Total compute cost)"},{"location":"product-overview/console/cost-report/#date-filter-this-month","text":"Current month spend: Sum of all complete days spend + sum of projected today's spend Monthly forecast: Takes into account all complete days and forecasts spending until the end of the month using 7-day moving average, e.g., tomorrow's average spend is the sum of today's spend + the last 6 days divided by 7. The day after tomorrow is calculated similarly, including tomorrow's projection. Average daily cost: The sum of all complete days' spend divided by the number of complete days. Average cost per CPU: Monthly - A monthly cluster compute cost forecast divided by an average number of provisioned CPUs in the period. Daily - An average daily cluster compute cost divided by an average provisioned CPUs in the period. Hourly - An average hourly cluster compute cost divided by an average provisioned CPUs in the period. Projected end of the day spend: Current day's total spend (complete hours) + average spent per hour multiplied by remaining time. This can also be broken down by on-demand, fallback, and spot spend.","title":"Date filter: This month"},{"location":"product-overview/console/cost-report/#date-filter-any-past-date-day-or-hour","text":"Compute spend: Actual cluster compute spend in the selected period. Avg. monthly cost: Actual cluster compute spend extrapolated into full monthly spend. Avg. daily cost: Total cluster compute cost in the selected period divided by days in the selected period. Avg. cost per CPU: Monthly - An avg. monthly cluster compute cost divided by an average provisioned CPUs in the period. Daily - An avg. daily cluster compute cost divided by an average provisioned CPUs in the period. Hourly - An avg. hourly cluster compute cost divided by an average provisioned CPUs in the period. Total compute spend: Total cluster cost in the selected period broken down by on-demand, fallback, and spot.","title":"Date filter: Any past date (day or hour)"},{"location":"product-overview/console/cost-report/#other-metrics","text":"Normalized cost per CPU is calculated as follows: Hourly: Average hourly CPU cost divided by an average hourly provisioned CPUs. Daily: Average hourly CPU cost divided by an average hourly provisioned CPUs and multiplied by 24. Monthly: Average hourly CPU cost divided by an average hourly provisioned CPUs and multiplied by 730. Different cost periods will yield different results, i.e. you can select to view monthly, daily, or hourly CPU cost rate. Daily compute cost details are calculated similarly (all metrics are calculated per hour): Normalized CPU: Total day's CPU cost divided by day's provisioned CPUs. On-demand: Total day's On-demand CPU cost divided by day's provisioned On-demand CPUs. Spot: Total day's Spot CPU cost divided by day's provisioned Spot CPUs. Fallback: Total day's Fallback CPU cost divided by day's provisioned Fallback CPUs.","title":"Other metrics"},{"location":"product-overview/console/cost-report/#workloads","text":"Workloads report uses the same underlying data as the cluster Cost report. This report allows you to view your cluster costs broken down by workload. Additional information includes controller type, namespace, and information about replicas, CPU requests, and costs. Changing the date filter does not impact logic of how costs are calculated (unlike in the total compute cost report). Replica: Average hourly replica count (rounded to integer). CPU: Average hourly CPU requests (rounded to 3 decimal numbers). $ per CPU: Average hourly cost divided by average hourly CPU requested count. Total cost: Sum of all pod costs per workload.","title":"Workloads"},{"location":"product-overview/console/cost-report/#namespaces","text":"This report allows you to view your cluster costs broken down by the namespace. Daily compute cost details for all namespaces are calculated similarly (all metrics are calculated per hour): Normalized CPU per namespace: Total day's CPU cost divided by day's provisioned CPUs. On-demand: Total day's On-demand CPU cost divided by day's provisioned On-demand CPUs. Fallback: Total day's Fallback CPU cost divided by day's provisioned Fallback CPUs. Spot: Total day's Spot CPU cost divided by day's provisioned Spot CPUs. Total cost: Sum of all compute costs associated with the workloads in a namespace.","title":"Namespaces"},{"location":"product-overview/console/logs/","text":"Logs \u00b6 Kubernetes UI \u00b6 View more detailed information about the selected cluster and manage it in the Kubernetes UI . Kibana logs \u00b6 View Kibana logs of the selected cluster. Grafana logs \u00b6 View Grafana logs of the selected cluster.","title":"Logs"},{"location":"product-overview/console/logs/#logs","text":"","title":"Logs"},{"location":"product-overview/console/logs/#kubernetes-ui","text":"View more detailed information about the selected cluster and manage it in the Kubernetes UI .","title":"Kubernetes UI"},{"location":"product-overview/console/logs/#kibana-logs","text":"View Kibana logs of the selected cluster.","title":"Kibana logs"},{"location":"product-overview/console/logs/#grafana-logs","text":"View Grafana logs of the selected cluster.","title":"Grafana logs"},{"location":"product-overview/console/nodes/","text":"Nodes \u00b6 Nodes view provides information about the nodes in the cluster. It allows you to quickly find a specific node, or sort and filter the list based on criteria. Depending on the use case, you can quickly identify the most expensive node, or the one that's failing to join the cluster. Apart from sorting and filtering, following actions can be performed on the selected nodes. View additional details about each individual node, just by clicking on it in the list 2. Delete a node or interrupt it (in case of spot nodes) 3. Select specific nodes to rebalance","title":"Nodes"},{"location":"product-overview/console/nodes/#nodes","text":"Nodes view provides information about the nodes in the cluster. It allows you to quickly find a specific node, or sort and filter the list based on criteria. Depending on the use case, you can quickly identify the most expensive node, or the one that's failing to join the cluster. Apart from sorting and filtering, following actions can be performed on the selected nodes. View additional details about each individual node, just by clicking on it in the list 2. Delete a node or interrupt it (in case of spot nodes) 3. Select specific nodes to rebalance","title":"Nodes"},{"location":"product-overview/console/security-insights/","text":"Security Report \u00b6 CAST AI performs various checks on information gathered by CAST AI agents. CAST AI security insights provide security recommendations for your Kubernetes clusters. You can visit the \"Security Report\" page from the side menu to see security insights. Overview \u00b6 When visiting Security Report, you will first see an overview of your cluster. The overview page allows you to understand how the security issues detected by CAST AI have changed over time. The top section of the overview page represents the state of failed checks in your cluster. In the diagram, you can see how that state has changed over time. Above the diagram, you can see the current state of security checks. The bottom section of the overview page contains two tabs. Tab \"Best practices\" shows an overview of Kubernetes resources that CAST AI has checked for possible misconfigurations. Tab \"Vulnerabilities\" shows how many Kubernetes resources CAST AI has scanned for vulnerabilities. Best Practices \u00b6 When you install the CAST AI agent, it observes changes in your Kubernetes cluster. CAST AI uses this information to check for possible misconfigurations and provides recommendations. To view a report on Kubernetes resource misconfigurations, you should go to the \"Best Practices\" tab from the Security Report page. The \"Best Practices\" report page also has two sections. In the top section of the \"Best Practices\" page, there is a summary of the current state report. You can browse all the performed checks and their status in the area below. While browsing the list of checks, you can click on the check name to see more detailed information about it. In the details sidebar, you will see a description of the check, issue severity level, issue severity score, category, and CVSS v3.1 vector. CAST AI uses the CVSS v3.1 standard to determine the issue's severity score and severity level. Click on \"Resources with this issue\" link to look at the resources that have this issue. Vulnerabilities \u00b6 CAST AI can also check container images and operating systems for potential vulnerabilities. To see these observations, open the \"Vulnerabilities\" tab. The vulnerabilities report also contains two sections. The top section summarizes the current information state, and in the area below, you can browse the report items. By clicking on an item in the list, you will open a sidebar with more information about it. Details sidebar has two tabs - \"Vulnerabilities\" and \"Affected resources.\" The vulnerabilities tab contains a list of CVE's (Common Vulnerabilities and Exposures) associated with an item. You can click the CVE to open its description in the new browser window. In the \"Affected resources\" tab, you will see the Kubernetes objects related to this item.","title":"Security Report"},{"location":"product-overview/console/security-insights/#security-report","text":"CAST AI performs various checks on information gathered by CAST AI agents. CAST AI security insights provide security recommendations for your Kubernetes clusters. You can visit the \"Security Report\" page from the side menu to see security insights.","title":"Security Report"},{"location":"product-overview/console/security-insights/#overview","text":"When visiting Security Report, you will first see an overview of your cluster. The overview page allows you to understand how the security issues detected by CAST AI have changed over time. The top section of the overview page represents the state of failed checks in your cluster. In the diagram, you can see how that state has changed over time. Above the diagram, you can see the current state of security checks. The bottom section of the overview page contains two tabs. Tab \"Best practices\" shows an overview of Kubernetes resources that CAST AI has checked for possible misconfigurations. Tab \"Vulnerabilities\" shows how many Kubernetes resources CAST AI has scanned for vulnerabilities.","title":"Overview"},{"location":"product-overview/console/security-insights/#best-practices","text":"When you install the CAST AI agent, it observes changes in your Kubernetes cluster. CAST AI uses this information to check for possible misconfigurations and provides recommendations. To view a report on Kubernetes resource misconfigurations, you should go to the \"Best Practices\" tab from the Security Report page. The \"Best Practices\" report page also has two sections. In the top section of the \"Best Practices\" page, there is a summary of the current state report. You can browse all the performed checks and their status in the area below. While browsing the list of checks, you can click on the check name to see more detailed information about it. In the details sidebar, you will see a description of the check, issue severity level, issue severity score, category, and CVSS v3.1 vector. CAST AI uses the CVSS v3.1 standard to determine the issue's severity score and severity level. Click on \"Resources with this issue\" link to look at the resources that have this issue.","title":"Best Practices"},{"location":"product-overview/console/security-insights/#vulnerabilities","text":"CAST AI can also check container images and operating systems for potential vulnerabilities. To see these observations, open the \"Vulnerabilities\" tab. The vulnerabilities report also contains two sections. The top section summarizes the current information state, and in the area below, you can browse the report items. By clicking on an item in the list, you will open a sidebar with more information about it. Details sidebar has two tabs - \"Vulnerabilities\" and \"Affected resources.\" The vulnerabilities tab contains a list of CVE's (Common Vulnerabilities and Exposures) associated with an item. You can click the CVE to open its description in the new browser window. In the \"Affected resources\" tab, you will see the Kubernetes objects related to this item.","title":"Vulnerabilities"},{"location":"product-overview/rebalancing/","text":"Rebalancing \u00b6 Rebalancing is a CAST AI feature that allows your clusters to reach the most cost-efficient state. Rebalancing is a process which replaces suboptimal nodes with new ones and moves the workloads automatically. To unlock this feature you will need to connect your cluster & use full autoscaler. See - External Cluster Overview . How it works \u00b6 Rebalancing works by taking all the workloads running in your cluster and finding the most optimal ways they can be distributed amongst the cheapest nodes. Rebalancing is based on the same algorithms that drive the CAST AI autoscaler to find optimal node configurations for your workloads. The only difference is that all workloads are run through them, rather than just unschedulable pods. The rebalancing process has multiple purposes: Rebalance the cluster during the initial onboarding to immediately achieve cost savings. The rebalancer aims to make it easy to start using CAST AI by running your cluster through the CAST AI algorithms and reshaping your cluster into an optimal state during onboarding. Remove fragmentation which is a normal byproduct of everyday cluster execution. The CAST AI autoscaler is a reactive autoscaling process which aims to satisfy unschedulable pods. As these reactive decisions accumulate, your cluster might become too fragmented. Consider this example: you are upscaling your workloads by 1 replica every hour. That replica is requesting 6 CPU. The cluster will end up with 24 new nodes with 8 CPU capacity each after a day. This means that you will have 48 unused fragmented CPUs. The rebalancer aims to solve this by consolidating the workloads into fewer cheaper nodes, reducing waste. Only nodes which don't have any problematic workloads will be rebalanced. Learn more about problematic workloads in the Preparation section.","title":"Rebalancing"},{"location":"product-overview/rebalancing/#rebalancing","text":"Rebalancing is a CAST AI feature that allows your clusters to reach the most cost-efficient state. Rebalancing is a process which replaces suboptimal nodes with new ones and moves the workloads automatically. To unlock this feature you will need to connect your cluster & use full autoscaler. See - External Cluster Overview .","title":"Rebalancing"},{"location":"product-overview/rebalancing/#how-it-works","text":"Rebalancing works by taking all the workloads running in your cluster and finding the most optimal ways they can be distributed amongst the cheapest nodes. Rebalancing is based on the same algorithms that drive the CAST AI autoscaler to find optimal node configurations for your workloads. The only difference is that all workloads are run through them, rather than just unschedulable pods. The rebalancing process has multiple purposes: Rebalance the cluster during the initial onboarding to immediately achieve cost savings. The rebalancer aims to make it easy to start using CAST AI by running your cluster through the CAST AI algorithms and reshaping your cluster into an optimal state during onboarding. Remove fragmentation which is a normal byproduct of everyday cluster execution. The CAST AI autoscaler is a reactive autoscaling process which aims to satisfy unschedulable pods. As these reactive decisions accumulate, your cluster might become too fragmented. Consider this example: you are upscaling your workloads by 1 replica every hour. That replica is requesting 6 CPU. The cluster will end up with 24 new nodes with 8 CPU capacity each after a day. This means that you will have 48 unused fragmented CPUs. The rebalancer aims to solve this by consolidating the workloads into fewer cheaper nodes, reducing waste. Only nodes which don't have any problematic workloads will be rebalanced. Learn more about problematic workloads in the Preparation section.","title":"How it works"},{"location":"product-overview/rebalancing/preparation/","text":"Preparation \u00b6 Problematic workloads \u00b6 Rebalancing replaces your suboptimal nodes with new ones. However, that is only done on nodes which aren't running any problematic workloads. To increase the value of the rebalancing operation, you should decrease the number of problematic pods as much as possible. Problematic workloads are pods which have unsupported node selector criteria. For example, pods which have a declared required node affinity on a custom label are considered problematic: spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : environment operator : In values : - production For a full list of supported node selector criteria visit the Configure pod placement by topology section. Go to the Rebalance page of your cluster to find the workloads which are preventing some nodes from being rebalanced. Check the Readiness column: Minimize disruption \u00b6 Existing nodes will be drained and the workloads will be migrated to the new nodes during the rebalancing. This mean that this process might be disruptive to your workloads. Rebalancing aims to minimize disruption by first creating new nodes and then draining the old ones. If your workloads do not tolerate interruptions, you might need to take special care. You have multiple options: Execute the rebalancing during maintenance hours. This would help you achieve the most cost savings. Disable rebalancing for certain nodes. This can be achieved by labelling or annotating the nodes which are running the critical pods or by annotating the critical pods. Please note, that annotated nodes will not be rebalanced and evicted by Evictor and this can reduce savings. It is recommended to annotate some nodes dedicated for critical workloads, rather than annotating multiple pods, which could be scheduled on multiple nodes and prevent their optimization. Name Value Type ( Annotation or Label ) Location ( Pod or Node ) Effect autoscaling.cast.ai/removal-disabled \"true\" Annotation on Pod , but can be both label or annotation on Node Both Pod and Node Rebalancer or Evictor won't drain a Node with this annotation or a Node running a Pod with this annotation. Example commands for finding and labelling a node running critical workloads: Find the node that hosts your critical pod: kubectl get pod -ojsonpath = '{.spec.nodeName}' critical-deployment-5ddb8f8995-94wdb Label the node that is running your critical pod: kubectl label node ip-10-0-101-156.eu-central-1.compute.internal autoscaling.cast.ai/removal-disabled = true","title":"Preparation"},{"location":"product-overview/rebalancing/preparation/#preparation","text":"","title":"Preparation"},{"location":"product-overview/rebalancing/preparation/#problematic-workloads","text":"Rebalancing replaces your suboptimal nodes with new ones. However, that is only done on nodes which aren't running any problematic workloads. To increase the value of the rebalancing operation, you should decrease the number of problematic pods as much as possible. Problematic workloads are pods which have unsupported node selector criteria. For example, pods which have a declared required node affinity on a custom label are considered problematic: spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : environment operator : In values : - production For a full list of supported node selector criteria visit the Configure pod placement by topology section. Go to the Rebalance page of your cluster to find the workloads which are preventing some nodes from being rebalanced. Check the Readiness column:","title":"Problematic workloads"},{"location":"product-overview/rebalancing/preparation/#minimize-disruption","text":"Existing nodes will be drained and the workloads will be migrated to the new nodes during the rebalancing. This mean that this process might be disruptive to your workloads. Rebalancing aims to minimize disruption by first creating new nodes and then draining the old ones. If your workloads do not tolerate interruptions, you might need to take special care. You have multiple options: Execute the rebalancing during maintenance hours. This would help you achieve the most cost savings. Disable rebalancing for certain nodes. This can be achieved by labelling or annotating the nodes which are running the critical pods or by annotating the critical pods. Please note, that annotated nodes will not be rebalanced and evicted by Evictor and this can reduce savings. It is recommended to annotate some nodes dedicated for critical workloads, rather than annotating multiple pods, which could be scheduled on multiple nodes and prevent their optimization. Name Value Type ( Annotation or Label ) Location ( Pod or Node ) Effect autoscaling.cast.ai/removal-disabled \"true\" Annotation on Pod , but can be both label or annotation on Node Both Pod and Node Rebalancer or Evictor won't drain a Node with this annotation or a Node running a Pod with this annotation. Example commands for finding and labelling a node running critical workloads: Find the node that hosts your critical pod: kubectl get pod -ojsonpath = '{.spec.nodeName}' critical-deployment-5ddb8f8995-94wdb Label the node that is running your critical pod: kubectl label node ip-10-0-101-156.eu-central-1.compute.internal autoscaling.cast.ai/removal-disabled = true","title":"Minimize disruption"},{"location":"product-overview/rebalancing/run/","text":"Rebalance your cluster \u00b6 Rebalancing your cluster will help you achieve cost savings. Before running the rebalancing process, we recommend you read the Preparation section. Generate the rebalancing plan \u00b6 Click on your cluster's Rebalance tab to inspect the workload for any problems: After inspecting the workloads click on the Generate plan button. This will redirect you to the rebalancing plan page. Depending on the size of your cluster, the plan generation might take up to two minutes: When the plan is generated, you can inspect how the cluster node configuration will look like after the rebalancing process is finished: Execute the rebalancing plan \u00b6 Once you're ready to rebalance, click the Rebalance button. Rebalancing is executed in three distinct phases, executed sequentially: Create new optimal nodes. Drain old suboptimal nodes. Delete old suboptimal nodes. When all phases complete successfully, you can enjoy great savings:","title":"Rebalance your cluster"},{"location":"product-overview/rebalancing/run/#rebalance-your-cluster","text":"Rebalancing your cluster will help you achieve cost savings. Before running the rebalancing process, we recommend you read the Preparation section.","title":"Rebalance your cluster"},{"location":"product-overview/rebalancing/run/#generate-the-rebalancing-plan","text":"Click on your cluster's Rebalance tab to inspect the workload for any problems: After inspecting the workloads click on the Generate plan button. This will redirect you to the rebalancing plan page. Depending on the size of your cluster, the plan generation might take up to two minutes: When the plan is generated, you can inspect how the cluster node configuration will look like after the rebalancing process is finished:","title":"Generate the rebalancing plan"},{"location":"product-overview/rebalancing/run/#execute-the-rebalancing-plan","text":"Once you're ready to rebalance, click the Rebalance button. Rebalancing is executed in three distinct phases, executed sequentially: Create new optimal nodes. Drain old suboptimal nodes. Delete old suboptimal nodes. When all phases complete successfully, you can enjoy great savings:","title":"Execute the rebalancing plan"}]}